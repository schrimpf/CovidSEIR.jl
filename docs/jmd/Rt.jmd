---
title       : "Rolling estimates of $R_t$"
subtitle    :
author      : Paul Schrimpf
date        : `j using Dates; print(Dates.today())`
bibliography: "covid.bib"
link-citations: true
options:
      out_width : 100%
      wrap : true
      fig_width : 800
      dpi : 192
---

[![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-sa/4.0/)

This work is licensed under a [Creative Commons Attribution-ShareAlike
4.0 International
License](http://creativecommons.org/licenses/by-sa/4.0/)



# Model

Following [Kevin
Systrom](http://systrom.com/blog/the-metric-we-need-to-manage-covid-19/),
we adapt the approach of [@bettencourt2008] to compute real-time
rolling estimates of pandemic parameters. [@bettencourt2008] begin from a SIR model,

$$
\begin{align*}
\dot{S} & = -\frac{S}{N} \beta I \\
\dot{I} & = \frac{S}{N} \beta I - \gamma I \\
\dot{R} & = \gamma I
\end{align*}
$$

To this we add the possibility that not all cases are known. Cases get
get detected at rate $I$, so cumulative confirmed cases, $C$, evolves
as

$$
\dot{C} = \tau I
$$

!!! question

    Should we add other states to this model? If yes, how? I think
    using death and hospitalization numbers in estimation makes sense.

The number of new confirmed cases from time $t$ to $t+\delta$ is then:

We will allow for the testing rate, $\tau$, and infection rate,
$\beta$, to vary over time.

$$
k_t \equiv \frac{C(t+\delta) - C(t)}{\delta} = \int_t^{t+\delta} \tau(s) I(s) ds
\approx \tau(t) I(t)
$$

As in [@bettencourt2008],

$$
\begin{align*}
I(t) = & I(t-\delta) \int_{t-\delta}^{t} e^{\frac{S(s)}{N} \beta(s) -
\gamma} ds \\
\approx & I(t-\delta) e^{\delta \left( \frac{S(t-\delta)}{N}
\beta(t-\delta) - \gamma \right)}
\end{align*}
$$

!!! note

    The reproductive number is:
    $R_t \equiv \frac{S(t)}{N}\frac{\beta(t)}{\gamma}$.

Substituting the expression for $I_t$ into $k_t$, we have

$$
\begin{align*}
k_t \approx & \tau(t) I(t-\delta) e^{\delta \left(
\frac{S(t-\delta)}{N} \beta(t-\delta) - \gamma\right)} \\
\approx & k_{t-\delta} \frac{\tau(t)}{\tau(t-\delta)}
e^{\delta \left(\frac{S(t-\delta)}{N} \beta(t-\delta) - \gamma \right)}
\end{align*}
$$

# Data

We use the same data as the [US state model](state.md).

The data combines information on

- Daily case counts and deaths from JHU CSSE
- Daily Hospitalizations, recoveries, and testing from the Covid Tracking Project
- Covid related policy changes from Raifman et al
- Movements from Google Mobility Reports
- Hourly workers from Hoembase

# Statistical Model

The above theoretical model gives a deterministic relationship between
$k_t$ and $k_{t-1}$ given the parameters. To bring it to data we must
add stochasticity.

## Systrom's approach

First we describe what Systrom does. He assumes that $R_{0} \sim
Gamma(4,1)$. Then for $t=1, ..., T$, he computes $P(R_t|k_{t},
k_{t-1}, ... ,k_0)$ iteratively using Bayes' rules. Specifically, he
assumes
$$
k_t | R_t, k_{t-1}, ... \sim Poisson(k_{t-1} e^{\gamma(R_t - 1)})
$$
and that $R_t$ follows a random walk, so the prior of $R_t | R_{t-1}$ is
$$
R_t | R_{t-1} \sim N(R_{t-1}, \sigma^2)
$$

so that

$$
P(R_t|k_{t}, k_{t-1}, ... ,k_0) = \frac{P(k_t | R_t, k_{t-1}) P(R_t |
R_{t-1}) P(R_{t-1} | k_{t-1}, ...)} {P(k_t)}
$$

Note that this computes posteriors of $R_t$ given current and past
cases. Future cases are also informative of $R_t$, and you could
instead compute $P(R_t | k_0, k_1, ..., k_T)$.

The notebook makes some mentions of Gaussian processes. There's likely
some way to recast the random walk assumption as a Gaussian process
prior (the kernel would be $\kappa(t,t') = \min{t,t'} \sigma^2$), but
that seems to me like an unusual way to describe it.

### Code

Let's see how Systrom's method works.

First the load data.

```julia
using DataFrames, Plots, StatsPlots, CovidSEIR
Plots.pyplot()

df = CovidData.statedata()
df = filter(x->x.fips<60, df)
# focus on 10 states with most cases as of April 1, 2020
sdf = select(df[df[!,:date].==Dates.Date("2020-04-01"),:], Symbol("cases.nyt"), :state) |>
    x->sort(x,Symbol("cases.nyt"), rev=true)
states=sdf[1:10,:state]
sdf = select(filter(r->r[:state] ∈ states, df), Symbol("cases.nyt"), :state, :date)
sdf = sort(sdf, [:state, :date])
sdf[!,:newcases] = by(sdf, :state, newcases = Symbol("cases.nyt") => x->(vcat(missing, diff(x))))[!,:newcases]

figs = []
for gdf in groupby(sdf, :state)
  f = @df gdf plot(:date, :newcases, legend=:none, linewidth=2, title=unique(gdf.state)[1])
  global figs = vcat(figs,f)
end
display(plot(figs[1:9]..., layout=(3,3)))
```

From this we can see that new cases are very noisy. This is especially
problematic when cases jump from near 0 to very high values, such as
in Illinois. The median value of and variance of new cases, $k_t$, are both
$k_{t-1} e^{\gamma(R_t - 1)}$. Only huge changes in $R_t$ can
rationalize huge jumps in new cases.

Let's compute posteriors for each state.

```julia
using Interpolations, Distributions

function rtpost(cases, γ, σ, prior0, casepdf)
  (rgrid, postgrid, ll) = rtpostgrid(cases)(γ, σ, prior0, casepdf)
  w = rgrid[2] - rgrid[1]
  T = length(cases)
  p = [LinearInterpolation(rgrid, postgrid[:,t]) for t in 1:T]
  coverage = 0.9
  cr = zeros(T,2)
  mu = vec(rgrid' * postgrid*w)
  for t in 1:T
    l = findfirst(cumsum(postgrid[:,t].*w).>(1-coverage)/2)
    h = findlast(cumsum(postgrid[:,t].*w).<(1-(1-coverage)/2))
    if !(l === nothing || h === nothing)
      cr[t,:] = [rgrid[l], rgrid[h]]
    end
  end
  return(p, mu, cr)
end

function rtpostgrid(cases)
  # We'll compute the posterior on these values of R_t
  rlo = 0
  rhi = 8
  steps = 500
  rgrid = range(rlo, rhi, length=steps)
  Δgrid = range(0.05, 0.95, length=10)
  w = rgrid[2] - rgrid[1]
  dr = rgrid .- rgrid'
  fn=function(γ, σ, prior0, casepdf)
    prr = pdf.(Normal(0,σ), dr) # P(r_{t+1} | r_t)
    for i in 1:size(prr,1)
      prr[i, : ] ./= sum(prr[i,:].*w)
    end
    postgrid = Matrix{typeof(σ)}(undef,length(rgrid), length(cases)) # P(R_t | k_t, k_{t-1},...)
    like = similar(postgrid, length(cases))
    for t in 1:length(cases)
      if (t==1)
        postgrid[:,t] .= prior0.(rgrid)
      else
        if (cases[t-1]===missing || cases[t]===missing)
          pkr = 1  # P(k_t | R_t)
        else
          λ = max(cases[t-1],1).* exp.(γ .* (rgrid .- 1))
          #r = λ*nbp/(1-nbp)
          #pkr = pdf.(NegativeBinomial.(r,nbp), cases[t])
          pkr = casepdf.(λ, cases[t])
          if (all(pkr.==0))
            @warn "all pkr=0"
            #@show t, cases[t], cases[t-1]
            pkr .= 1
          end
        end
        postgrid[:,t] = pkr.*(prr*postgrid[:,t-1])
        like[t] = sum(postgrid[:,t].*w)
        postgrid[:,t] ./= max(like[t], 1e-15)
      end
    end
    ll = try
      sum(log.(like))
    catch
      -710*length(like)
    end
    return((rgrid, postgrid, ll))
  end
  return(fn)
end

for σ in [0.1, 0.25, 1]
  γ =1/7
  nbp = 0.01
  figs = []
  for gdf in groupby(sdf, :state)
    p, m, cr = rtpost(gdf.newcases, γ, σ, x->pdf(truncated(Gamma(4,1),0,8), x),
                      (λ,x)->pdf(Poisson(λ),x))
    f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel="Rₜ")
    f = hline!(f,[1.0])
    figs = vcat(figs, f)
  end
  l = @layout [a{.1h};grid(1,1)]
  display(plot(plot(annotation=(0.5,0.5, "Poisson & σ=$σ"), framestyle = :none),
               plot(figs[1:9]..., layout=(3,3)), layout=l))
end
```

In these results, what is happening is that when new cases fluctuate
too much, the likelihood is identically 0, causing the posterior
calculation to break down. Increasing the variance of changes in
$R_t$, widens the posterior confidence intervals, but does not solve
the problem of vanishing likelihoods.

One thing that can "solve" the problem is choosing a distribution of $k_t |
\lambda, k_{t-1}$ with higher variance. The negative binomial with
parameters $\lambda p/(1-p)$ and $p$ has mean $\lambda$ and variance
$\lambda/p$.

```julia
γ =1/7
σ = 0.25

Plots.closeall()
for σ in [0.1, 0.25, 0.5]
  for nbp in [0.5, 0.1, 0.01]
    figs = []
    for gdf in groupby(sdf, :state)
      p, m, cr = rtpost(gdf.newcases, γ, σ, x->pdf(truncated(Gamma(4,1),0,8), x),
                      (λ,x)->pdf(NegativeBinomial(λ*nbp/(1-nbp), nbp),x));
      f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel="Rₜ")
      f = hline!(f,[1.0])
      figs = vcat(figs, f)
    end
    l = @layout [a{.1h};grid(1,1)]
    display(plot(plot(annotation=(0.5,0.5, "Negative binomial, p=$nbp, & σ=$σ"), framestyle = :none),
                 plot(figs[1:9]..., layout=(3,3)), layout=l, reuse=false))
  end
end
```

What Systrom did was smooth the new cases before using the Poisson
distribution. He used a window width of $7$ and Gaussian weights with
standard deviation $2$.

```julia
include("jmd/rtmod.jl")
σ = 0.25
Plots.closeall()
for w in [3, 7, 11]
  for s in [0.5, 2, 4]
    γ =1/7
    nbp = 0.01
    figs = []
    for gdf in groupby(sdf, :state)
      windowsize = w
      weights = pdf(Normal(0, s), -floor(windowsize/2):floor(windowsize/2))
      weights = weights/sum(weights)
      smoothcases = RT.smoother(gdf.newcases, w=weights)
      p, m, cr = rtpost(smoothcases, γ, σ, x->pdf(truncated(Gamma(4,1),0,8), x),
                        (λ,x)->pdf(Poisson(λ),x))
      f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel="Rₜ")
      f = hline!(f,[1.0])
      figs = vcat(figs, f)
    end
    l = @layout [a{.1h};grid(1,1)]
    display(plot(plot(annotation=(0.5,0.5, "Poisson & σ=$σ, s=$s, w=$w"), framestyle = :none),
                 plot(figs[1:9]..., layout=(3,3)), layout=l, reuse=false))
  end
end
```

Here we see that we can get a variety of results depending on the
smoothing used. All of these posteriors ignore the uncertainty in the
choice of smoothing parameters (and procedure).


# An alternative approach

Here we follow an approach similar in spirit to Systrom, with a few
modifications and additions. The primary modification is that we alter
the model of $k_t|k_{t-1}, R_t$ to allow measurement error in both
$k_t$ and $k_{t-1}$. We make four additions.
First, we utilize data on movement and business operations
as auxillary noisy measures of $R_t$. Second, we allow state policies
to shift the mean of $R_t$. Third, we combine data from all states to
improve precision in each. Fourth, we incorporate testing numbers into
the data.

As above, we begin from the approximation

$$
k^*_{s,t} \approx k^*_{s,t-1} \frac{\tau_{s,t}}{\tau_{s,t-1}} e^{\gamma(R_{st} - 1)})
$$

where $k^*$ is the true, unobserved number of new cases. Taking logs
and rearranging we have

$$
\log(k^*_{s,t}) - \log(k^*_{s,t-1}) = \gamma(R_{s,t} - 1) +
\log\left(\frac{\tau_{s,t}}{\tau_{s,t-1}}\right)
$$

Let $k_{s,t}$ be the noisy observed value of $k^*_{s,t}$, then

$$
\log(k_{s,t}) - \log(k_{s,t-1}) = \gamma(R_{s,t} - 1) +
\log\left(\frac{\tau_{s,t}}{\tau_{s,t-1}}\right) - \epsilon_{s,t} + \epsilon_{s,t-1}
$$

where
$$
 \log(k^*_{s,t}) =  \log(k_{s,t}) +\epsilon_{s,t}
$$
and $\epsilon_{s,t}$ is measurement error.

With appropriate assumptions on $\epsilon$, $\tau$, $R$ and other
observables, we can then use regression to estimate $R$.

As a simple example, let's assume

1. $R_{s,t} = R_{s,0} + \alpha d_{s,t}$ where $d_{s,t}$ are indicators
for NPI's being in place.
2. That $\tau_{s,t}$ is constant over time for each $s$
3. $E[\epsilon_{s,t} - \epsilon_{s,t-1}|d] = 0$ and $\epsilon_{s,t} -
\epsilon_{s,t-1}$ is uncorrelated over time (just to simplify; this is
not a good assumption).

```julia
using GLM, RegressionTables
pvars = [Symbol("Stay.at.home..shelter.in.place"),
         Symbol("State.of.emergency"),
         Symbol("Date.closed.K.12.schools"),
         Symbol("Closed.gyms"),
         Symbol("Closed.movie.theaters"),
         Symbol("Closed.day.cares"),
         Symbol("Date.banned.visitors.to.nursing.homes"),
         Symbol("Closed.non.essential.businesses"),
         Symbol("Closed.restaurants.except.take.out")]
sdf = copy(df)
for p in pvars
  sdf[!,p] = by(sdf, :state, (:date, p) => x->(!ismissing(unique(x[p])[1]) .& (x.date .>= unique(x[p])[1]))).x1
end
sdf = sort(sdf, [:state, :date])
sdf[!,:newcases] = by(sdf, :state, newcases = Symbol("cases.nyt") => x->(vcat(missing, diff(x))))[!,:newcases]
sdf[!,:dlogk] = by(sdf, :state, dlogk = :newcases => x->(vcat(missing, diff(log.(max.(x,0.1))))))[!,:dlogk]

fmla = FormulaTerm(Term(:dlogk), Tuple(Term.(vcat(pvars,:state))))
reg = lm(fmla, sdf)
regtable(reg, renderSettings=asciiOutput())
```

From this we get that if we assume $\gamma = 1/7$, then the the
baseline estimate of $R$ in Illinois is $7(0.046 + 0.034) + 1\approx
1.56$ with a stay at home order, $R$ in Illinois becomes $7(0.046 +
0.035 - 0.147) + 1 \approx 0.53$.

Some of the policies have positive coefficient estimates, which is
strange. This is likely due to assumption 1 being incorrect. There is
likely an unobserved component of $R_{s,t}$ that is positively
correlated with policy indicators.

# State space model

A direct analog of Systrom's approach is to treat $R_{s,t}$ as an
unobserved latent process. Specifically, we will assume that
$$
\begin{align*}
\tilde{R}_{s,0} & \sim N(\alpha_0, \sigma^2_{R,0}) \\
\tilde{R}_{s,t} & = \rho \tilde{R}_{s,t} + u_{s,t} \;,\; u_{s,t} \sim
N(0, \sigma^2_R) \\
R_{s,t} & = \alpha + \tilde{R}_{s,t} \\
\Delta \log(k)_{s,t} & = \gamma (R_{s,t} - 1) + \epsilon_{s,t} -
\epsilon_{s,t-1} \;, \; \epsilon_{s,t} \sim N(0, \sigma^2_k)
\end{align*}
$$

Note that the Poisson assumption on the distribution of $k_{s,t}$ used
by Systrom implies an extremely small $\sigma^2_k$, since the variance
of log Poisson($\lambda$) distribution is $1/\lambda$.

If $\epsilon_{s,t} - \epsilon_{s,t-1}$ weere independent over $t$, we
could compute the likelihood and posteriors of $R_{s,t}$ through the
standard Kalman filter. Of course, $\epsilon_{s,t} - \epsilon_{s,t-1}$
is not independent over time, so we must adjust the Kalman filter
accordingly. We follow the approach of [@kurtz2019] to make
this adjustment.

!!! question

    Is there a better reference? I'm sure someone did this much
    earlier than 2019...

We estimate the parameters using data from US states. We set time 0 as
the first day in which a state had at least 10 cumulative cases. We
then compute posteriors for the parameters by MCMC. We place the
following priors on the parameters.

```julia
using Distributions, TransformVariables, DynamicHMC, MCMCChains, Plots, StatsPlots,
  LogDensityProblems, Random, LinearAlgebra, JLD2
include("jmd/rtmod.jl")

rlo=-1
rhi=1.1
priors = (γ = truncated(Normal(1/7,1/7), 1/28, 1/1),
          σR0 = truncated(Normal(1, 3), 0, Inf),
          α0 = MvNormal([1], 3),
          σR = truncated(Normal(0.25,1),0,Inf),
          σk = truncated(Normal(0.1, 5), 0, Inf),
          α = MvNormal([1], 3),
          ρ = Uniform(rlo, rhi))

```

The estimation is fast and the chain appears to mix well.

```julia
reestimate=false
sdf = sort(sdf, (:state, :date));
dlogk = [filter(x->((x.state==st) .&
                    (x.cases .>=10)),
                sdf).dlogk for st in unique(sdf.state)];
dates = [filter(x->((x.state==st) .&
                    (x.cases .>=10)),
                sdf).date for st in unique(sdf.state)];

mdl = RT.RtRW(dlogk, priors)
trans = as( (γ = asℝ₊, σR0 = asℝ₊, α0 = as(Array, 1),
             σR = asℝ₊, σk = asℝ₊, α = as(Array,1),
             ρ=as(Real, rlo, rhi)) )
P = TransformedLogDensity(trans, mdl)
∇P = ADgradient(:ForwardDiff, P)
p0 = (γ = 1/7, σR0=1.0, α0=[4.0],σR=0.25, σk=2.0, α=[1], ρ=0.9)
x0 = inverse(trans,p0)
@time LogDensityProblems.logdensity_and_gradient(∇P, x0);

rng = MersenneTwister()
steps = 100
warmup=default_warmup_stages(local_optimization=nothing,
                             stepsize_search=nothing,
                             init_steps=steps, middle_steps=steps,
                             terminating_steps=2*steps,  doubling_stages=3, M=Symmetric)
x0 = x0
if (!isfile("rt1.jld2") || reestimate)
  res = DynamicHMC.mcmc_keep_warmup(rng, ∇P, 2000;initialization = (q = x0, ϵ=0.1),
                                    reporter = LogProgressReport(nothing, 25, 15),
                                    warmup_stages =warmup);
  post = transform.(trans,res.inference.chain)
  @save "rt1.jld2" post
end
@load "rt1.jld2" post
p = post[1]
vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)'
vals = reshape(vals, size(vals)..., 1)
names = vcat([length(p[s])==1 ? String(s) : String.(s).*"[".*string.(1:length(p[s])).*"]" for s in keys(p)]...)
cc = MCMCChains.Chains(vals, names)
display(cc)
display(plot(cc))
```

The posterior for the initial distribution of $R_{0,s}$ is not very
precise. The other parameters have fairly precise posteriors. Systrom
fixed all these parameters, except $\sigma_R$, which he estimated by
maximum likelihood to be 0.25. In these posteriors, a 95\% credible
region for $\sigma_R$ contains his estimate. The posterior of $\rho$
is not far from his imposed value of $1$, although $1$ is out of the
95\% credible region. A 95\% posterior region for $\gamma$ contains
Systrom's calibrated value of $1/7$.

It is worth noting that the estimate of $\sigma_k$ is large compared
to $\sigma_r$. This will cause new observations of $\Delta \log k$
will have a small effect on the posterior mean of $R$.

Given values of the parameters, we can compute state and time specific
posterior estimates of $R_{s,t}$.

```julia
states = unique(sdf.state)
s = findfirst(states.=="New York")
figr = RT.plotpostr(dates[s],dlogk[s],post, ones(length(dlogk[s]),1), [1])
display(plot(figr, ylim=(-1,10)))
```

This figure shows the posterior distribution of
$R_{s,t}$ in New York. The black line is the posterior mean. The dark
grey region is the average (over model parameters) of a 90\% credible
region conditional on the model parameters. This is comparable to what
Systrom (and many others) report, and ignores uncertainty in the model
parameters. The light grey region is a 90\% credile region taking into
account parameter uncertainty. The points and error bars are mean and
90\% credible regions for
$$
\Delta \log k_{t}/\gamma + 1 = R_{t} + \epsilon_t/\gamma
$$


## Posteriors for additional states

```julia
states_to_plot = ["New Jersey","Massachusetts","California",
                  "Georgia","Illinois","Michigan",
                  "Ohio","Wisconsin","Washington"]
S = length(states_to_plot)
figs = fill(plot(), 9*(S ÷ 9 + 1))
for (i,st) in enumerate(states_to_plot)
  s = findfirst(states.==st)
  figr = RT.plotpostr(dates[s],dlogk[s],post, ones(length(dlogk[s]),1),[1])
  l = @layout [a{.1h}; grid(1,1)]
  figs[i] = plot(plot(annotation=(0.5,0.5, st), framestyle = :none),
                 plot(figr, ylim=(-1,10)), layout=l)
  if ((i % 9) ==0 || ( i==length(states_to_plot)))
    display(plot(figs[(i-8):i]..., layout=(3,3), reuse=false))
  end
end
```

We can see that the posteriors vary very little from state to
state. The model picks up a general downward trend in $\Delta \log k$
through the slightly less than 1 estimate of $\rho$. This drives the
posteriors of $R_{s,t}$ in every state to decrease over time. Since
$\sigma_k >> \sigma_R$, the actual realizations of $\Delta \log k$
do not affect the state-specific posteriors very much.

!!! note

    I also tried fixing $\rho=1$. This gives similar results in terms
    of $\sigma_k >> \sigma_R$, and gives a posterior for $R_{s,t}$ that is
    approximately constant over time.

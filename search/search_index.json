{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CovidSEIR.jl \u00b6 Index \u00b6 CovidSEIR.countrymodel CovidSEIR.covidjhudata CovidSEIR.odeSEIR CovidSEIR.paramvars CovidSEIR.paramvec CovidSEIR.plotvars CovidSEIR.priorreport CovidSEIR.simtrajectories CovidSEIR.systemvars CovidSEIR.systemvec","title":"Package Docs"},{"location":"#covidseirjl","text":"","title":"CovidSEIR.jl"},{"location":"#index","text":"CovidSEIR.countrymodel CovidSEIR.covidjhudata CovidSEIR.odeSEIR CovidSEIR.paramvars CovidSEIR.paramvec CovidSEIR.plotvars CovidSEIR.priorreport CovidSEIR.simtrajectories CovidSEIR.systemvars CovidSEIR.systemvec","title":"Index"},{"location":"Rt/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License Model \u00b6 Following Kevin Systrom , we adapt the approach of (Bettencourt 2008 ) to compute real-time rolling estimates of pandemic parameters. (Bettencourt 2008 ) begin from a SIR model, \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\beta I \\\\ \\dot{I} & = \\frac{S}{N} \\beta I - \\gamma I \\\\ \\dot{R} & = \\gamma I \\end{align*} To this we add the possibility that not all cases are known. Cases get get detected at rate $I$, so cumulative confirmed cases, $C$, evolves as \\dot{C} = \\tau I Question Should we add other states to this model? If yes, how? I think using death and hospitalization numbers in estimation makes sense. The number of new confirmed cases from time $t$ to $t+\\delta$ is then: We will allow for the testing rate, $\\tau$, and infection rate, $\\beta$, to vary over time. k_t \\equiv \\frac{C(t+\\delta) - C(t)}{\\delta} = \\int_t^{t+\\delta} \\tau(s) I(s) ds \\approx \\tau(t) I(t) As in (Bettencourt 2008 ), \\begin{align*} I(t) = & I(t-\\delta) \\int_{t-\\delta}^{t} e^{\\frac{S(s)}{N} \\beta(s) - \\gamma} ds \\\\ \\approx & I(t-\\delta) e^{\\delta \\left( \\frac{S(t-\\delta)}{N} \\beta(t-\\delta) - \\gamma \\right)} \\end{align*} Note The reproductive number is: $R_t \\equiv \\frac{S(t)}{N}\\frac{\\beta(t)}{\\gamma}$. Substituting the expression for $I_t$ into $k_t$, we have \\begin{align*} k_t \\approx & \\tau(t) I(t-\\delta) e^{\\delta \\left( \\frac{S(t-\\delta)}{N} \\beta(t-\\delta) - \\gamma\\right)} \\\\ \\approx & k_{t-\\delta} \\frac{\\tau(t)}{\\tau(t-\\delta)} e^{\\delta \\left(\\frac{S(t-\\delta)}{N} \\beta(t-\\delta) - \\gamma \\right)} \\end{align*} Data \u00b6 We use the same data as the US state model . The data combines information on Daily case counts and deaths from JHU CSSE Daily Hospitalizations, recoveries, and testing from the Covid Tracking Project Covid related policy changes from Raifman et al Movements from Google Mobility Reports Hourly workers from Hoembase Statistical Model \u00b6 The above theoretical model gives a deterministic relationship between $k_t$ and $k_{t-1}$ given the parameters. To bring it to data we must add stochasticity. Systrom\u2019s approach \u00b6 First we describe what Systrom does. He assumes that $R_{0} \\sim Gamma(4,1)$. Then for $t=1, \u2026, T$, he computes $P(R_t|k_{t}, k_{t-1}, \u2026 ,k_0)$ iteratively using Bayes\u2019 rules. Specifically, he assumes k_t | R_t, k_{t-1}, ... \\sim Poisson(k_{t-1} e^{\\gamma(R_t - 1)}) and that $R_t$ follows a random walk, so the prior of $R_t | R_{t-1}$ is R_t | R_{t-1} \\sim N(R_{t-1}, \\sigma^2) so that P(R_t|k_{t}, k_{t-1}, ... ,k_0) = \\frac{P(k_t | R_t, k_{t-1}) P(R_t | R_{t-1}) P(R_{t-1} | k_{t-1}, ...)} {P(k_t)} Note that this computes posteriors of $R_t$ given current and past cases. Future cases are also informative of $R_t$, and you could instead compute $P(R_t | k_0, k_1, \u2026, k_T)$. The notebook makes some mentions of Gaussian processes. There\u2019s likely some way to recast the random walk assumption as a Gaussian process prior (the kernel would be $\\kappa(t,t\u2019) = \\min{t,t\u2019} \\sigma^2$), but that seems to me like an unusual way to describe it. Code \u00b6 Let\u2019s see how Systrom\u2019s method works. First the load data. using DataFrames, Plots, StatsPlots, CovidSEIR Plots.pyplot() df = CovidSEIR.statedata() df = filter(x->x.fips<60, df) # focus on 10 states with most cases as of April 1, 2020 sdf = select(df[df[!,:date].==Dates.Date(\"2020-04-01\"),:], Symbol(\"cases.nyt\"), :state) |> x->sort(x,Symbol(\"cases.nyt\"), rev=true) states=sdf[1:10,:state] sdf = select(filter(r->r[:state] \u2208 states, df), Symbol(\"cases.nyt\"), :state, :date) sdf = sort(sdf, [:state, :date]) sdf[!,:newcases] = by(sdf, :state, newcases = Symbol(\"cases.nyt\") => x->(vcat(missing, diff(x))))[!,:newcases] figs = [] for gdf in groupby(sdf, :state) f = @df gdf plot(:date, :newcases, legend=:none, linewidth=2, title=unique(gdf.state)[1]) global figs = vcat(figs,f) end display(plot(figs[1:9]..., layout=(3,3))) From this we can see that new cases are very noisy. This is especially problematic when cases jump from near 0 to very high values, such as in Illinois. The median value of and variance of new cases, $k_t$, are both $k_{t-1} e^{\\gamma(R_t - 1)}$. Only huge changes in $R_t$ can rationalize huge jumps in new cases. Let\u2019s compute posteriors for each state. using Interpolations, Distributions function rtpost(cases, \u03b3, \u03c3, prior0, casepdf) (rgrid, postgrid, ll) = rtpostgrid(cases)(\u03b3, \u03c3, prior0, casepdf) w = rgrid[2] - rgrid[1] T = length(cases) p = [LinearInterpolation(rgrid, postgrid[:,t]) for t in 1:T] coverage = 0.9 cr = zeros(T,2) mu = vec(rgrid' * postgrid*w) for t in 1:T l = findfirst(cumsum(postgrid[:,t].*w).>(1-coverage)/2) h = findlast(cumsum(postgrid[:,t].*w).<(1-(1-coverage)/2)) if !(l === nothing || h === nothing) cr[t,:] = [rgrid[l], rgrid[h]] end end return(p, mu, cr) end function rtpostgrid(cases) # We'll compute the posterior on these values of R_t rlo = 0 rhi = 8 steps = 500 rgrid = range(rlo, rhi, length=steps) \u0394grid = range(0.05, 0.95, length=10) w = rgrid[2] - rgrid[1] dr = rgrid .- rgrid' fn=function(\u03b3, \u03c3, prior0, casepdf) prr = pdf.(Normal(0,\u03c3), dr) # P(r_{t+1} | r_t) for i in 1:size(prr,1) prr[i, : ] ./= sum(prr[i,:].*w) end postgrid = Matrix{typeof(\u03c3)}(undef,length(rgrid), length(cases)) # P(R_t | k_t, k_{t-1},...) like = similar(postgrid, length(cases)) for t in 1:length(cases) if (t==1) postgrid[:,t] .= prior0.(rgrid) else if (cases[t-1]===missing || cases[t]===missing) pkr = 1 # P(k_t | R_t) else \u03bb = max(cases[t-1],1).* exp.(\u03b3 .* (rgrid .- 1)) #r = \u03bb*nbp/(1-nbp) #pkr = pdf.(NegativeBinomial.(r,nbp), cases[t]) pkr = casepdf.(\u03bb, cases[t]) if (all(pkr.==0)) @warn \"all pkr=0\" #@show t, cases[t], cases[t-1] pkr .= 1 end end postgrid[:,t] = pkr.*(prr*postgrid[:,t-1]) like[t] = sum(postgrid[:,t].*w) postgrid[:,t] ./= max(like[t], 1e-15) end end ll = try sum(log.(like)) catch -710*length(like) end return((rgrid, postgrid, ll)) end return(fn) end for \u03c3 in [0.1, 0.25, 1] \u03b3 =1/7 nbp = 0.01 figs = [] for gdf in groupby(sdf, :state) p, m, cr = rtpost(gdf.newcases, \u03b3, \u03c3, x->pdf(truncated(Gamma(4,1),0,8), x), (\u03bb,x)->pdf(Poisson(\u03bb),x)) f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel=\"R\u209c\") f = hline!(f,[1.0]) figs = vcat(figs, f) end l = @layout [a{.1h};grid(1,1)] display(plot(plot(annotation=(0.5,0.5, \"Poisson & \u03c3=$\u03c3\"), framestyle = :none), plot(figs[1:9]..., layout=(3,3)), layout=l)) end In these results, what is happening is that when new cases fluctuate too much, the likelihood is identically 0, causing the posterior calculation to break down. Increasing the variance of changes in $R_t$, widens the posterior confidence intervals, but does not solve the problem of vanishing likelihoods. One thing that can \u201csolve\u201d the problem is choosing a distribution of $k_t | \\lambda, k_{t-1}$ with higher variance. The negative binomial with parameters $\\lambda p/(1-p)$ and $p$ has mean $\\lambda$ and variance $\\lambda/p$. \u03b3 =1/7 \u03c3 = 0.25 Plots.closeall() for \u03c3 in [0.1, 0.25, 0.5] for nbp in [0.5, 0.1, 0.01] figs = [] for gdf in groupby(sdf, :state) p, m, cr = rtpost(gdf.newcases, \u03b3, \u03c3, x->pdf(truncated(Gamma(4,1),0,8), x), (\u03bb,x)->pdf(NegativeBinomial(\u03bb*nbp/(1-nbp), nbp),x)); f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel=\"R\u209c\") f = hline!(f,[1.0]) figs = vcat(figs, f) end l = @layout [a{.1h};grid(1,1)] display(plot(plot(annotation=(0.5,0.5, \"Negative binomial, p=$nbp, & \u03c3=$\u03c3\"), framestyle = :none), plot(figs[1:9]..., layout=(3,3)), layout=l, reuse=false)) end end What Systrom did was smooth the new cases before using the Poisson distribution. He used a window width of $7$ and Gaussian weights with standard deviation $2$. include(\"jmd/rtmod.jl\") \u03c3 = 0.25 Plots.closeall() for w in [3, 7, 11] for s in [0.5, 2, 4] \u03b3 =1/7 nbp = 0.01 figs = [] for gdf in groupby(sdf, :state) windowsize = w weights = pdf(Normal(0, s), -floor(windowsize/2):floor(windowsize/2)) weights = weights/sum(weights) smoothcases = RT.smoother(gdf.newcases, w=weights) p, m, cr = rtpost(smoothcases, \u03b3, \u03c3, x->pdf(truncated(Gamma(4,1),0,8), x), (\u03bb,x)->pdf(Poisson(\u03bb),x)) f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel=\"R\u209c\") f = hline!(f,[1.0]) figs = vcat(figs, f) end l = @layout [a{.1h};grid(1,1)] display(plot(plot(annotation=(0.5,0.5, \"Poisson & \u03c3=$\u03c3, s=$s, w=$w\"), framestyle = :none), plot(figs[1:9]..., layout=(3,3)), layout=l, reuse=false)) end end Here we see that we can get a variety of results depending on the smoothing used. All of these posteriors ignore the uncertainty in the choice of smoothing parameters (and procedure). An alternative approach \u00b6 Here we follow an approach similar in spirit to Systrom, with a few modifications and additions. The primary modification is that we alter the model of $k_t|k_{t-1}, R_t$ to allow measurement error in both $k_t$ and $k_{t-1}$. We make four additions. First, we utilize data on movement and business operations as auxillary noisy measures of $R_t$. Second, we allow state policies to shift the mean of $R_t$. Third, we combine data from all states to improve precision in each. Fourth, we incorporate testing numbers into the data. As above, we begin from the approximation k^*_{s,t} \\approx k^*_{s,t-1} \\frac{\\tau_{s,t}}{\\tau_{s,t-1}} e^{\\gamma(R_{st} - 1)}) where $k^*$ is the true, unobserved number of new cases. Taking logs and rearranging we have \\log(k^*_{s,t}) - \\log(k^*_{s,t-1}) = \\gamma(R_{s,t} - 1) + \\log\\left(\\frac{\\tau_{s,t}}{\\tau_{s,t-1}}\\right) Let $k_{s,t}$ be the noisy observed value of $k^*_{s,t}$, then \\log(k_{s,t}) - \\log(k_{s,t-1}) = \\gamma(R_{s,t} - 1) + \\log\\left(\\frac{\\tau_{s,t}}{\\tau_{s,t-1}}\\right) - \\epsilon_{s,t} + \\epsilon_{s,t-1} where \\log(k^*_{s,t}) = \\log(k_{s,t}) +\\epsilon_{s,t} and $\\epsilon_{s,t}$ is measurement error. With appropriate assumptions on $\\epsilon$, $\\tau$, $R$ and other observables, we can then use regression to estimate $R$. As a simple example, let\u2019s assume $R_{s,t} = R_{s,0} + \\alpha d_{s,t}$ where $d_{s,t}$ are indicators for NPI\u2019s being in place. That $\\tau_{s,t}$ is constant over time for each $s$ $E[\\epsilon_{s,t} - \\epsilon_{s,t-1}|d] = 0$ and $\\epsilon_{s,t} - \\epsilon_{s,t-1}$ is uncorrelated over time (just to simplify; this is not a good assumption). ```{=html} ``` julia using GLM, RegressionTables pvars = [Symbol(\"Stay.at.home..shelter.in.place\"), Symbol(\"State.of.emergency\"), Symbol(\"Date.closed.K.12.schools\"), Symbol(\"Closed.gyms\"), Symbol(\"Closed.movie.theaters\"), Symbol(\"Closed.day.cares\"), Symbol(\"Date.banned.visitors.to.nursing.homes\"), Symbol(\"Closed.non.essential.businesses\"), Symbol(\"Closed.restaurants.except.take.out\")] sdf = copy(df) for p in pvars sdf[!,p] = by(sdf, :state, (:date, p) => x->(!ismissing(unique(x[p])[1]) .& (x.date .>= unique(x[p])[1]))).x1 end sdf = sort(sdf, [:state, :date]) sdf[!,:newcases] = by(sdf, :state, newcases = Symbol(\"cases.nyt\") => x->(vcat(missing, diff(x))))[!,:newcases] sdf[!,:dlogk] = by(sdf, :state, dlogk = :newcases => x->(vcat(missing, diff(log.(max.(x,0.1))))))[!,:dlogk] fmla = FormulaTerm(Term(:dlogk), Tuple(Term.(vcat(pvars,:state)))) reg = lm(fmla, sdf) regtable(reg, renderSettings=asciiOutput()) ----------------------------------------------- dlogk ------- (1) ----------------------------------------------- (Intercept) 0.185 (0.184) Stay.at.home..shelter.in.place -0.057 (0.067) State.of.emergency 0.037 (0.080) Date.closed.K.12.schools -0.174* (0.087) Closed.gyms -0.086 (0.129) Closed.movie.theaters 0.074 (0.136) Closed.day.cares -0.012 (0.063) Date.banned.visitors.to.nursing.homes 0.042 (0.052) Closed.non.essential.businesses 0.040 (0.079) Closed.restaurants.except.take.out -0.006 (0.114) state: Alaska -0.032 (0.249) state: Arizona -0.009 (0.214) state: Arkansas 0.005 (0.247) state: California -0.032 (0.214) state: Colorado -0.014 (0.239) state: Connecticut 0.051 (0.243) state: Delaware -0.005 (0.247) state: District of Columbia 0.065 (0.242) state: Florida 0.078 (0.235) state: Georgia 0.073 (0.236) state: Hawaii -0.031 (0.240) state: Idaho -0.058 (0.250) state: Illinois -0.020 (0.213) state: Indiana 0.062 (0.241) state: Iowa -0.040 (0.243) state: Kansas 0.071 (0.242) state: Kentucky 0.046 (0.241) state: Louisiana -0.012 (0.244) state: Maine -0.021 (0.249) state: Maryland 0.091 (0.239) state: Massachusetts 0.011 (0.216) state: Michigan 0.119 (0.246) state: Minnesota 0.068 (0.241) state: Mississippi 0.105 (0.247) state: Missouri 0.091 (0.242) state: Montana -0.055 (0.250) state: Nebraska 0.039 (0.225) state: Nevada 0.077 (0.239) state: New Hampshire 0.007 (0.236) state: New Jersey 0.106 (0.238) state: New Mexico 0.042 (0.247) state: New York 0.136 (0.235) state: North Carolina 0.095 (0.237) state: North Dakota 0.069 (0.247) state: Ohio 0.135 (0.244) state: Oklahoma 0.083 (0.240) state: Oregon 0.038 (0.233) state: Pennsylvania 0.072 (0.241) state: Rhode Island 0.079 (0.235) state: South Carolina 0.082 (0.241) state: South Dakota 0.010 (0.246) state: Tennessee 0.093 (0.239) state: Texas -0.004 (0.222) state: Utah 0.024 (0.231) state: Vermont -0.027 (0.242) state: Virginia 0.052 (0.242) state: Washington -0.032 (0.212) state: West Virginia 0.025 (0.257) state: Wisconsin -0.008 (0.218) state: Wyoming 0.003 (0.247) ----------------------------------------------- Estimator OLS ----------------------------------------------- N 2,621 R2 0.007 ----------------------------------------------- From this we get that if we assume $\\gamma = 1/7$, then the the baseline estimate of $R$ in Illinois is $7(0.046 + 0.034) + 1\\approx 1.56$ with a stay at home order, $R$ in Illinois becomes $7(0.046 + 0.035 - 0.147) + 1 \\approx 0.53$. Some of the policies have positive coefficient estimates, which is strange. This is likely due to assumption 1 being incorrect. There is likely an unobserved component of $R_{s,t}$ that is positively correlated with policy indicators. State space model \u00b6 A direct analog of Systrom\u2019s approach is to treat $R_{s,t}$ as an unobserved latent process. Specifically, we will assume that \\begin{align*} \\tilde{R}_{s,0} & \\sim N(\\alpha_0, \\sigma^2_{R,0}) \\\\ \\tilde{R}_{s,t} & = \\rho \\tilde{R}_{s,t} + u_{s,t} \\;,\\; u_{s,t} \\sim N(0, \\sigma^2_R) \\\\ R_{s,t} & = \\alpha + \\tilde{R}_{s,t} \\\\ \\Delta \\log(k)_{s,t} & = \\gamma (R_{s,t} - 1) + \\epsilon_{s,t} - \\epsilon_{s,t-1} \\;, \\; \\epsilon_{s,t} \\sim N(0, \\sigma^2_k) \\end{align*} Note that the Poisson assumption on the distribution of $k_{s,t}$ used by Systrom implies an extremely small $\\sigma^2_k$, since the variance of log Poisson($\\lambda$) distribution is $1/\\lambda$. If $\\epsilon_{s,t} - \\epsilon_{s,t-1}$ weere independent over $t$, we could compute the likelihood and posteriors of $R_{s,t}$ through the standard Kalman filter. Of course, $\\epsilon_{s,t} - \\epsilon_{s,t-1}$ is not independent over time, so we must adjust the Kalman filter accordingly. We follow the approach of (Kurtz and Lin 2019 ) to make this adjustment. Question Is there a better reference? I\u2019m sure someone did this much earlier than 2019\u2026 We estimate the parameters using data from US states. We set time 0 as the first day in which a state had at least 10 cumulative cases. We then compute posteriors for the parameters by MCMC. We place the following priors on the parameters. using Distributions, TransformVariables, DynamicHMC, MCMCChains, Plots, StatsPlots, LogDensityProblems, Random, LinearAlgebra, JLD2 include(\"jmd/rtmod.jl\") rlo=-1 rhi=1.1 priors = (\u03b3 = truncated(Normal(1/7,1/7), 1/28, 1/1), \u03c3R0 = truncated(Normal(1, 3), 0, Inf), \u03b10 = MvNormal([1], 3), \u03c3R = truncated(Normal(0.25,1),0,Inf), \u03c3k = truncated(Normal(0.1, 5), 0, Inf), \u03b1 = MvNormal([1], 3), \u03c1 = Uniform(rlo, rhi)) (\u03b3 = Truncated(Normal{Float64}(\u03bc=0.14285714285714285, \u03c3=0.14285714285714285 ), range=(0.03571428571428571, 1.0)), \u03c3R0 = Truncated(Normal{Float64}(\u03bc=1.0 , \u03c3=3.0), range=(0.0, Inf)), \u03b10 = IsoNormal( dim: 1 \u03bc: [1.0] \u03a3: [9.0] ) , \u03c3R = Truncated(Normal{Float64}(\u03bc=0.25, \u03c3=1.0), range=(0.0, Inf)), \u03c3k = Tr uncated(Normal{Float64}(\u03bc=0.1, \u03c3=5.0), range=(0.0, Inf)), \u03b1 = IsoNormal( dim: 1 \u03bc: [1.0] \u03a3: [9.0] ) , \u03c1 = Uniform{Float64}(a=-1.0, b=1.1)) The estimation is fast and the chain appears to mix well. reestimate=false sdf = sort(sdf, (:state, :date)); dlogk = [filter(x->((x.state==st) .& (x.cases .>=10)), sdf).dlogk for st in unique(sdf.state)]; dates = [filter(x->((x.state==st) .& (x.cases .>=10)), sdf).date for st in unique(sdf.state)]; mdl = RT.RtRW(dlogk, priors) trans = as( (\u03b3 = as\u211d\u208a, \u03c3R0 = as\u211d\u208a, \u03b10 = as(Array, 1), \u03c3R = as\u211d\u208a, \u03c3k = as\u211d\u208a, \u03b1 = as(Array,1), \u03c1=as(Real, rlo, rhi)) ) P = TransformedLogDensity(trans, mdl) \u2207P = ADgradient(:ForwardDiff, P) p0 = (\u03b3 = 1/7, \u03c3R0=1.0, \u03b10=[4.0],\u03c3R=0.25, \u03c3k=2.0, \u03b1=[1], \u03c1=0.9) x0 = inverse(trans,p0) @time LogDensityProblems.logdensity_and_gradient(\u2207P, x0); 1.901263 seconds (3.89 M allocations: 190.595 MiB) rng = MersenneTwister() steps = 100 warmup=default_warmup_stages(local_optimization=nothing, stepsize_search=nothing, init_steps=steps, middle_steps=steps, terminating_steps=2*steps, doubling_stages=3, M=Symmetric) x0 = x0 if (!isfile(\"rt1.jld2\") || reestimate) res = DynamicHMC.mcmc_keep_warmup(rng, \u2207P, 2000;initialization = (q = x0, \u03f5=0.1), reporter = LogProgressReport(nothing, 25, 15), warmup_stages =warmup); post = transform.(trans,res.inference.chain) @save \"rt1.jld2\" post end @load \"rt1.jld2\" post p = post[1] vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)' vals = reshape(vals, size(vals)..., 1) names = vcat([length(p[s])==1 ? String(s) : String.(s).*\"[\".*string.(1:length(p[s])).*\"]\" for s in keys(p)]...) cc = MCMCChains.Chains(vals, names) display(cc) Object of type Chains, with data of type 2000\u00d77\u00d71 reshape(::Adjoint{Float64 ,Array{Float64,2}}, 2000, 7, 1) with eltype Float64 Iterations = 1:2000 Thinning interval = 1 Chains = 1 Samples per chain = 2000 parameters = \u03b3, \u03c3R0, \u03b10, \u03c3R, \u03c3k, \u03b1, \u03c1 2-element Array{ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03b3 0.0931 0.0381 0.0009 0.0023 194.3582 1.0034 \u03c3R0 1.1247 0.7666 0.0171 0.0361 515.4800 1.0000 \u03b10 5.0630 1.8530 0.0414 0.0870 344.8905 1.0025 \u03c3R 0.0957 0.0855 0.0019 0.0022 1213.6056 0.9996 \u03c3k 0.5685 0.0087 0.0002 0.0002 2659.9467 0.9999 \u03b1 0.6724 0.4373 0.0098 0.0181 498.8290 1.0004 \u03c1 0.9337 0.0144 0.0003 0.0005 672.3568 0.9997 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03b3 0.0438 0.0648 0.0828 0.1129 0.1883 \u03c3R0 0.0525 0.5365 0.9818 1.5750 2.8887 \u03b10 2.2124 3.6443 4.8462 6.2334 9.1984 \u03c3R 0.0030 0.0336 0.0725 0.1313 0.3281 \u03c3k 0.5519 0.5626 0.5684 0.5744 0.5861 \u03b1 -0.4498 0.4618 0.7480 0.9520 1.3733 \u03c1 0.9037 0.9249 0.9350 0.9435 0.9598 display(plot(cc)) The posterior for the initial distribution of $R_{0,s}$ is not very precise. The other parameters have fairly precise posteriors. Systrom fixed all these parameters, except $\\sigma_R$, which he estimated by maximum likelihood to be 0.25. In these posteriors, a 95% credible region for $\\sigma_R$ contains his estimate. The posterior of $\\rho$ is not far from his imposed value of $1$, although $1$ is out of the 95% credible region. A 95% posterior region for $\\gamma$ contains Systrom\u2019s calibrated value of $1/7$. It is worth noting that the estimate of $\\sigma_k$ is large compared to $\\sigma_r$. This will cause new observations of $\\Delta \\log k$ will have a small effect on the posterior mean of $R$. Given values of the parameters, we can compute state and time specific posterior estimates of $R_{s,t}$. states = unique(sdf.state) s = findfirst(states.==\"New York\") figr = RT.plotpostr(dates[s],dlogk[s],post, ones(length(dlogk[s]),1), [1]) display(plot(figr, ylim=(-1,10))) This figure shows the posterior distribution of $R_{s,t}$ in New York. The black line is the posterior mean. The dark grey region is the average (over model parameters) of a 90% credible region conditional on the model parameters. This is comparable to what Systrom (and many others) report, and ignores uncertainty in the model parameters. The light grey region is a 90% credile region taking into account parameter uncertainty. The points and error bars are mean and 90% credible regions for \\Delta \\log k_{t}/\\gamma + 1 = R_{t} + \\epsilon_t/\\gamma Posteriors for additional states \u00b6 states_to_plot = [\"New Jersey\",\"Massachusetts\",\"California\", \"Georgia\",\"Illinois\",\"Michigan\", \"Ohio\",\"Wisconsin\",\"Washington\"] S = length(states_to_plot) figs = fill(plot(), 9*(S \u00f7 9 + 1)) for (i,st) in enumerate(states_to_plot) s = findfirst(states.==st) figr = RT.plotpostr(dates[s],dlogk[s],post, ones(length(dlogk[s]),1),[1]) l = @layout [a{.1h}; grid(1,1)] figs[i] = plot(plot(annotation=(0.5,0.5, st), framestyle = :none), plot(figr, ylim=(-1,10)), layout=l) if ((i % 9) ==0 || ( i==length(states_to_plot))) display(plot(figs[(i-8):i]..., layout=(3,3), reuse=false)) end end We can see that the posteriors vary very little from state to state. The model picks up a general downward trend in $\\Delta \\log k$ through the slightly less than 1 estimate of $\\rho$. This drives the posteriors of $R_{s,t}$ in every state to decrease over time. Since $\\sigma_k >> \\sigma_R$, the actual realizations of $\\Delta \\log k$ do not affect the state-specific posteriors very much. Note I also tried fixing $\\rho=1$. This gives similar results in terms of $\\sigma_k >> \\sigma_R$, and gives a posterior for $R_{s,t}$ that is approximately constant over time. Bettencourt, Ruy M., Lu\u00eds M. A. AND Ribeiro. 2008. \u201cReal Time Bayesian Estimation of the Epidemic Potential of Emerging Infectious Diseases.\u201d PLOS ONE 3 (5): 1\u20139. https://doi.org/10.1371/journal.pone.0002185 . Kurtz, Vince, and Hai Lin. 2019. \u201cKalman Filtering with Gaussian Processes Measurement Noise.\u201d","title":"Systrom Approach"},{"location":"Rt/#model","text":"Following Kevin Systrom , we adapt the approach of (Bettencourt 2008 ) to compute real-time rolling estimates of pandemic parameters. (Bettencourt 2008 ) begin from a SIR model, \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\beta I \\\\ \\dot{I} & = \\frac{S}{N} \\beta I - \\gamma I \\\\ \\dot{R} & = \\gamma I \\end{align*} To this we add the possibility that not all cases are known. Cases get get detected at rate $I$, so cumulative confirmed cases, $C$, evolves as \\dot{C} = \\tau I Question Should we add other states to this model? If yes, how? I think using death and hospitalization numbers in estimation makes sense. The number of new confirmed cases from time $t$ to $t+\\delta$ is then: We will allow for the testing rate, $\\tau$, and infection rate, $\\beta$, to vary over time. k_t \\equiv \\frac{C(t+\\delta) - C(t)}{\\delta} = \\int_t^{t+\\delta} \\tau(s) I(s) ds \\approx \\tau(t) I(t) As in (Bettencourt 2008 ), \\begin{align*} I(t) = & I(t-\\delta) \\int_{t-\\delta}^{t} e^{\\frac{S(s)}{N} \\beta(s) - \\gamma} ds \\\\ \\approx & I(t-\\delta) e^{\\delta \\left( \\frac{S(t-\\delta)}{N} \\beta(t-\\delta) - \\gamma \\right)} \\end{align*} Note The reproductive number is: $R_t \\equiv \\frac{S(t)}{N}\\frac{\\beta(t)}{\\gamma}$. Substituting the expression for $I_t$ into $k_t$, we have \\begin{align*} k_t \\approx & \\tau(t) I(t-\\delta) e^{\\delta \\left( \\frac{S(t-\\delta)}{N} \\beta(t-\\delta) - \\gamma\\right)} \\\\ \\approx & k_{t-\\delta} \\frac{\\tau(t)}{\\tau(t-\\delta)} e^{\\delta \\left(\\frac{S(t-\\delta)}{N} \\beta(t-\\delta) - \\gamma \\right)} \\end{align*}","title":"Model"},{"location":"Rt/#data","text":"We use the same data as the US state model . The data combines information on Daily case counts and deaths from JHU CSSE Daily Hospitalizations, recoveries, and testing from the Covid Tracking Project Covid related policy changes from Raifman et al Movements from Google Mobility Reports Hourly workers from Hoembase","title":"Data"},{"location":"Rt/#statistical-model","text":"The above theoretical model gives a deterministic relationship between $k_t$ and $k_{t-1}$ given the parameters. To bring it to data we must add stochasticity.","title":"Statistical Model"},{"location":"Rt/#systroms-approach","text":"First we describe what Systrom does. He assumes that $R_{0} \\sim Gamma(4,1)$. Then for $t=1, \u2026, T$, he computes $P(R_t|k_{t}, k_{t-1}, \u2026 ,k_0)$ iteratively using Bayes\u2019 rules. Specifically, he assumes k_t | R_t, k_{t-1}, ... \\sim Poisson(k_{t-1} e^{\\gamma(R_t - 1)}) and that $R_t$ follows a random walk, so the prior of $R_t | R_{t-1}$ is R_t | R_{t-1} \\sim N(R_{t-1}, \\sigma^2) so that P(R_t|k_{t}, k_{t-1}, ... ,k_0) = \\frac{P(k_t | R_t, k_{t-1}) P(R_t | R_{t-1}) P(R_{t-1} | k_{t-1}, ...)} {P(k_t)} Note that this computes posteriors of $R_t$ given current and past cases. Future cases are also informative of $R_t$, and you could instead compute $P(R_t | k_0, k_1, \u2026, k_T)$. The notebook makes some mentions of Gaussian processes. There\u2019s likely some way to recast the random walk assumption as a Gaussian process prior (the kernel would be $\\kappa(t,t\u2019) = \\min{t,t\u2019} \\sigma^2$), but that seems to me like an unusual way to describe it.","title":"Systrom\u2019s approach"},{"location":"Rt/#code","text":"Let\u2019s see how Systrom\u2019s method works. First the load data. using DataFrames, Plots, StatsPlots, CovidSEIR Plots.pyplot() df = CovidSEIR.statedata() df = filter(x->x.fips<60, df) # focus on 10 states with most cases as of April 1, 2020 sdf = select(df[df[!,:date].==Dates.Date(\"2020-04-01\"),:], Symbol(\"cases.nyt\"), :state) |> x->sort(x,Symbol(\"cases.nyt\"), rev=true) states=sdf[1:10,:state] sdf = select(filter(r->r[:state] \u2208 states, df), Symbol(\"cases.nyt\"), :state, :date) sdf = sort(sdf, [:state, :date]) sdf[!,:newcases] = by(sdf, :state, newcases = Symbol(\"cases.nyt\") => x->(vcat(missing, diff(x))))[!,:newcases] figs = [] for gdf in groupby(sdf, :state) f = @df gdf plot(:date, :newcases, legend=:none, linewidth=2, title=unique(gdf.state)[1]) global figs = vcat(figs,f) end display(plot(figs[1:9]..., layout=(3,3))) From this we can see that new cases are very noisy. This is especially problematic when cases jump from near 0 to very high values, such as in Illinois. The median value of and variance of new cases, $k_t$, are both $k_{t-1} e^{\\gamma(R_t - 1)}$. Only huge changes in $R_t$ can rationalize huge jumps in new cases. Let\u2019s compute posteriors for each state. using Interpolations, Distributions function rtpost(cases, \u03b3, \u03c3, prior0, casepdf) (rgrid, postgrid, ll) = rtpostgrid(cases)(\u03b3, \u03c3, prior0, casepdf) w = rgrid[2] - rgrid[1] T = length(cases) p = [LinearInterpolation(rgrid, postgrid[:,t]) for t in 1:T] coverage = 0.9 cr = zeros(T,2) mu = vec(rgrid' * postgrid*w) for t in 1:T l = findfirst(cumsum(postgrid[:,t].*w).>(1-coverage)/2) h = findlast(cumsum(postgrid[:,t].*w).<(1-(1-coverage)/2)) if !(l === nothing || h === nothing) cr[t,:] = [rgrid[l], rgrid[h]] end end return(p, mu, cr) end function rtpostgrid(cases) # We'll compute the posterior on these values of R_t rlo = 0 rhi = 8 steps = 500 rgrid = range(rlo, rhi, length=steps) \u0394grid = range(0.05, 0.95, length=10) w = rgrid[2] - rgrid[1] dr = rgrid .- rgrid' fn=function(\u03b3, \u03c3, prior0, casepdf) prr = pdf.(Normal(0,\u03c3), dr) # P(r_{t+1} | r_t) for i in 1:size(prr,1) prr[i, : ] ./= sum(prr[i,:].*w) end postgrid = Matrix{typeof(\u03c3)}(undef,length(rgrid), length(cases)) # P(R_t | k_t, k_{t-1},...) like = similar(postgrid, length(cases)) for t in 1:length(cases) if (t==1) postgrid[:,t] .= prior0.(rgrid) else if (cases[t-1]===missing || cases[t]===missing) pkr = 1 # P(k_t | R_t) else \u03bb = max(cases[t-1],1).* exp.(\u03b3 .* (rgrid .- 1)) #r = \u03bb*nbp/(1-nbp) #pkr = pdf.(NegativeBinomial.(r,nbp), cases[t]) pkr = casepdf.(\u03bb, cases[t]) if (all(pkr.==0)) @warn \"all pkr=0\" #@show t, cases[t], cases[t-1] pkr .= 1 end end postgrid[:,t] = pkr.*(prr*postgrid[:,t-1]) like[t] = sum(postgrid[:,t].*w) postgrid[:,t] ./= max(like[t], 1e-15) end end ll = try sum(log.(like)) catch -710*length(like) end return((rgrid, postgrid, ll)) end return(fn) end for \u03c3 in [0.1, 0.25, 1] \u03b3 =1/7 nbp = 0.01 figs = [] for gdf in groupby(sdf, :state) p, m, cr = rtpost(gdf.newcases, \u03b3, \u03c3, x->pdf(truncated(Gamma(4,1),0,8), x), (\u03bb,x)->pdf(Poisson(\u03bb),x)) f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel=\"R\u209c\") f = hline!(f,[1.0]) figs = vcat(figs, f) end l = @layout [a{.1h};grid(1,1)] display(plot(plot(annotation=(0.5,0.5, \"Poisson & \u03c3=$\u03c3\"), framestyle = :none), plot(figs[1:9]..., layout=(3,3)), layout=l)) end In these results, what is happening is that when new cases fluctuate too much, the likelihood is identically 0, causing the posterior calculation to break down. Increasing the variance of changes in $R_t$, widens the posterior confidence intervals, but does not solve the problem of vanishing likelihoods. One thing that can \u201csolve\u201d the problem is choosing a distribution of $k_t | \\lambda, k_{t-1}$ with higher variance. The negative binomial with parameters $\\lambda p/(1-p)$ and $p$ has mean $\\lambda$ and variance $\\lambda/p$. \u03b3 =1/7 \u03c3 = 0.25 Plots.closeall() for \u03c3 in [0.1, 0.25, 0.5] for nbp in [0.5, 0.1, 0.01] figs = [] for gdf in groupby(sdf, :state) p, m, cr = rtpost(gdf.newcases, \u03b3, \u03c3, x->pdf(truncated(Gamma(4,1),0,8), x), (\u03bb,x)->pdf(NegativeBinomial(\u03bb*nbp/(1-nbp), nbp),x)); f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel=\"R\u209c\") f = hline!(f,[1.0]) figs = vcat(figs, f) end l = @layout [a{.1h};grid(1,1)] display(plot(plot(annotation=(0.5,0.5, \"Negative binomial, p=$nbp, & \u03c3=$\u03c3\"), framestyle = :none), plot(figs[1:9]..., layout=(3,3)), layout=l, reuse=false)) end end What Systrom did was smooth the new cases before using the Poisson distribution. He used a window width of $7$ and Gaussian weights with standard deviation $2$. include(\"jmd/rtmod.jl\") \u03c3 = 0.25 Plots.closeall() for w in [3, 7, 11] for s in [0.5, 2, 4] \u03b3 =1/7 nbp = 0.01 figs = [] for gdf in groupby(sdf, :state) windowsize = w weights = pdf(Normal(0, s), -floor(windowsize/2):floor(windowsize/2)) weights = weights/sum(weights) smoothcases = RT.smoother(gdf.newcases, w=weights) p, m, cr = rtpost(smoothcases, \u03b3, \u03c3, x->pdf(truncated(Gamma(4,1),0,8), x), (\u03bb,x)->pdf(Poisson(\u03bb),x)) f = plot(gdf.date, m, ribbon=(m-cr[:,1], cr[:,2] - m), title=unique(gdf.state)[1], legend=:none, ylabel=\"R\u209c\") f = hline!(f,[1.0]) figs = vcat(figs, f) end l = @layout [a{.1h};grid(1,1)] display(plot(plot(annotation=(0.5,0.5, \"Poisson & \u03c3=$\u03c3, s=$s, w=$w\"), framestyle = :none), plot(figs[1:9]..., layout=(3,3)), layout=l, reuse=false)) end end Here we see that we can get a variety of results depending on the smoothing used. All of these posteriors ignore the uncertainty in the choice of smoothing parameters (and procedure).","title":"Code"},{"location":"Rt/#an-alternative-approach","text":"Here we follow an approach similar in spirit to Systrom, with a few modifications and additions. The primary modification is that we alter the model of $k_t|k_{t-1}, R_t$ to allow measurement error in both $k_t$ and $k_{t-1}$. We make four additions. First, we utilize data on movement and business operations as auxillary noisy measures of $R_t$. Second, we allow state policies to shift the mean of $R_t$. Third, we combine data from all states to improve precision in each. Fourth, we incorporate testing numbers into the data. As above, we begin from the approximation k^*_{s,t} \\approx k^*_{s,t-1} \\frac{\\tau_{s,t}}{\\tau_{s,t-1}} e^{\\gamma(R_{st} - 1)}) where $k^*$ is the true, unobserved number of new cases. Taking logs and rearranging we have \\log(k^*_{s,t}) - \\log(k^*_{s,t-1}) = \\gamma(R_{s,t} - 1) + \\log\\left(\\frac{\\tau_{s,t}}{\\tau_{s,t-1}}\\right) Let $k_{s,t}$ be the noisy observed value of $k^*_{s,t}$, then \\log(k_{s,t}) - \\log(k_{s,t-1}) = \\gamma(R_{s,t} - 1) + \\log\\left(\\frac{\\tau_{s,t}}{\\tau_{s,t-1}}\\right) - \\epsilon_{s,t} + \\epsilon_{s,t-1} where \\log(k^*_{s,t}) = \\log(k_{s,t}) +\\epsilon_{s,t} and $\\epsilon_{s,t}$ is measurement error. With appropriate assumptions on $\\epsilon$, $\\tau$, $R$ and other observables, we can then use regression to estimate $R$. As a simple example, let\u2019s assume $R_{s,t} = R_{s,0} + \\alpha d_{s,t}$ where $d_{s,t}$ are indicators for NPI\u2019s being in place. That $\\tau_{s,t}$ is constant over time for each $s$ $E[\\epsilon_{s,t} - \\epsilon_{s,t-1}|d] = 0$ and $\\epsilon_{s,t} - \\epsilon_{s,t-1}$ is uncorrelated over time (just to simplify; this is not a good assumption). ```{=html} ``` julia using GLM, RegressionTables pvars = [Symbol(\"Stay.at.home..shelter.in.place\"), Symbol(\"State.of.emergency\"), Symbol(\"Date.closed.K.12.schools\"), Symbol(\"Closed.gyms\"), Symbol(\"Closed.movie.theaters\"), Symbol(\"Closed.day.cares\"), Symbol(\"Date.banned.visitors.to.nursing.homes\"), Symbol(\"Closed.non.essential.businesses\"), Symbol(\"Closed.restaurants.except.take.out\")] sdf = copy(df) for p in pvars sdf[!,p] = by(sdf, :state, (:date, p) => x->(!ismissing(unique(x[p])[1]) .& (x.date .>= unique(x[p])[1]))).x1 end sdf = sort(sdf, [:state, :date]) sdf[!,:newcases] = by(sdf, :state, newcases = Symbol(\"cases.nyt\") => x->(vcat(missing, diff(x))))[!,:newcases] sdf[!,:dlogk] = by(sdf, :state, dlogk = :newcases => x->(vcat(missing, diff(log.(max.(x,0.1))))))[!,:dlogk] fmla = FormulaTerm(Term(:dlogk), Tuple(Term.(vcat(pvars,:state)))) reg = lm(fmla, sdf) regtable(reg, renderSettings=asciiOutput()) ----------------------------------------------- dlogk ------- (1) ----------------------------------------------- (Intercept) 0.185 (0.184) Stay.at.home..shelter.in.place -0.057 (0.067) State.of.emergency 0.037 (0.080) Date.closed.K.12.schools -0.174* (0.087) Closed.gyms -0.086 (0.129) Closed.movie.theaters 0.074 (0.136) Closed.day.cares -0.012 (0.063) Date.banned.visitors.to.nursing.homes 0.042 (0.052) Closed.non.essential.businesses 0.040 (0.079) Closed.restaurants.except.take.out -0.006 (0.114) state: Alaska -0.032 (0.249) state: Arizona -0.009 (0.214) state: Arkansas 0.005 (0.247) state: California -0.032 (0.214) state: Colorado -0.014 (0.239) state: Connecticut 0.051 (0.243) state: Delaware -0.005 (0.247) state: District of Columbia 0.065 (0.242) state: Florida 0.078 (0.235) state: Georgia 0.073 (0.236) state: Hawaii -0.031 (0.240) state: Idaho -0.058 (0.250) state: Illinois -0.020 (0.213) state: Indiana 0.062 (0.241) state: Iowa -0.040 (0.243) state: Kansas 0.071 (0.242) state: Kentucky 0.046 (0.241) state: Louisiana -0.012 (0.244) state: Maine -0.021 (0.249) state: Maryland 0.091 (0.239) state: Massachusetts 0.011 (0.216) state: Michigan 0.119 (0.246) state: Minnesota 0.068 (0.241) state: Mississippi 0.105 (0.247) state: Missouri 0.091 (0.242) state: Montana -0.055 (0.250) state: Nebraska 0.039 (0.225) state: Nevada 0.077 (0.239) state: New Hampshire 0.007 (0.236) state: New Jersey 0.106 (0.238) state: New Mexico 0.042 (0.247) state: New York 0.136 (0.235) state: North Carolina 0.095 (0.237) state: North Dakota 0.069 (0.247) state: Ohio 0.135 (0.244) state: Oklahoma 0.083 (0.240) state: Oregon 0.038 (0.233) state: Pennsylvania 0.072 (0.241) state: Rhode Island 0.079 (0.235) state: South Carolina 0.082 (0.241) state: South Dakota 0.010 (0.246) state: Tennessee 0.093 (0.239) state: Texas -0.004 (0.222) state: Utah 0.024 (0.231) state: Vermont -0.027 (0.242) state: Virginia 0.052 (0.242) state: Washington -0.032 (0.212) state: West Virginia 0.025 (0.257) state: Wisconsin -0.008 (0.218) state: Wyoming 0.003 (0.247) ----------------------------------------------- Estimator OLS ----------------------------------------------- N 2,621 R2 0.007 ----------------------------------------------- From this we get that if we assume $\\gamma = 1/7$, then the the baseline estimate of $R$ in Illinois is $7(0.046 + 0.034) + 1\\approx 1.56$ with a stay at home order, $R$ in Illinois becomes $7(0.046 + 0.035 - 0.147) + 1 \\approx 0.53$. Some of the policies have positive coefficient estimates, which is strange. This is likely due to assumption 1 being incorrect. There is likely an unobserved component of $R_{s,t}$ that is positively correlated with policy indicators.","title":"An alternative approach"},{"location":"Rt/#state-space-model","text":"A direct analog of Systrom\u2019s approach is to treat $R_{s,t}$ as an unobserved latent process. Specifically, we will assume that \\begin{align*} \\tilde{R}_{s,0} & \\sim N(\\alpha_0, \\sigma^2_{R,0}) \\\\ \\tilde{R}_{s,t} & = \\rho \\tilde{R}_{s,t} + u_{s,t} \\;,\\; u_{s,t} \\sim N(0, \\sigma^2_R) \\\\ R_{s,t} & = \\alpha + \\tilde{R}_{s,t} \\\\ \\Delta \\log(k)_{s,t} & = \\gamma (R_{s,t} - 1) + \\epsilon_{s,t} - \\epsilon_{s,t-1} \\;, \\; \\epsilon_{s,t} \\sim N(0, \\sigma^2_k) \\end{align*} Note that the Poisson assumption on the distribution of $k_{s,t}$ used by Systrom implies an extremely small $\\sigma^2_k$, since the variance of log Poisson($\\lambda$) distribution is $1/\\lambda$. If $\\epsilon_{s,t} - \\epsilon_{s,t-1}$ weere independent over $t$, we could compute the likelihood and posteriors of $R_{s,t}$ through the standard Kalman filter. Of course, $\\epsilon_{s,t} - \\epsilon_{s,t-1}$ is not independent over time, so we must adjust the Kalman filter accordingly. We follow the approach of (Kurtz and Lin 2019 ) to make this adjustment. Question Is there a better reference? I\u2019m sure someone did this much earlier than 2019\u2026 We estimate the parameters using data from US states. We set time 0 as the first day in which a state had at least 10 cumulative cases. We then compute posteriors for the parameters by MCMC. We place the following priors on the parameters. using Distributions, TransformVariables, DynamicHMC, MCMCChains, Plots, StatsPlots, LogDensityProblems, Random, LinearAlgebra, JLD2 include(\"jmd/rtmod.jl\") rlo=-1 rhi=1.1 priors = (\u03b3 = truncated(Normal(1/7,1/7), 1/28, 1/1), \u03c3R0 = truncated(Normal(1, 3), 0, Inf), \u03b10 = MvNormal([1], 3), \u03c3R = truncated(Normal(0.25,1),0,Inf), \u03c3k = truncated(Normal(0.1, 5), 0, Inf), \u03b1 = MvNormal([1], 3), \u03c1 = Uniform(rlo, rhi)) (\u03b3 = Truncated(Normal{Float64}(\u03bc=0.14285714285714285, \u03c3=0.14285714285714285 ), range=(0.03571428571428571, 1.0)), \u03c3R0 = Truncated(Normal{Float64}(\u03bc=1.0 , \u03c3=3.0), range=(0.0, Inf)), \u03b10 = IsoNormal( dim: 1 \u03bc: [1.0] \u03a3: [9.0] ) , \u03c3R = Truncated(Normal{Float64}(\u03bc=0.25, \u03c3=1.0), range=(0.0, Inf)), \u03c3k = Tr uncated(Normal{Float64}(\u03bc=0.1, \u03c3=5.0), range=(0.0, Inf)), \u03b1 = IsoNormal( dim: 1 \u03bc: [1.0] \u03a3: [9.0] ) , \u03c1 = Uniform{Float64}(a=-1.0, b=1.1)) The estimation is fast and the chain appears to mix well. reestimate=false sdf = sort(sdf, (:state, :date)); dlogk = [filter(x->((x.state==st) .& (x.cases .>=10)), sdf).dlogk for st in unique(sdf.state)]; dates = [filter(x->((x.state==st) .& (x.cases .>=10)), sdf).date for st in unique(sdf.state)]; mdl = RT.RtRW(dlogk, priors) trans = as( (\u03b3 = as\u211d\u208a, \u03c3R0 = as\u211d\u208a, \u03b10 = as(Array, 1), \u03c3R = as\u211d\u208a, \u03c3k = as\u211d\u208a, \u03b1 = as(Array,1), \u03c1=as(Real, rlo, rhi)) ) P = TransformedLogDensity(trans, mdl) \u2207P = ADgradient(:ForwardDiff, P) p0 = (\u03b3 = 1/7, \u03c3R0=1.0, \u03b10=[4.0],\u03c3R=0.25, \u03c3k=2.0, \u03b1=[1], \u03c1=0.9) x0 = inverse(trans,p0) @time LogDensityProblems.logdensity_and_gradient(\u2207P, x0); 1.901263 seconds (3.89 M allocations: 190.595 MiB) rng = MersenneTwister() steps = 100 warmup=default_warmup_stages(local_optimization=nothing, stepsize_search=nothing, init_steps=steps, middle_steps=steps, terminating_steps=2*steps, doubling_stages=3, M=Symmetric) x0 = x0 if (!isfile(\"rt1.jld2\") || reestimate) res = DynamicHMC.mcmc_keep_warmup(rng, \u2207P, 2000;initialization = (q = x0, \u03f5=0.1), reporter = LogProgressReport(nothing, 25, 15), warmup_stages =warmup); post = transform.(trans,res.inference.chain) @save \"rt1.jld2\" post end @load \"rt1.jld2\" post p = post[1] vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)' vals = reshape(vals, size(vals)..., 1) names = vcat([length(p[s])==1 ? String(s) : String.(s).*\"[\".*string.(1:length(p[s])).*\"]\" for s in keys(p)]...) cc = MCMCChains.Chains(vals, names) display(cc) Object of type Chains, with data of type 2000\u00d77\u00d71 reshape(::Adjoint{Float64 ,Array{Float64,2}}, 2000, 7, 1) with eltype Float64 Iterations = 1:2000 Thinning interval = 1 Chains = 1 Samples per chain = 2000 parameters = \u03b3, \u03c3R0, \u03b10, \u03c3R, \u03c3k, \u03b1, \u03c1 2-element Array{ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03b3 0.0931 0.0381 0.0009 0.0023 194.3582 1.0034 \u03c3R0 1.1247 0.7666 0.0171 0.0361 515.4800 1.0000 \u03b10 5.0630 1.8530 0.0414 0.0870 344.8905 1.0025 \u03c3R 0.0957 0.0855 0.0019 0.0022 1213.6056 0.9996 \u03c3k 0.5685 0.0087 0.0002 0.0002 2659.9467 0.9999 \u03b1 0.6724 0.4373 0.0098 0.0181 498.8290 1.0004 \u03c1 0.9337 0.0144 0.0003 0.0005 672.3568 0.9997 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03b3 0.0438 0.0648 0.0828 0.1129 0.1883 \u03c3R0 0.0525 0.5365 0.9818 1.5750 2.8887 \u03b10 2.2124 3.6443 4.8462 6.2334 9.1984 \u03c3R 0.0030 0.0336 0.0725 0.1313 0.3281 \u03c3k 0.5519 0.5626 0.5684 0.5744 0.5861 \u03b1 -0.4498 0.4618 0.7480 0.9520 1.3733 \u03c1 0.9037 0.9249 0.9350 0.9435 0.9598 display(plot(cc)) The posterior for the initial distribution of $R_{0,s}$ is not very precise. The other parameters have fairly precise posteriors. Systrom fixed all these parameters, except $\\sigma_R$, which he estimated by maximum likelihood to be 0.25. In these posteriors, a 95% credible region for $\\sigma_R$ contains his estimate. The posterior of $\\rho$ is not far from his imposed value of $1$, although $1$ is out of the 95% credible region. A 95% posterior region for $\\gamma$ contains Systrom\u2019s calibrated value of $1/7$. It is worth noting that the estimate of $\\sigma_k$ is large compared to $\\sigma_r$. This will cause new observations of $\\Delta \\log k$ will have a small effect on the posterior mean of $R$. Given values of the parameters, we can compute state and time specific posterior estimates of $R_{s,t}$. states = unique(sdf.state) s = findfirst(states.==\"New York\") figr = RT.plotpostr(dates[s],dlogk[s],post, ones(length(dlogk[s]),1), [1]) display(plot(figr, ylim=(-1,10))) This figure shows the posterior distribution of $R_{s,t}$ in New York. The black line is the posterior mean. The dark grey region is the average (over model parameters) of a 90% credible region conditional on the model parameters. This is comparable to what Systrom (and many others) report, and ignores uncertainty in the model parameters. The light grey region is a 90% credile region taking into account parameter uncertainty. The points and error bars are mean and 90% credible regions for \\Delta \\log k_{t}/\\gamma + 1 = R_{t} + \\epsilon_t/\\gamma","title":"State space model"},{"location":"Rt/#posteriors-for-additional-states","text":"states_to_plot = [\"New Jersey\",\"Massachusetts\",\"California\", \"Georgia\",\"Illinois\",\"Michigan\", \"Ohio\",\"Wisconsin\",\"Washington\"] S = length(states_to_plot) figs = fill(plot(), 9*(S \u00f7 9 + 1)) for (i,st) in enumerate(states_to_plot) s = findfirst(states.==st) figr = RT.plotpostr(dates[s],dlogk[s],post, ones(length(dlogk[s]),1),[1]) l = @layout [a{.1h}; grid(1,1)] figs[i] = plot(plot(annotation=(0.5,0.5, st), framestyle = :none), plot(figr, ylim=(-1,10)), layout=l) if ((i % 9) ==0 || ( i==length(states_to_plot))) display(plot(figs[(i-8):i]..., layout=(3,3), reuse=false)) end end We can see that the posteriors vary very little from state to state. The model picks up a general downward trend in $\\Delta \\log k$ through the slightly less than 1 estimate of $\\rho$. This drives the posteriors of $R_{s,t}$ in every state to decrease over time. Since $\\sigma_k >> \\sigma_R$, the actual realizations of $\\Delta \\log k$ do not affect the state-specific posteriors very much. Note I also tried fixing $\\rho=1$. This gives similar results in terms of $\\sigma_k >> \\sigma_R$, and gives a posterior for $R_{s,t}$ that is approximately constant over time. Bettencourt, Ruy M., Lu\u00eds M. A. AND Ribeiro. 2008. \u201cReal Time Bayesian Estimation of the Epidemic Potential of Emerging Infectious Diseases.\u201d PLOS ONE 3 (5): 1\u20139. https://doi.org/10.1371/journal.pone.0002185 . Kurtz, Vince, and Hai Lin. 2019. \u201cKalman Filtering with Gaussian Processes Measurement Noise.\u201d","title":"Posteriors for additional states"},{"location":"canada/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License using CovidSEIR, Plots, DataFrames, JLD2, StatsPlots, MCMCChains Plots.pyplot() jmddir = normpath(joinpath(dirname(Base.find_package(\"CovidSEIR\")),\"..\",\"docs\",\"jmd\")) covdf = covidjhudata(); Canada \u00b6 We estimate the model with the following code. It takes about an hour. canada = CountryData(covdf, \"Canada\"); using Turing canmod = CovidSEIR.TimeVarying.countrymodel(canada) cc = Turing.psample(canmod, NUTS(0.65), 5000, 4) import JLD2 JLD2.@save \"$jmddir/canada_$(Dates.today()).jld2\" cc JLD2.@load \"$jmddir/canada_dhmc_2020-04-13.jld2\" cc dayt0; cc = MCMCChains.Chains(collect(cc.value.data), replace.(cc.name_map.parameters, r\"([^\\[])([1-9])\" => s\"\\1[\\2]\")) Object of type Chains, with data of type 5000\u00d715\u00d74 Array{Float64,3} Iterations = 1:5000 Thinning interval = 1 Chains = 1, 2, 3, 4 Samples per chain = 5000 parameters = \u03c4, sigD, sigC, sigRc, a, pE0, p[1], p[2], \u03b2[1], \u03b2[2], \u03b2 [3], \u03b3[1], \u03b3[2], \u03c1[1], \u03c1[2] 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.2357 0.6185 0.0044 0.0434 80.3213 2.7437 sigD 6.5869 0.8252 0.0058 0.0488 84.3120 1.2775 sigC 663.0470 132.2966 0.9355 8.8404 80.3213 2.0702 sigRc 109.1233 8.8844 0.0628 0.4840 122.8527 1.1655 a 0.6428 0.2756 0.0019 0.0193 80.3213 2.8064 pE0 0.0000 0.0000 0.0000 0.0000 80.3213 1.8518 p[1] 0.0819 0.2048 0.0014 0.0143 80.3213 2.3034 p[2] 0.0054 0.0035 0.0000 0.0002 80.3213 4.0799 \u03b2[1] 0.2341 0.2845 0.0020 0.0186 80.3213 1.3055 \u03b2[2] 0.6854 0.5439 0.0038 0.0373 80.3213 1.8702 \u03b2[3] 3.4579 0.8677 0.0061 0.0608 80.3213 2.7532 \u03b3[1] 2.1268 1.0667 0.0075 0.0754 80.3213 4.6674 \u03b3[2] 0.0439 0.0157 0.0001 0.0011 80.3213 3.1084 \u03c1[1] 0.2345 0.3476 0.0025 0.0244 80.3213 4.3679 \u03c1[2] 56.7301 23.8913 0.1689 1.6324 80.3213 1.8519 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0001 0.0010 2.2230 sigD 5.2897 6.0577 6.3179 7.1150 8.5042 sigC 340.9755 643.1188 676.1973 738.0302 872.2481 sigRc 92.9782 103.0199 110.7701 112.5595 129.1059 a 0.1631 0.3985 0.6762 0.9309 0.9894 pE0 0.0000 0.0000 0.0000 0.0000 0.0000 p[1] 0.0028 0.0038 0.0044 0.0163 0.8283 p[2] 0.0039 0.0040 0.0041 0.0041 0.0163 \u03b2[1] 0.0000 0.0002 0.0995 0.4113 0.8889 \u03b2[2] 0.0241 0.1801 0.5443 1.3260 1.5558 \u03b2[3] 1.2217 3.3502 3.6691 3.9622 4.6629 \u03b3[1] 0.0171 2.2217 2.6733 2.7457 2.9976 \u03b3[2] 0.0338 0.0380 0.0381 0.0401 0.0932 \u03c1[1] 0.0000 0.0004 0.0050 0.5601 0.9289 \u03c1[2] 24.6373 24.7181 67.2189 71.7995 99.9910 Estimates \u00b6 plot(cc) We can see that there might be convergence issues. There are large differences between the four chains for some parameters. describe(cc) 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.2357 0.6185 0.0044 0.0434 80.3213 2.7437 sigD 6.5869 0.8252 0.0058 0.0488 84.3120 1.2775 sigC 663.0470 132.2966 0.9355 8.8404 80.3213 2.0702 sigRc 109.1233 8.8844 0.0628 0.4840 122.8527 1.1655 a 0.6428 0.2756 0.0019 0.0193 80.3213 2.8064 pE0 0.0000 0.0000 0.0000 0.0000 80.3213 1.8518 p[1] 0.0819 0.2048 0.0014 0.0143 80.3213 2.3034 p[2] 0.0054 0.0035 0.0000 0.0002 80.3213 4.0799 \u03b2[1] 0.2341 0.2845 0.0020 0.0186 80.3213 1.3055 \u03b2[2] 0.6854 0.5439 0.0038 0.0373 80.3213 1.8702 \u03b2[3] 3.4579 0.8677 0.0061 0.0608 80.3213 2.7532 \u03b3[1] 2.1268 1.0667 0.0075 0.0754 80.3213 4.6674 \u03b3[2] 0.0439 0.0157 0.0001 0.0011 80.3213 3.1084 \u03c1[1] 0.2345 0.3476 0.0025 0.0244 80.3213 4.3679 \u03c1[2] 56.7301 23.8913 0.1689 1.6324 80.3213 1.8519 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0001 0.0010 2.2230 sigD 5.2897 6.0577 6.3179 7.1150 8.5042 sigC 340.9755 643.1188 676.1973 738.0302 872.2481 sigRc 92.9782 103.0199 110.7701 112.5595 129.1059 a 0.1631 0.3985 0.6762 0.9309 0.9894 pE0 0.0000 0.0000 0.0000 0.0000 0.0000 p[1] 0.0028 0.0038 0.0044 0.0163 0.8283 p[2] 0.0039 0.0040 0.0041 0.0041 0.0163 \u03b2[1] 0.0000 0.0002 0.0995 0.4113 0.8889 \u03b2[2] 0.0241 0.1801 0.5443 1.3260 1.5558 \u03b2[3] 1.2217 3.3502 3.6691 3.9622 4.6629 \u03b3[1] 0.0171 2.2217 2.6733 2.7457 2.9976 \u03b3[2] 0.0338 0.0380 0.0381 0.0401 0.0932 \u03c1[1] 0.0000 0.0004 0.0050 0.5601 0.9289 \u03c1[2] 24.6373 24.7181 67.2189 71.7995 99.9910 The parameter estimates are generally not very precise. Fit \u00b6 sdf = simtrajectories(cc, canada, 1:200) f = plotvars(sdf, canada) f.fit In this figure, solid lines are observed data, dashed lines are posterior means, and the shaded region is a pointwise 90% credible interval. Implications \u00b6 We now look at the model\u2019s projections for some observed and unobserved variables. for fig in f.trajectories display(fig) end In general we see a similar pattern as noted above: the posteriors for observed variables are fairly precise. However, the posteriors for unobserved variables, such as the portion of undetected infections and the portion of mild infections, are very imprecise.","title":"Canada"},{"location":"canada/#canada","text":"We estimate the model with the following code. It takes about an hour. canada = CountryData(covdf, \"Canada\"); using Turing canmod = CovidSEIR.TimeVarying.countrymodel(canada) cc = Turing.psample(canmod, NUTS(0.65), 5000, 4) import JLD2 JLD2.@save \"$jmddir/canada_$(Dates.today()).jld2\" cc JLD2.@load \"$jmddir/canada_dhmc_2020-04-13.jld2\" cc dayt0; cc = MCMCChains.Chains(collect(cc.value.data), replace.(cc.name_map.parameters, r\"([^\\[])([1-9])\" => s\"\\1[\\2]\")) Object of type Chains, with data of type 5000\u00d715\u00d74 Array{Float64,3} Iterations = 1:5000 Thinning interval = 1 Chains = 1, 2, 3, 4 Samples per chain = 5000 parameters = \u03c4, sigD, sigC, sigRc, a, pE0, p[1], p[2], \u03b2[1], \u03b2[2], \u03b2 [3], \u03b3[1], \u03b3[2], \u03c1[1], \u03c1[2] 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.2357 0.6185 0.0044 0.0434 80.3213 2.7437 sigD 6.5869 0.8252 0.0058 0.0488 84.3120 1.2775 sigC 663.0470 132.2966 0.9355 8.8404 80.3213 2.0702 sigRc 109.1233 8.8844 0.0628 0.4840 122.8527 1.1655 a 0.6428 0.2756 0.0019 0.0193 80.3213 2.8064 pE0 0.0000 0.0000 0.0000 0.0000 80.3213 1.8518 p[1] 0.0819 0.2048 0.0014 0.0143 80.3213 2.3034 p[2] 0.0054 0.0035 0.0000 0.0002 80.3213 4.0799 \u03b2[1] 0.2341 0.2845 0.0020 0.0186 80.3213 1.3055 \u03b2[2] 0.6854 0.5439 0.0038 0.0373 80.3213 1.8702 \u03b2[3] 3.4579 0.8677 0.0061 0.0608 80.3213 2.7532 \u03b3[1] 2.1268 1.0667 0.0075 0.0754 80.3213 4.6674 \u03b3[2] 0.0439 0.0157 0.0001 0.0011 80.3213 3.1084 \u03c1[1] 0.2345 0.3476 0.0025 0.0244 80.3213 4.3679 \u03c1[2] 56.7301 23.8913 0.1689 1.6324 80.3213 1.8519 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0001 0.0010 2.2230 sigD 5.2897 6.0577 6.3179 7.1150 8.5042 sigC 340.9755 643.1188 676.1973 738.0302 872.2481 sigRc 92.9782 103.0199 110.7701 112.5595 129.1059 a 0.1631 0.3985 0.6762 0.9309 0.9894 pE0 0.0000 0.0000 0.0000 0.0000 0.0000 p[1] 0.0028 0.0038 0.0044 0.0163 0.8283 p[2] 0.0039 0.0040 0.0041 0.0041 0.0163 \u03b2[1] 0.0000 0.0002 0.0995 0.4113 0.8889 \u03b2[2] 0.0241 0.1801 0.5443 1.3260 1.5558 \u03b2[3] 1.2217 3.3502 3.6691 3.9622 4.6629 \u03b3[1] 0.0171 2.2217 2.6733 2.7457 2.9976 \u03b3[2] 0.0338 0.0380 0.0381 0.0401 0.0932 \u03c1[1] 0.0000 0.0004 0.0050 0.5601 0.9289 \u03c1[2] 24.6373 24.7181 67.2189 71.7995 99.9910","title":"Canada"},{"location":"canada/#estimates","text":"plot(cc) We can see that there might be convergence issues. There are large differences between the four chains for some parameters. describe(cc) 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.2357 0.6185 0.0044 0.0434 80.3213 2.7437 sigD 6.5869 0.8252 0.0058 0.0488 84.3120 1.2775 sigC 663.0470 132.2966 0.9355 8.8404 80.3213 2.0702 sigRc 109.1233 8.8844 0.0628 0.4840 122.8527 1.1655 a 0.6428 0.2756 0.0019 0.0193 80.3213 2.8064 pE0 0.0000 0.0000 0.0000 0.0000 80.3213 1.8518 p[1] 0.0819 0.2048 0.0014 0.0143 80.3213 2.3034 p[2] 0.0054 0.0035 0.0000 0.0002 80.3213 4.0799 \u03b2[1] 0.2341 0.2845 0.0020 0.0186 80.3213 1.3055 \u03b2[2] 0.6854 0.5439 0.0038 0.0373 80.3213 1.8702 \u03b2[3] 3.4579 0.8677 0.0061 0.0608 80.3213 2.7532 \u03b3[1] 2.1268 1.0667 0.0075 0.0754 80.3213 4.6674 \u03b3[2] 0.0439 0.0157 0.0001 0.0011 80.3213 3.1084 \u03c1[1] 0.2345 0.3476 0.0025 0.0244 80.3213 4.3679 \u03c1[2] 56.7301 23.8913 0.1689 1.6324 80.3213 1.8519 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0001 0.0010 2.2230 sigD 5.2897 6.0577 6.3179 7.1150 8.5042 sigC 340.9755 643.1188 676.1973 738.0302 872.2481 sigRc 92.9782 103.0199 110.7701 112.5595 129.1059 a 0.1631 0.3985 0.6762 0.9309 0.9894 pE0 0.0000 0.0000 0.0000 0.0000 0.0000 p[1] 0.0028 0.0038 0.0044 0.0163 0.8283 p[2] 0.0039 0.0040 0.0041 0.0041 0.0163 \u03b2[1] 0.0000 0.0002 0.0995 0.4113 0.8889 \u03b2[2] 0.0241 0.1801 0.5443 1.3260 1.5558 \u03b2[3] 1.2217 3.3502 3.6691 3.9622 4.6629 \u03b3[1] 0.0171 2.2217 2.6733 2.7457 2.9976 \u03b3[2] 0.0338 0.0380 0.0381 0.0401 0.0932 \u03c1[1] 0.0000 0.0004 0.0050 0.5601 0.9289 \u03c1[2] 24.6373 24.7181 67.2189 71.7995 99.9910 The parameter estimates are generally not very precise.","title":"Estimates"},{"location":"canada/#fit","text":"sdf = simtrajectories(cc, canada, 1:200) f = plotvars(sdf, canada) f.fit In this figure, solid lines are observed data, dashed lines are posterior means, and the shaded region is a pointwise 90% credible interval.","title":"Fit"},{"location":"canada/#implications","text":"We now look at the model\u2019s projections for some observed and unobserved variables. for fig in f.trajectories display(fig) end In general we see a similar pattern as noted above: the posteriors for observed variables are fairly precise. However, the posteriors for unobserved variables, such as the portion of undetected infections and the portion of mild infections, are very imprecise.","title":"Implications"},{"location":"china/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License using CovidSEIR, Plots, DataFrames, JLD2, StatsPlots, MCMCChains Plots.pyplot() jmddir = normpath(joinpath(dirname(Base.find_package(\"CovidSEIR\")),\"..\",\"docs\",\"jmd\")) covdf = covidjhudata(); China \u00b6 using Dates dayt0 = Dates.Date(\"2020-01-22\") - Dates.Day(65) china = CountryData(covdf, \"China\", 65) CovidSEIR.CountryData{Float64,Int64}(1.39273e9, [65, 66, 67, 68, 69, 70, 71 , 72, 73, 74 \u2026 141, 142, 143, 144, 145, 146, 147, 148, 149, 150], [17.0, 18.0, 26.0, 42.0, 56.0, 82.0, 131.0, 133.0, 171.0, 213.0 \u2026 3335.0, 3337.0 , 3339.0, 3340.0, 3343.0, 3343.0, 3345.0, 3345.0, 3346.0, 3346.0], [28.0, 3 0.0, 36.0, 39.0, 49.0, 58.0, 101.0, 120.0, 135.0, 214.0 \u2026 77410.0, 77567. 0, 77679.0, 77791.0, 77877.0, 77956.0, 78039.0, 78200.0, 78311.0, 78401.0], [503.0, 595.0, 858.0, 1325.0, 1970.0, 2737.0, 5277.0, 5834.0, 7835.0, 9375 .0 \u2026 1973.0, 1905.0, 1865.0, 1810.0, 1794.0, 1835.0, 1829.0, 1761.0, 1699 .0, 1656.0]) using Turing mdl = CovidSEIR.TimeVarying.countrymodel(china) cc = Turing.psample(mdl, NUTS(0.65), 5000, 4) import JLD2 JLD2.@save \"$jmddir/china_$(Dates.today()).jld2\" cc dayt0 JLD2.@load \"$jmddir/china_dhmc_2020-04-13.jld2\" cc dayt0; cc = MCMCChains.Chains(collect(cc.value.data), replace.(cc.name_map.parameters, r\"([^\\[])([1-9])\" => s\"\\1[\\2]\")) Object of type Chains, with data of type 5000\u00d715\u00d74 Array{Float64,3} Iterations = 1:5000 Thinning interval = 1 Chains = 1, 2, 3, 4 Samples per chain = 5000 parameters = \u03c4, sigD, sigC, sigRc, a, pE0, p[1], p[2], \u03b2[1], \u03b2[2], \u03b2 [3], \u03b3[1], \u03b3[2], \u03c1[1], \u03c1[2] 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0000 80.3213 1.5862 sigD 90.1865 108.1679 0.7649 7.6652 80.3213 16.4095 sigC 12749.7844 4659.6012 32.9484 322.0747 80.3213 4.5433 sigRc 8238.5858 3466.7500 24.5136 242.3239 80.3213 4.9360 a 0.7721 0.1378 0.0010 0.0069 120.2959 1.1108 pE0 0.0000 0.0000 0.0000 0.0000 80.3213 1.6474 p[1] 0.0004 0.0005 0.0000 0.0000 80.3213 7.1795 p[2] 0.4376 0.4398 0.0031 0.0309 80.3213 6.8857 \u03b2[1] 0.3384 0.3067 0.0022 0.0213 80.3213 2.0684 \u03b2[2] 0.7464 0.6525 0.0046 0.0298 124.5414 1.1639 \u03b2[3] 1.4304 1.4707 0.0104 0.1042 80.3213 9.1300 \u03b3[1] 1.2668 1.2386 0.0088 0.0878 80.3213 13.3651 \u03b3[2] 0.1599 0.1464 0.0010 0.0081 86.9155 1.3707 \u03c1[1] 0.8703 0.1652 0.0012 0.0117 80.3213 2.2865 \u03c1[2] 87.7994 1.4004 0.0099 0.0850 80.3213 1.5699 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0000 0.0000 sigD 23.6088 26.6456 28.8155 91.4797 292.9900 sigC 4388.5359 10075.1023 14738.3582 15853.7267 17853.3371 sigRc 4613.6964 5290.9410 6950.5976 10492.7804 14948.0661 a 0.4875 0.6742 0.7763 0.8873 0.9793 pE0 0.0000 0.0000 0.0000 0.0000 0.0000 p[1] 0.0000 0.0000 0.0001 0.0007 0.0014 p[2] 0.0021 0.0033 0.1933 0.8908 0.9903 \u03b2[1] 0.0269 0.1482 0.2338 0.3377 1.1417 \u03b2[2] 0.0000 0.2806 0.5279 1.1032 2.3880 \u03b2[3] 0.0530 0.1431 0.8618 2.9038 3.8533 \u03b3[1] 0.0819 0.0852 0.6694 2.5336 3.0000 \u03b3[2] 0.0336 0.0461 0.0849 0.2376 0.5292 \u03c1[1] 0.4234 0.7741 0.9711 0.9943 0.9995 \u03c1[2] 85.6977 86.6899 87.6515 88.7356 90.8120 Estimates \u00b6 plot(cc) describe(cc) 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0000 80.3213 1.5862 sigD 90.1865 108.1679 0.7649 7.6652 80.3213 16.4095 sigC 12749.7844 4659.6012 32.9484 322.0747 80.3213 4.5433 sigRc 8238.5858 3466.7500 24.5136 242.3239 80.3213 4.9360 a 0.7721 0.1378 0.0010 0.0069 120.2959 1.1108 pE0 0.0000 0.0000 0.0000 0.0000 80.3213 1.6474 p[1] 0.0004 0.0005 0.0000 0.0000 80.3213 7.1795 p[2] 0.4376 0.4398 0.0031 0.0309 80.3213 6.8857 \u03b2[1] 0.3384 0.3067 0.0022 0.0213 80.3213 2.0684 \u03b2[2] 0.7464 0.6525 0.0046 0.0298 124.5414 1.1639 \u03b2[3] 1.4304 1.4707 0.0104 0.1042 80.3213 9.1300 \u03b3[1] 1.2668 1.2386 0.0088 0.0878 80.3213 13.3651 \u03b3[2] 0.1599 0.1464 0.0010 0.0081 86.9155 1.3707 \u03c1[1] 0.8703 0.1652 0.0012 0.0117 80.3213 2.2865 \u03c1[2] 87.7994 1.4004 0.0099 0.0850 80.3213 1.5699 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0000 0.0000 sigD 23.6088 26.6456 28.8155 91.4797 292.9900 sigC 4388.5359 10075.1023 14738.3582 15853.7267 17853.3371 sigRc 4613.6964 5290.9410 6950.5976 10492.7804 14948.0661 a 0.4875 0.6742 0.7763 0.8873 0.9793 pE0 0.0000 0.0000 0.0000 0.0000 0.0000 p[1] 0.0000 0.0000 0.0001 0.0007 0.0014 p[2] 0.0021 0.0033 0.1933 0.8908 0.9903 \u03b2[1] 0.0269 0.1482 0.2338 0.3377 1.1417 \u03b2[2] 0.0000 0.2806 0.5279 1.1032 2.3880 \u03b2[3] 0.0530 0.1431 0.8618 2.9038 3.8533 \u03b3[1] 0.0819 0.0852 0.6694 2.5336 3.0000 \u03b3[2] 0.0336 0.0461 0.0849 0.2376 0.5292 \u03c1[1] 0.4234 0.7741 0.9711 0.9943 0.9995 \u03c1[2] 85.6977 86.6899 87.6515 88.7356 90.8120 Fit \u00b6 sdf = simtrajectories(cc, china, 1:150) f = plotvars(sdf, china, dayt0=dayt0) plot(f.fit, xlim=nothing, ylim=(0, maximum(china.active)*2)) Implications \u00b6 for fig in f.trajectories display(plot(fig, xlim=nothing)) end","title":"China"},{"location":"china/#china","text":"using Dates dayt0 = Dates.Date(\"2020-01-22\") - Dates.Day(65) china = CountryData(covdf, \"China\", 65) CovidSEIR.CountryData{Float64,Int64}(1.39273e9, [65, 66, 67, 68, 69, 70, 71 , 72, 73, 74 \u2026 141, 142, 143, 144, 145, 146, 147, 148, 149, 150], [17.0, 18.0, 26.0, 42.0, 56.0, 82.0, 131.0, 133.0, 171.0, 213.0 \u2026 3335.0, 3337.0 , 3339.0, 3340.0, 3343.0, 3343.0, 3345.0, 3345.0, 3346.0, 3346.0], [28.0, 3 0.0, 36.0, 39.0, 49.0, 58.0, 101.0, 120.0, 135.0, 214.0 \u2026 77410.0, 77567. 0, 77679.0, 77791.0, 77877.0, 77956.0, 78039.0, 78200.0, 78311.0, 78401.0], [503.0, 595.0, 858.0, 1325.0, 1970.0, 2737.0, 5277.0, 5834.0, 7835.0, 9375 .0 \u2026 1973.0, 1905.0, 1865.0, 1810.0, 1794.0, 1835.0, 1829.0, 1761.0, 1699 .0, 1656.0]) using Turing mdl = CovidSEIR.TimeVarying.countrymodel(china) cc = Turing.psample(mdl, NUTS(0.65), 5000, 4) import JLD2 JLD2.@save \"$jmddir/china_$(Dates.today()).jld2\" cc dayt0 JLD2.@load \"$jmddir/china_dhmc_2020-04-13.jld2\" cc dayt0; cc = MCMCChains.Chains(collect(cc.value.data), replace.(cc.name_map.parameters, r\"([^\\[])([1-9])\" => s\"\\1[\\2]\")) Object of type Chains, with data of type 5000\u00d715\u00d74 Array{Float64,3} Iterations = 1:5000 Thinning interval = 1 Chains = 1, 2, 3, 4 Samples per chain = 5000 parameters = \u03c4, sigD, sigC, sigRc, a, pE0, p[1], p[2], \u03b2[1], \u03b2[2], \u03b2 [3], \u03b3[1], \u03b3[2], \u03c1[1], \u03c1[2] 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0000 80.3213 1.5862 sigD 90.1865 108.1679 0.7649 7.6652 80.3213 16.4095 sigC 12749.7844 4659.6012 32.9484 322.0747 80.3213 4.5433 sigRc 8238.5858 3466.7500 24.5136 242.3239 80.3213 4.9360 a 0.7721 0.1378 0.0010 0.0069 120.2959 1.1108 pE0 0.0000 0.0000 0.0000 0.0000 80.3213 1.6474 p[1] 0.0004 0.0005 0.0000 0.0000 80.3213 7.1795 p[2] 0.4376 0.4398 0.0031 0.0309 80.3213 6.8857 \u03b2[1] 0.3384 0.3067 0.0022 0.0213 80.3213 2.0684 \u03b2[2] 0.7464 0.6525 0.0046 0.0298 124.5414 1.1639 \u03b2[3] 1.4304 1.4707 0.0104 0.1042 80.3213 9.1300 \u03b3[1] 1.2668 1.2386 0.0088 0.0878 80.3213 13.3651 \u03b3[2] 0.1599 0.1464 0.0010 0.0081 86.9155 1.3707 \u03c1[1] 0.8703 0.1652 0.0012 0.0117 80.3213 2.2865 \u03c1[2] 87.7994 1.4004 0.0099 0.0850 80.3213 1.5699 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0000 0.0000 sigD 23.6088 26.6456 28.8155 91.4797 292.9900 sigC 4388.5359 10075.1023 14738.3582 15853.7267 17853.3371 sigRc 4613.6964 5290.9410 6950.5976 10492.7804 14948.0661 a 0.4875 0.6742 0.7763 0.8873 0.9793 pE0 0.0000 0.0000 0.0000 0.0000 0.0000 p[1] 0.0000 0.0000 0.0001 0.0007 0.0014 p[2] 0.0021 0.0033 0.1933 0.8908 0.9903 \u03b2[1] 0.0269 0.1482 0.2338 0.3377 1.1417 \u03b2[2] 0.0000 0.2806 0.5279 1.1032 2.3880 \u03b2[3] 0.0530 0.1431 0.8618 2.9038 3.8533 \u03b3[1] 0.0819 0.0852 0.6694 2.5336 3.0000 \u03b3[2] 0.0336 0.0461 0.0849 0.2376 0.5292 \u03c1[1] 0.4234 0.7741 0.9711 0.9943 0.9995 \u03c1[2] 85.6977 86.6899 87.6515 88.7356 90.8120","title":"China"},{"location":"china/#estimates","text":"plot(cc) describe(cc) 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0000 80.3213 1.5862 sigD 90.1865 108.1679 0.7649 7.6652 80.3213 16.4095 sigC 12749.7844 4659.6012 32.9484 322.0747 80.3213 4.5433 sigRc 8238.5858 3466.7500 24.5136 242.3239 80.3213 4.9360 a 0.7721 0.1378 0.0010 0.0069 120.2959 1.1108 pE0 0.0000 0.0000 0.0000 0.0000 80.3213 1.6474 p[1] 0.0004 0.0005 0.0000 0.0000 80.3213 7.1795 p[2] 0.4376 0.4398 0.0031 0.0309 80.3213 6.8857 \u03b2[1] 0.3384 0.3067 0.0022 0.0213 80.3213 2.0684 \u03b2[2] 0.7464 0.6525 0.0046 0.0298 124.5414 1.1639 \u03b2[3] 1.4304 1.4707 0.0104 0.1042 80.3213 9.1300 \u03b3[1] 1.2668 1.2386 0.0088 0.0878 80.3213 13.3651 \u03b3[2] 0.1599 0.1464 0.0010 0.0081 86.9155 1.3707 \u03c1[1] 0.8703 0.1652 0.0012 0.0117 80.3213 2.2865 \u03c1[2] 87.7994 1.4004 0.0099 0.0850 80.3213 1.5699 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0000 0.0000 sigD 23.6088 26.6456 28.8155 91.4797 292.9900 sigC 4388.5359 10075.1023 14738.3582 15853.7267 17853.3371 sigRc 4613.6964 5290.9410 6950.5976 10492.7804 14948.0661 a 0.4875 0.6742 0.7763 0.8873 0.9793 pE0 0.0000 0.0000 0.0000 0.0000 0.0000 p[1] 0.0000 0.0000 0.0001 0.0007 0.0014 p[2] 0.0021 0.0033 0.1933 0.8908 0.9903 \u03b2[1] 0.0269 0.1482 0.2338 0.3377 1.1417 \u03b2[2] 0.0000 0.2806 0.5279 1.1032 2.3880 \u03b2[3] 0.0530 0.1431 0.8618 2.9038 3.8533 \u03b3[1] 0.0819 0.0852 0.6694 2.5336 3.0000 \u03b3[2] 0.0336 0.0461 0.0849 0.2376 0.5292 \u03c1[1] 0.4234 0.7741 0.9711 0.9943 0.9995 \u03c1[2] 85.6977 86.6899 87.6515 88.7356 90.8120","title":"Estimates"},{"location":"china/#fit","text":"sdf = simtrajectories(cc, china, 1:150) f = plotvars(sdf, china, dayt0=dayt0) plot(f.fit, xlim=nothing, ylim=(0, maximum(china.active)*2))","title":"Fit"},{"location":"china/#implications","text":"for fig in f.trajectories display(plot(fig, xlim=nothing)) end","title":"Implications"},{"location":"covid/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License using CovidSEIR using Plots Plots.pyplot() using DataFrames, JLD2 jmddir = normpath(joinpath(dirname(Base.find_package(\"CovidSEIR\")),\"..\",\"docs\",\"jmd\")) \"/home/paul/.julia/dev/CovidSEIR/docs/jmd\" Introduction \u00b6 Data \u00b6 We will use data from Johns Hopkins University Center for Systems Science and Engineering . It is gathered from a variety of sources and updated daily. JHU CSSE uses the data for this interactive website. . For another course, I wrote some notes using this data in python here. This data has daily cumulative counts of confirmed cases, recoveries, and deaths in each country (and provinces within some countries). covdf = covidjhudata() describe(covdf) 11\u00d78 DataFrames.DataFrame. Omitted printing of 2 columns \u2502 Row \u2502 variable \u2502 mean \u2502 min \u2502 median \u2502 max \u2502 nuni que \u2502 \u2502 \u2502 Symbol \u2502 Union\u2026 \u2502 Any \u2502 Union\u2026 \u2502 Any \u2502 Unio n\u2026 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2524 \u2502 1 \u2502 Date \u2502 \u2502 2020-01-22 \u2502 \u2502 2020-04-15 \u2502 85 \u2502 \u2502 2 \u2502 confirmed \u2502 1481.88 \u2502 -1 \u2502 2.0 \u2502 636350 \u2502 \u2502 \u2502 3 \u2502 Province \u2502 \u2502 Alberta \u2502 \u2502 Zhejiang \u2502 82 \u2502 \u2502 4 \u2502 Country \u2502 \u2502 Afghanistan \u2502 \u2502 Zimbabwe \u2502 185 \u2502 \u2502 5 \u2502 Lat \u2502 21.1709 \u2502 -51.7963 \u2502 22.3 \u2502 71.7069 \u2502 \u2502 \u2502 6 \u2502 Long \u2502 21.7781 \u2502 -135.0 \u2502 20.903 \u2502 178.065 \u2502 \u2502 \u2502 7 \u2502 deaths \u2502 78.9457 \u2502 -1 \u2502 0.0 \u2502 28326 \u2502 \u2502 \u2502 8 \u2502 recovered \u2502 381.939 \u2502 0 \u2502 0.0 \u2502 72600 \u2502 \u2502 \u2502 9 \u2502 iso2c \u2502 \u2502 AD \u2502 \u2502 ZW \u2502 178 \u2502 \u2502 10 \u2502 cpop \u2502 2.25233e8 \u2502 33785.0 \u2502 2.49924e7 \u2502 1.39273e9 \u2502 \u2502 \u2502 11 \u2502 ppop \u2502 2.74801e7 \u2502 41078 \u2502 1.557e7 \u2502 111690000 \u2502 \u2502 Model \u00b6 We will estimate a susceptible, exposed, infectious, recovered (SEIR) epidemiological model of Covid transmission. In particular, we will use a version based on this webapp by Allison Hill . The model contains the following variables, all of which are functions of time $S$: Susceptible individuals $E$: Exposed individuals - infected but not yet infectious or symptomatic $I_i$: Undetected infected individuals in severity class $i$. Severity increaes with $i$ and we assume individuals must pass through all previous classes $I_1$: Mild infection $I_2$: Severe infection $C_i$ confirmed infected individuals in severity class $i$ $R = R_u + R_c$: individuals who have recovered from disease and are now immune $R_u$ recovered individuals whose infection were never detected $R_c$ recovered individuals who were confirmed cases $X$: Dead individuals Compared to Hill\u2019s model, we have reduced the number of severity classes and from 3 to 2, and we have added undetected infections and recoveries. In the data, we observe active confirmed cases $\\approx \\sum_i C_i$, deaths $\\approx X$, and confirmed recoveries $\\approx R_c$. These variables evolve according to the following system of differential equations. \\begin{align*} \\dot{S} & = -S \\left( R(t)(\\beta_1C_1 + (\\beta_1 + \\beta_3)I_1) + C_2 \\beta_2 \\right)/N \\\\ \\dot{E} & = S \\left( R(t)(\\beta_1C_1 + (\\beta_1 + \\beta_3)I_1) + C_2 \\beta_2 \\right)/N - a E \\\\ \\dot{I_1} & = a E - \\gamma_1 I_1 - p_1 I_1 - \\tau C_1 \\\\ \\dot{I_2} & = 0 \\\\ \\dot{C_1} & = \\tau I - \\gamma_1 C_1 - p_1 C_1 \\\\ \\dot{C_2} & = p_1(I_1 + C_1) - \\gamma_2 C_2 - p_2 C_2 \\\\ \\dot{R_u} & = \\sum_i \\gamma_i I_i \\\\ \\dot{R_c} & = \\sum_i \\gamma_i C_i \\\\ \\dot{X} & = p_2 C_2 \\end{align*} Where the parameters are : $\\beta_1$, $\\beta_2$ baseline rate at which confirmed infected individuals in class $i$ contact susceptibles and infect them $\\beta_1+\\beta_3$ baseline rate at which undetected infected individuals infect others $R(t)$ reduction in infection rate due to isolation, quarantine, and/or lockdown $a$ rate of progression from the exposed to infected class $\\gamma_i$ rate at which infected individuals in class $i$ recover from disease and become immune $p_1$ rate at which infected individuals in class $i$ progress to class $i+1$ $p_2$ death rate for individuals in the most severe stage of disease $\\tau$ rate at which infections of class $1$ are detected Note that we are assuming that all severe infections are detected (and hence $I_2 = 0$). We are also assuming that confirmed and unconfirmed cases have the same recovery and progression rates. Empirical Model \u00b6 Our data has country population, $N$, daily cumulative confirmed cases, $c_t$, deaths, $d_t$, and recoveries, $r_t$. We will assume that at a known time 0, there is an unknown portion of exposed individuals, $p_0$, so \\begin{align*} S(0) = & (1-p_0) N \\\\ E(0) = & p_0 N \\end{align*} and all other model variables are 0 at time 0. We assume that the observed data is distributed as \\begin{align*} d_t \\sim & N(X(t), \\sigma_X^2) \\\\ r_t \\sim & N(R_c(t), \\sigma_R^2) \\\\ c_t - d_t - r_t \\sim & N(C_1(t) + C_2(t), \\sigma_C^2) \\end{align*} We parameterize the reduction in infection rates from public policy as R(t) = 1- \\frac{\\rho_1}{1+\\exp(\\rho_2 -t)} This implies that infection rates drop by roughly 100$\\rho_1$ percent in the week centered on $t=\\rho_2$. Model Limitations \u00b6 An important limitation is that the model assumes all other parameters are constant over time. Although we allow changes in the infection rate, efforts have also been made to increase $\\tau$. Innovations in treatment and crowding of the medical system likely lead to variation in $\\gamma$ and $p$. Single Country Estimates \u00b6 Priors \u00b6 We use the follow prior distributions. The means of these are loosely based on Hill\u2019s defaults . using Distributions defaultcountrypriors() = Dict( \"a\" => truncated(Normal(1/5, 3), 1/14, 1.0), \"p[1]\" => truncated(Normal(0.05, 0.3), 0, 1), \"p[2]\" => truncated(Normal(0.05, 0.3), 0, 1), \"\u03b3[1]\" => truncated(Normal(0.133, 0.5), 0, 3), \"\u03b3[2]\" => truncated(Normal(0.05, 0.3), 0, 1), \"\u03b2[1]\" => truncated(Normal(0.5, 1), 0, 10), \"\u03b2[2]\" => truncated(Normal(0.5, 1), 0, 10), \"\u03c4\" => truncated(Normal(0.2, 2), 0, 10), \"pE0\" => truncated(Normal(0.001, 0.1), 0, 1), \"sigD\" => InverseGamma(2,3), \"sigC\" => InverseGamma(2,3), \"sigRc\" => InverseGamma(2,3), \"\u03c1[1]\" => truncated(Normal(0.5, 2), 0, 1), \"\u03c1[2]\" => truncated(Normal(30, 30), 0, 100) Summary statistics of draws from this prior distribution are below. priors = CovidSEIR.TimeVarying.defaultpriors() population=1e6 T = 150 ode = CovidSEIR.TimeVarying.odeSEIR() model=CovidSEIR.TimeVarying.turingmodel1(population, 1:T, missing, missing, missing,ode, priors); pr = CovidSEIR.priorreport(priors, 150,population,model=model) pr.tbl 15\u00d76 DataFrames.DataFrame \u2502 Row \u2502 parameter \u2502 mean \u2502 stddev \u2502 q5 \u2502 q50 \u2502 q95 \u2502 \u2502 \u2502 Any \u2502 Any \u2502 Any \u2502 Any \u2502 Any \u2502 Any \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2524 \u2502 1 \u2502 a \u2502 0.532228 \u2502 0.267655 \u2502 0.116771 \u2502 0.534005 \u2502 0.9516 6 \u2502 \u2502 2 \u2502 pE0 \u2502 0.0797192 \u2502 0.0601517 \u2502 0.00642362 \u2502 0.0676311 \u2502 0.1964 49 \u2502 \u2502 3 \u2502 p[1] \u2502 0.259243 \u2502 0.188619 \u2502 0.0212131 \u2502 0.223932 \u2502 0.6150 81 \u2502 \u2502 4 \u2502 p[2] \u2502 0.255765 \u2502 0.187429 \u2502 0.0199915 \u2502 0.21967 \u2502 0.6183 19 \u2502 \u2502 5 \u2502 sigC \u2502 3.06972 \u2502 7.54916 \u2502 0.635259 \u2502 1.78636 \u2502 8.5077 7 \u2502 \u2502 6 \u2502 sigD \u2502 2.95921 \u2502 6.21134 \u2502 0.622761 \u2502 1.76887 \u2502 8.3176 1 \u2502 \u2502 7 \u2502 sigRc \u2502 2.9368 \u2502 5.06829 \u2502 0.626819 \u2502 1.73917 \u2502 8.3492 8 \u2502 \u2502 8 \u2502 \u03b2[1] \u2502 1.00155 \u2502 0.697947 \u2502 0.0940284 \u2502 0.895416 \u2502 2.2893 \u2502 \u2502 9 \u2502 \u03b2[2] \u2502 1.00996 \u2502 0.698382 \u2502 0.100554 \u2502 0.890034 \u2502 2.3079 8 \u2502 \u2502 10 \u2502 \u03b2[3] \u2502 1.00707 \u2502 0.695008 \u2502 0.0919612 \u2502 0.896013 \u2502 2.3323 8 \u2502 \u2502 11 \u2502 \u03b3[1] \u2502 0.451627 \u2502 0.327572 \u2502 0.0399892 \u2502 0.389729 \u2502 1.0706 3 \u2502 \u2502 12 \u2502 \u03b3[2] \u2502 0.25761 \u2502 0.188806 \u2502 0.021021 \u2502 0.221721 \u2502 0.6254 96 \u2502 \u2502 13 \u2502 \u03c1[1] \u2502 0.500063 \u2502 0.287893 \u2502 0.0505366 \u2502 0.497889 \u2502 0.9526 98 \u2502 \u2502 14 \u2502 \u03c1[2] \u2502 37.6915 \u2502 22.4854 \u2502 4.90265 \u2502 35.701 \u2502 78.667 1 \u2502 \u2502 15 \u2502 \u03c4 \u2502 1.64859 \u2502 1.22829 \u2502 0.135559 \u2502 1.39673 \u2502 4.0162 \u2502 The following plots show the implications of this prior for the observed data. The faint lines on each figure shows 1000 trajectories sampled from the prior distribution. The black line is the prior mean. The shaded region is a pointwise 90% prior credible interval. plot(pr.figs[1], xlabel=\"Days\", ylabel=\"Portion of population\") plot(pr.figs[2], xlabel=\"Days\", ylabel=\"Portion of population\") plot(pr.figs[3], xlabel=\"Days\", ylabel=\"Portion of population\") Subjectively this prior seems reasonable. It is perhaps too concentrated on relatively fast epidemics. I may alter it, but it\u2019s what I used for the current results. Estimation \u00b6 We estimate the model by MCMC. Specifically, we use the Turing.jl package (Ge, Xu, and Ghahramani 2018 ) . For sampling, we use the No-U-Turn-Sampler variant of Hamiltonian Monte Carlo. In the results below we use 4 chains with 1000 warmup iterations, and 4000 iterations for the results. Results \u00b6 Canada Italy South Korea China United States Extensions \u00b6 Improve chain mixing. Estimate a multi-country model with some parameters common across countries and others multi-level distributions. About this document \u00b6 This document was created using Weave.jl. The code is available in on github . Ge, Hong, Kai Xu, and Zoubin Ghahramani. 2018. \u201cTuring: A Language for Flexible Probabilistic Inference.\u201d In International Conference on Artificial Intelligence and Statistics, AISTATS 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain , 1682\u201390. http://proceedings.mlr.press/v84/ge18b.html .","title":"Model"},{"location":"covid/#introduction","text":"","title":"Introduction"},{"location":"covid/#data","text":"We will use data from Johns Hopkins University Center for Systems Science and Engineering . It is gathered from a variety of sources and updated daily. JHU CSSE uses the data for this interactive website. . For another course, I wrote some notes using this data in python here. This data has daily cumulative counts of confirmed cases, recoveries, and deaths in each country (and provinces within some countries). covdf = covidjhudata() describe(covdf) 11\u00d78 DataFrames.DataFrame. Omitted printing of 2 columns \u2502 Row \u2502 variable \u2502 mean \u2502 min \u2502 median \u2502 max \u2502 nuni que \u2502 \u2502 \u2502 Symbol \u2502 Union\u2026 \u2502 Any \u2502 Union\u2026 \u2502 Any \u2502 Unio n\u2026 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2524 \u2502 1 \u2502 Date \u2502 \u2502 2020-01-22 \u2502 \u2502 2020-04-15 \u2502 85 \u2502 \u2502 2 \u2502 confirmed \u2502 1481.88 \u2502 -1 \u2502 2.0 \u2502 636350 \u2502 \u2502 \u2502 3 \u2502 Province \u2502 \u2502 Alberta \u2502 \u2502 Zhejiang \u2502 82 \u2502 \u2502 4 \u2502 Country \u2502 \u2502 Afghanistan \u2502 \u2502 Zimbabwe \u2502 185 \u2502 \u2502 5 \u2502 Lat \u2502 21.1709 \u2502 -51.7963 \u2502 22.3 \u2502 71.7069 \u2502 \u2502 \u2502 6 \u2502 Long \u2502 21.7781 \u2502 -135.0 \u2502 20.903 \u2502 178.065 \u2502 \u2502 \u2502 7 \u2502 deaths \u2502 78.9457 \u2502 -1 \u2502 0.0 \u2502 28326 \u2502 \u2502 \u2502 8 \u2502 recovered \u2502 381.939 \u2502 0 \u2502 0.0 \u2502 72600 \u2502 \u2502 \u2502 9 \u2502 iso2c \u2502 \u2502 AD \u2502 \u2502 ZW \u2502 178 \u2502 \u2502 10 \u2502 cpop \u2502 2.25233e8 \u2502 33785.0 \u2502 2.49924e7 \u2502 1.39273e9 \u2502 \u2502 \u2502 11 \u2502 ppop \u2502 2.74801e7 \u2502 41078 \u2502 1.557e7 \u2502 111690000 \u2502 \u2502","title":"Data"},{"location":"covid/#model","text":"We will estimate a susceptible, exposed, infectious, recovered (SEIR) epidemiological model of Covid transmission. In particular, we will use a version based on this webapp by Allison Hill . The model contains the following variables, all of which are functions of time $S$: Susceptible individuals $E$: Exposed individuals - infected but not yet infectious or symptomatic $I_i$: Undetected infected individuals in severity class $i$. Severity increaes with $i$ and we assume individuals must pass through all previous classes $I_1$: Mild infection $I_2$: Severe infection $C_i$ confirmed infected individuals in severity class $i$ $R = R_u + R_c$: individuals who have recovered from disease and are now immune $R_u$ recovered individuals whose infection were never detected $R_c$ recovered individuals who were confirmed cases $X$: Dead individuals Compared to Hill\u2019s model, we have reduced the number of severity classes and from 3 to 2, and we have added undetected infections and recoveries. In the data, we observe active confirmed cases $\\approx \\sum_i C_i$, deaths $\\approx X$, and confirmed recoveries $\\approx R_c$. These variables evolve according to the following system of differential equations. \\begin{align*} \\dot{S} & = -S \\left( R(t)(\\beta_1C_1 + (\\beta_1 + \\beta_3)I_1) + C_2 \\beta_2 \\right)/N \\\\ \\dot{E} & = S \\left( R(t)(\\beta_1C_1 + (\\beta_1 + \\beta_3)I_1) + C_2 \\beta_2 \\right)/N - a E \\\\ \\dot{I_1} & = a E - \\gamma_1 I_1 - p_1 I_1 - \\tau C_1 \\\\ \\dot{I_2} & = 0 \\\\ \\dot{C_1} & = \\tau I - \\gamma_1 C_1 - p_1 C_1 \\\\ \\dot{C_2} & = p_1(I_1 + C_1) - \\gamma_2 C_2 - p_2 C_2 \\\\ \\dot{R_u} & = \\sum_i \\gamma_i I_i \\\\ \\dot{R_c} & = \\sum_i \\gamma_i C_i \\\\ \\dot{X} & = p_2 C_2 \\end{align*} Where the parameters are : $\\beta_1$, $\\beta_2$ baseline rate at which confirmed infected individuals in class $i$ contact susceptibles and infect them $\\beta_1+\\beta_3$ baseline rate at which undetected infected individuals infect others $R(t)$ reduction in infection rate due to isolation, quarantine, and/or lockdown $a$ rate of progression from the exposed to infected class $\\gamma_i$ rate at which infected individuals in class $i$ recover from disease and become immune $p_1$ rate at which infected individuals in class $i$ progress to class $i+1$ $p_2$ death rate for individuals in the most severe stage of disease $\\tau$ rate at which infections of class $1$ are detected Note that we are assuming that all severe infections are detected (and hence $I_2 = 0$). We are also assuming that confirmed and unconfirmed cases have the same recovery and progression rates.","title":"Model"},{"location":"covid/#empirical-model","text":"Our data has country population, $N$, daily cumulative confirmed cases, $c_t$, deaths, $d_t$, and recoveries, $r_t$. We will assume that at a known time 0, there is an unknown portion of exposed individuals, $p_0$, so \\begin{align*} S(0) = & (1-p_0) N \\\\ E(0) = & p_0 N \\end{align*} and all other model variables are 0 at time 0. We assume that the observed data is distributed as \\begin{align*} d_t \\sim & N(X(t), \\sigma_X^2) \\\\ r_t \\sim & N(R_c(t), \\sigma_R^2) \\\\ c_t - d_t - r_t \\sim & N(C_1(t) + C_2(t), \\sigma_C^2) \\end{align*} We parameterize the reduction in infection rates from public policy as R(t) = 1- \\frac{\\rho_1}{1+\\exp(\\rho_2 -t)} This implies that infection rates drop by roughly 100$\\rho_1$ percent in the week centered on $t=\\rho_2$.","title":"Empirical Model"},{"location":"covid/#model-limitations","text":"An important limitation is that the model assumes all other parameters are constant over time. Although we allow changes in the infection rate, efforts have also been made to increase $\\tau$. Innovations in treatment and crowding of the medical system likely lead to variation in $\\gamma$ and $p$.","title":"Model Limitations"},{"location":"covid/#single-country-estimates","text":"","title":"Single Country Estimates"},{"location":"covid/#priors","text":"We use the follow prior distributions. The means of these are loosely based on Hill\u2019s defaults . using Distributions defaultcountrypriors() = Dict( \"a\" => truncated(Normal(1/5, 3), 1/14, 1.0), \"p[1]\" => truncated(Normal(0.05, 0.3), 0, 1), \"p[2]\" => truncated(Normal(0.05, 0.3), 0, 1), \"\u03b3[1]\" => truncated(Normal(0.133, 0.5), 0, 3), \"\u03b3[2]\" => truncated(Normal(0.05, 0.3), 0, 1), \"\u03b2[1]\" => truncated(Normal(0.5, 1), 0, 10), \"\u03b2[2]\" => truncated(Normal(0.5, 1), 0, 10), \"\u03c4\" => truncated(Normal(0.2, 2), 0, 10), \"pE0\" => truncated(Normal(0.001, 0.1), 0, 1), \"sigD\" => InverseGamma(2,3), \"sigC\" => InverseGamma(2,3), \"sigRc\" => InverseGamma(2,3), \"\u03c1[1]\" => truncated(Normal(0.5, 2), 0, 1), \"\u03c1[2]\" => truncated(Normal(30, 30), 0, 100) Summary statistics of draws from this prior distribution are below. priors = CovidSEIR.TimeVarying.defaultpriors() population=1e6 T = 150 ode = CovidSEIR.TimeVarying.odeSEIR() model=CovidSEIR.TimeVarying.turingmodel1(population, 1:T, missing, missing, missing,ode, priors); pr = CovidSEIR.priorreport(priors, 150,population,model=model) pr.tbl 15\u00d76 DataFrames.DataFrame \u2502 Row \u2502 parameter \u2502 mean \u2502 stddev \u2502 q5 \u2502 q50 \u2502 q95 \u2502 \u2502 \u2502 Any \u2502 Any \u2502 Any \u2502 Any \u2502 Any \u2502 Any \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2524 \u2502 1 \u2502 a \u2502 0.532228 \u2502 0.267655 \u2502 0.116771 \u2502 0.534005 \u2502 0.9516 6 \u2502 \u2502 2 \u2502 pE0 \u2502 0.0797192 \u2502 0.0601517 \u2502 0.00642362 \u2502 0.0676311 \u2502 0.1964 49 \u2502 \u2502 3 \u2502 p[1] \u2502 0.259243 \u2502 0.188619 \u2502 0.0212131 \u2502 0.223932 \u2502 0.6150 81 \u2502 \u2502 4 \u2502 p[2] \u2502 0.255765 \u2502 0.187429 \u2502 0.0199915 \u2502 0.21967 \u2502 0.6183 19 \u2502 \u2502 5 \u2502 sigC \u2502 3.06972 \u2502 7.54916 \u2502 0.635259 \u2502 1.78636 \u2502 8.5077 7 \u2502 \u2502 6 \u2502 sigD \u2502 2.95921 \u2502 6.21134 \u2502 0.622761 \u2502 1.76887 \u2502 8.3176 1 \u2502 \u2502 7 \u2502 sigRc \u2502 2.9368 \u2502 5.06829 \u2502 0.626819 \u2502 1.73917 \u2502 8.3492 8 \u2502 \u2502 8 \u2502 \u03b2[1] \u2502 1.00155 \u2502 0.697947 \u2502 0.0940284 \u2502 0.895416 \u2502 2.2893 \u2502 \u2502 9 \u2502 \u03b2[2] \u2502 1.00996 \u2502 0.698382 \u2502 0.100554 \u2502 0.890034 \u2502 2.3079 8 \u2502 \u2502 10 \u2502 \u03b2[3] \u2502 1.00707 \u2502 0.695008 \u2502 0.0919612 \u2502 0.896013 \u2502 2.3323 8 \u2502 \u2502 11 \u2502 \u03b3[1] \u2502 0.451627 \u2502 0.327572 \u2502 0.0399892 \u2502 0.389729 \u2502 1.0706 3 \u2502 \u2502 12 \u2502 \u03b3[2] \u2502 0.25761 \u2502 0.188806 \u2502 0.021021 \u2502 0.221721 \u2502 0.6254 96 \u2502 \u2502 13 \u2502 \u03c1[1] \u2502 0.500063 \u2502 0.287893 \u2502 0.0505366 \u2502 0.497889 \u2502 0.9526 98 \u2502 \u2502 14 \u2502 \u03c1[2] \u2502 37.6915 \u2502 22.4854 \u2502 4.90265 \u2502 35.701 \u2502 78.667 1 \u2502 \u2502 15 \u2502 \u03c4 \u2502 1.64859 \u2502 1.22829 \u2502 0.135559 \u2502 1.39673 \u2502 4.0162 \u2502 The following plots show the implications of this prior for the observed data. The faint lines on each figure shows 1000 trajectories sampled from the prior distribution. The black line is the prior mean. The shaded region is a pointwise 90% prior credible interval. plot(pr.figs[1], xlabel=\"Days\", ylabel=\"Portion of population\") plot(pr.figs[2], xlabel=\"Days\", ylabel=\"Portion of population\") plot(pr.figs[3], xlabel=\"Days\", ylabel=\"Portion of population\") Subjectively this prior seems reasonable. It is perhaps too concentrated on relatively fast epidemics. I may alter it, but it\u2019s what I used for the current results.","title":"Priors"},{"location":"covid/#estimation","text":"We estimate the model by MCMC. Specifically, we use the Turing.jl package (Ge, Xu, and Ghahramani 2018 ) . For sampling, we use the No-U-Turn-Sampler variant of Hamiltonian Monte Carlo. In the results below we use 4 chains with 1000 warmup iterations, and 4000 iterations for the results.","title":"Estimation"},{"location":"covid/#results","text":"Canada Italy South Korea China United States","title":"Results"},{"location":"covid/#extensions","text":"Improve chain mixing. Estimate a multi-country model with some parameters common across countries and others multi-level distributions.","title":"Extensions"},{"location":"covid/#about-this-document","text":"This document was created using Weave.jl. The code is available in on github . Ge, Hong, Kai Xu, and Zoubin Ghahramani. 2018. \u201cTuring: A Language for Flexible Probabilistic Inference.\u201d In International Conference on Artificial Intelligence and Statistics, AISTATS 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands, Spain , 1682\u201390. http://proceedings.mlr.press/v84/ge18b.html .","title":"About this document"},{"location":"functions/","text":"Function Reference \u00b6 # CovidSEIR.countrymodel \u2014 Function . countrymodel(data::CountryData, priors=defaultcountrypriors(), ::Type{R}=Float64) where {R <: Real} = begni Returns Turing model for single country source # CovidSEIR.covidjhudata \u2014 Method . covidjhudata() Downloads most recent JHU CSSE data on covid cases, deaths, and recoveries. Returns a DataFrame source # CovidSEIR.plotvars \u2014 Method . plotvars(simdf::DataFrames.AbstractDataFrame, data::CountryData; dayt0=Dates.Date(\"2020-01-21\"), # one day before JHU data begins colors=ColorSchemes.colorschemes[:Set1_9]) Create plots showing fit and implications of simulated trajectories. source # CovidSEIR.priorreport \u2014 Function . priorreport((priors=defaultcountrypriors(), T=100, population=1e7) Create tables and figures summarizing priors. source # CovidSEIR.simtrajectories \u2014 Method . simtrajectories(cc::AbstractMCMC.AbstractChains, data::CountryData, ts; ic=Iterators.product(StatsBase.sample(1:size(cc,1),300, replace=false), 1:size(cc,3))) Simulate trajectories based on parameter values in chain cc . source # CovidSEIR.odeSEIR \u2014 Method . odeSEIR() Sets up ODE for SEIR model with unconfirmed cases. Returns an ODEProblem source # CovidSEIR.paramvars \u2014 Method . Transfrom ODE parameters to/from vector source # CovidSEIR.paramvec \u2014 Method . Transfrom ODE parameters to/from vector source # CovidSEIR.systemvars \u2014 Method . Transfrom ODE variables to/from vector source # CovidSEIR.systemvec \u2014 Method . Transfrom ODE variables to/from vector source Index \u00b6 CovidSEIR.countrymodel CovidSEIR.covidjhudata CovidSEIR.odeSEIR CovidSEIR.paramvars CovidSEIR.paramvec CovidSEIR.plotvars CovidSEIR.priorreport CovidSEIR.simtrajectories CovidSEIR.systemvars CovidSEIR.systemvec","title":"Functions"},{"location":"functions/#function-reference","text":"# CovidSEIR.countrymodel \u2014 Function . countrymodel(data::CountryData, priors=defaultcountrypriors(), ::Type{R}=Float64) where {R <: Real} = begni Returns Turing model for single country source # CovidSEIR.covidjhudata \u2014 Method . covidjhudata() Downloads most recent JHU CSSE data on covid cases, deaths, and recoveries. Returns a DataFrame source # CovidSEIR.plotvars \u2014 Method . plotvars(simdf::DataFrames.AbstractDataFrame, data::CountryData; dayt0=Dates.Date(\"2020-01-21\"), # one day before JHU data begins colors=ColorSchemes.colorschemes[:Set1_9]) Create plots showing fit and implications of simulated trajectories. source # CovidSEIR.priorreport \u2014 Function . priorreport((priors=defaultcountrypriors(), T=100, population=1e7) Create tables and figures summarizing priors. source # CovidSEIR.simtrajectories \u2014 Method . simtrajectories(cc::AbstractMCMC.AbstractChains, data::CountryData, ts; ic=Iterators.product(StatsBase.sample(1:size(cc,1),300, replace=false), 1:size(cc,3))) Simulate trajectories based on parameter values in chain cc . source # CovidSEIR.odeSEIR \u2014 Method . odeSEIR() Sets up ODE for SEIR model with unconfirmed cases. Returns an ODEProblem source # CovidSEIR.paramvars \u2014 Method . Transfrom ODE parameters to/from vector source # CovidSEIR.paramvec \u2014 Method . Transfrom ODE parameters to/from vector source # CovidSEIR.systemvars \u2014 Method . Transfrom ODE variables to/from vector source # CovidSEIR.systemvec \u2014 Method . Transfrom ODE variables to/from vector source","title":"Function Reference"},{"location":"functions/#index","text":"CovidSEIR.countrymodel CovidSEIR.covidjhudata CovidSEIR.odeSEIR CovidSEIR.paramvars CovidSEIR.paramvec CovidSEIR.plotvars CovidSEIR.priorreport CovidSEIR.simtrajectories CovidSEIR.systemvars CovidSEIR.systemvec","title":"Index"},{"location":"italy/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License using CovidSEIR, Plots, DataFrames, JLD2, StatsPlots, MCMCChains Plots.pyplot() jmddir = normpath(joinpath(dirname(Base.find_package(\"CovidSEIR\")),\"..\",\"docs\",\"jmd\")) covdf = covidjhudata(); Italy \u00b6 italy = CountryData(covdf, \"Italy\"); itmod = CovidSEIR.TimeVarying.countrymodel(italy) cc = Turing.psample(itmod, NUTS(0.65), 5000, 4) import JLD2 JLD2.@save \"$jmddir/italy_tv_$(Dates.today()).jld2\" cc JLD2.@load \"$jmddir/italy_dhmc_2020-04-13.jld2\" cc; Error: SystemError: opening file \"/home/paul/.julia/dev/CovidSEIR/docs/jmd/ italy_dhmc_2020-04-13.jld2\": No such file or directory cc = MCMCChains.Chains(collect(cc.value.data), replace.(cc.name_map.parameters, r\"([^\\[])([1-9])\" => s\"\\1[\\2]\")) Error: UndefVarError: cc not defined Estimates \u00b6 plot(cc) Error: UndefVarError: cc not defined describe(cc) Error: UndefVarError: cc not defined Fit \u00b6 sdf = simtrajectories(cc, italy, 1:200) Error: UndefVarError: cc not defined f = plotvars(sdf, italy) Error: UndefVarError: sdf not defined f.fit Error: UndefVarError: f not defined As with Canada, the fit is very good, and the posterior distribution of observables is very precise. Implications \u00b6 for fig in f.trajectories display(fig) end Error: UndefVarError: f not defined","title":"Italy"},{"location":"italy/#italy","text":"italy = CountryData(covdf, \"Italy\"); itmod = CovidSEIR.TimeVarying.countrymodel(italy) cc = Turing.psample(itmod, NUTS(0.65), 5000, 4) import JLD2 JLD2.@save \"$jmddir/italy_tv_$(Dates.today()).jld2\" cc JLD2.@load \"$jmddir/italy_dhmc_2020-04-13.jld2\" cc; Error: SystemError: opening file \"/home/paul/.julia/dev/CovidSEIR/docs/jmd/ italy_dhmc_2020-04-13.jld2\": No such file or directory cc = MCMCChains.Chains(collect(cc.value.data), replace.(cc.name_map.parameters, r\"([^\\[])([1-9])\" => s\"\\1[\\2]\")) Error: UndefVarError: cc not defined","title":"Italy"},{"location":"italy/#estimates","text":"plot(cc) Error: UndefVarError: cc not defined describe(cc) Error: UndefVarError: cc not defined","title":"Estimates"},{"location":"italy/#fit","text":"sdf = simtrajectories(cc, italy, 1:200) Error: UndefVarError: cc not defined f = plotvars(sdf, italy) Error: UndefVarError: sdf not defined f.fit Error: UndefVarError: f not defined As with Canada, the fit is very good, and the posterior distribution of observables is very precise.","title":"Fit"},{"location":"italy/#implications","text":"for fig in f.trajectories display(fig) end Error: UndefVarError: f not defined","title":"Implications"},{"location":"korea/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License using CovidSEIR, Plots, DataFrames, JLD2, StatsPlots, MCMCChains Plots.pyplot() jmddir = normpath(joinpath(dirname(Base.find_package(\"CovidSEIR\")),\"..\",\"docs\",\"jmd\")) covdf = covidjhudata(); South Korea \u00b6 korea = CountryData(covdf, \"Korea, South\") CovidSEIR.CountryData{Float64,Int64}(5.1635256e7, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10 \u2026 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], [0.0, 0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0, 0.0 \u2026 192.0, 200.0, 204.0, 208.0, 211.0, 214.0, 217 .0, 222.0, 225.0, 229.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 \u2026 6694.0, 6776.0, 6973.0, 7117.0, 7243.0, 7368.0, 7447.0, 7534.0, 7616.0 , 7757.0], [1.0, 1.0, 2.0, 2.0, 3.0, 4.0, 4.0, 4.0, 4.0, 11.0 \u2026 3445.0, 3 408.0, 3246.0, 3125.0, 3026.0, 2930.0, 2873.0, 2808.0, 2750.0, 2627.0]) using Turing mdl = CovidSEIR.TimeVarying.countrymodel(korea) cc = Turing.psample(mdl, NUTS(0.65), 5000, 4) import JLD2 JLD2.@save \"$jmddir/korea_tv_$(Dates.today()).jld2\" cc JLD2.@load \"$jmddir/korea_dhmc_2020-04-13.jld2\" cc dayt0; cc = MCMCChains.Chains(collect(cc.value.data), replace.(cc.name_map.parameters, r\"([^\\[])([1-9])\" => s\"\\1[\\2]\")) Object of type Chains, with data of type 5000\u00d715\u00d74 Array{Float64,3} Iterations = 1:5000 Thinning interval = 1 Chains = 1, 2, 3, 4 Samples per chain = 5000 parameters = \u03c4, sigD, sigC, sigRc, a, pE0, p[1], p[2], \u03b2[1], \u03b2[2], \u03b2 [3], \u03b3[1], \u03b3[2], \u03c1[1], \u03c1[2] 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0146 0.0597 0.0004 0.0041 80.3213 1.3408 sigD 5.3642 2.4216 0.0171 0.1694 80.3213 5.0384 sigC 951.3287 978.9917 6.9225 69.0719 80.3213 2.2197 sigRc 845.4857 105.0627 0.7429 5.9862 80.3213 1.4793 a 0.5215 0.2690 0.0019 0.0176 80.3213 1.8935 pE0 0.0002 0.0005 0.0000 0.0000 80.3213 1.4380 p[1] 0.0316 0.0626 0.0004 0.0044 80.3213 2.4790 p[2] 0.0031 0.0089 0.0001 0.0006 80.3213 1.2621 \u03b2[1] 0.7778 0.8358 0.0059 0.0555 80.3213 1.4458 \u03b2[2] 0.6354 0.7074 0.0050 0.0398 80.3370 1.2070 \u03b2[3] 2.0175 1.3794 0.0098 0.0966 80.3213 3.3188 \u03b3[1] 1.5137 1.1407 0.0081 0.0801 80.3213 3.0029 \u03b3[2] 0.0515 0.0970 0.0007 0.0068 80.3213 1.3837 \u03c1[1] 0.5549 0.2274 0.0016 0.0136 80.3213 1.3732 \u03c1[2] 47.0605 11.1717 0.0790 0.7350 80.3213 1.2892 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0001 0.2457 sigD 2.4786 2.9594 5.5550 7.6868 8.8722 sigC 217.4268 265.3300 550.1108 1232.7812 4068.7156 sigRc 673.8938 765.5432 836.1810 916.0544 1064.8168 a 0.0746 0.3490 0.5506 0.7427 0.9411 pE0 0.0000 0.0000 0.0000 0.0000 0.0016 p[1] 0.0000 0.0001 0.0003 0.0240 0.2330 p[2] 0.0008 0.0008 0.0008 0.0009 0.0354 \u03b2[1] 0.0000 0.0026 0.5025 1.2954 2.8868 \u03b2[2] 0.0000 0.0583 0.3820 1.0063 2.4575 \u03b2[3] 0.0405 0.7791 1.7061 3.3934 4.2446 \u03b3[1] 0.0316 0.5668 1.1303 3.0000 3.0000 \u03b3[2] 0.0214 0.0264 0.0275 0.0286 0.4204 \u03c1[1] 0.1889 0.3367 0.5769 0.7514 0.9503 \u03c1[2] 28.7635 38.8180 46.5332 53.4753 75.3812 Estimates \u00b6 plot(cc) describe(cc) 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0146 0.0597 0.0004 0.0041 80.3213 1.3408 sigD 5.3642 2.4216 0.0171 0.1694 80.3213 5.0384 sigC 951.3287 978.9917 6.9225 69.0719 80.3213 2.2197 sigRc 845.4857 105.0627 0.7429 5.9862 80.3213 1.4793 a 0.5215 0.2690 0.0019 0.0176 80.3213 1.8935 pE0 0.0002 0.0005 0.0000 0.0000 80.3213 1.4380 p[1] 0.0316 0.0626 0.0004 0.0044 80.3213 2.4790 p[2] 0.0031 0.0089 0.0001 0.0006 80.3213 1.2621 \u03b2[1] 0.7778 0.8358 0.0059 0.0555 80.3213 1.4458 \u03b2[2] 0.6354 0.7074 0.0050 0.0398 80.3370 1.2070 \u03b2[3] 2.0175 1.3794 0.0098 0.0966 80.3213 3.3188 \u03b3[1] 1.5137 1.1407 0.0081 0.0801 80.3213 3.0029 \u03b3[2] 0.0515 0.0970 0.0007 0.0068 80.3213 1.3837 \u03c1[1] 0.5549 0.2274 0.0016 0.0136 80.3213 1.3732 \u03c1[2] 47.0605 11.1717 0.0790 0.7350 80.3213 1.2892 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0001 0.2457 sigD 2.4786 2.9594 5.5550 7.6868 8.8722 sigC 217.4268 265.3300 550.1108 1232.7812 4068.7156 sigRc 673.8938 765.5432 836.1810 916.0544 1064.8168 a 0.0746 0.3490 0.5506 0.7427 0.9411 pE0 0.0000 0.0000 0.0000 0.0000 0.0016 p[1] 0.0000 0.0001 0.0003 0.0240 0.2330 p[2] 0.0008 0.0008 0.0008 0.0009 0.0354 \u03b2[1] 0.0000 0.0026 0.5025 1.2954 2.8868 \u03b2[2] 0.0000 0.0583 0.3820 1.0063 2.4575 \u03b2[3] 0.0405 0.7791 1.7061 3.3934 4.2446 \u03b3[1] 0.0316 0.5668 1.1303 3.0000 3.0000 \u03b3[2] 0.0214 0.0264 0.0275 0.0286 0.4204 \u03c1[1] 0.1889 0.3367 0.5769 0.7514 0.9503 \u03c1[2] 28.7635 38.8180 46.5332 53.4753 75.3812 Fit \u00b6 sdf = simtrajectories(cc, korea, 1:150) f = plotvars(sdf, korea, dayt0=dayt0) plot!(f.fit, xlim=nothing) We see that the model does not fit the rapid drop in new cases in South Korea. This may be caused by the model\u2019s implausible assumption that transmission and testing rates are constant over time. Implications \u00b6 for fig in f.trajectories display(plot(fig, xlim=nothing)) end","title":"South Korea"},{"location":"korea/#south-korea","text":"korea = CountryData(covdf, \"Korea, South\") CovidSEIR.CountryData{Float64,Int64}(5.1635256e7, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10 \u2026 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], [0.0, 0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0, 0.0 \u2026 192.0, 200.0, 204.0, 208.0, 211.0, 214.0, 217 .0, 222.0, 225.0, 229.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 \u2026 6694.0, 6776.0, 6973.0, 7117.0, 7243.0, 7368.0, 7447.0, 7534.0, 7616.0 , 7757.0], [1.0, 1.0, 2.0, 2.0, 3.0, 4.0, 4.0, 4.0, 4.0, 11.0 \u2026 3445.0, 3 408.0, 3246.0, 3125.0, 3026.0, 2930.0, 2873.0, 2808.0, 2750.0, 2627.0]) using Turing mdl = CovidSEIR.TimeVarying.countrymodel(korea) cc = Turing.psample(mdl, NUTS(0.65), 5000, 4) import JLD2 JLD2.@save \"$jmddir/korea_tv_$(Dates.today()).jld2\" cc JLD2.@load \"$jmddir/korea_dhmc_2020-04-13.jld2\" cc dayt0; cc = MCMCChains.Chains(collect(cc.value.data), replace.(cc.name_map.parameters, r\"([^\\[])([1-9])\" => s\"\\1[\\2]\")) Object of type Chains, with data of type 5000\u00d715\u00d74 Array{Float64,3} Iterations = 1:5000 Thinning interval = 1 Chains = 1, 2, 3, 4 Samples per chain = 5000 parameters = \u03c4, sigD, sigC, sigRc, a, pE0, p[1], p[2], \u03b2[1], \u03b2[2], \u03b2 [3], \u03b3[1], \u03b3[2], \u03c1[1], \u03c1[2] 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0146 0.0597 0.0004 0.0041 80.3213 1.3408 sigD 5.3642 2.4216 0.0171 0.1694 80.3213 5.0384 sigC 951.3287 978.9917 6.9225 69.0719 80.3213 2.2197 sigRc 845.4857 105.0627 0.7429 5.9862 80.3213 1.4793 a 0.5215 0.2690 0.0019 0.0176 80.3213 1.8935 pE0 0.0002 0.0005 0.0000 0.0000 80.3213 1.4380 p[1] 0.0316 0.0626 0.0004 0.0044 80.3213 2.4790 p[2] 0.0031 0.0089 0.0001 0.0006 80.3213 1.2621 \u03b2[1] 0.7778 0.8358 0.0059 0.0555 80.3213 1.4458 \u03b2[2] 0.6354 0.7074 0.0050 0.0398 80.3370 1.2070 \u03b2[3] 2.0175 1.3794 0.0098 0.0966 80.3213 3.3188 \u03b3[1] 1.5137 1.1407 0.0081 0.0801 80.3213 3.0029 \u03b3[2] 0.0515 0.0970 0.0007 0.0068 80.3213 1.3837 \u03c1[1] 0.5549 0.2274 0.0016 0.0136 80.3213 1.3732 \u03c1[2] 47.0605 11.1717 0.0790 0.7350 80.3213 1.2892 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0001 0.2457 sigD 2.4786 2.9594 5.5550 7.6868 8.8722 sigC 217.4268 265.3300 550.1108 1232.7812 4068.7156 sigRc 673.8938 765.5432 836.1810 916.0544 1064.8168 a 0.0746 0.3490 0.5506 0.7427 0.9411 pE0 0.0000 0.0000 0.0000 0.0000 0.0016 p[1] 0.0000 0.0001 0.0003 0.0240 0.2330 p[2] 0.0008 0.0008 0.0008 0.0009 0.0354 \u03b2[1] 0.0000 0.0026 0.5025 1.2954 2.8868 \u03b2[2] 0.0000 0.0583 0.3820 1.0063 2.4575 \u03b2[3] 0.0405 0.7791 1.7061 3.3934 4.2446 \u03b3[1] 0.0316 0.5668 1.1303 3.0000 3.0000 \u03b3[2] 0.0214 0.0264 0.0275 0.0286 0.4204 \u03c1[1] 0.1889 0.3367 0.5769 0.7514 0.9503 \u03c1[2] 28.7635 38.8180 46.5332 53.4753 75.3812","title":"South Korea"},{"location":"korea/#estimates","text":"plot(cc) describe(cc) 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0146 0.0597 0.0004 0.0041 80.3213 1.3408 sigD 5.3642 2.4216 0.0171 0.1694 80.3213 5.0384 sigC 951.3287 978.9917 6.9225 69.0719 80.3213 2.2197 sigRc 845.4857 105.0627 0.7429 5.9862 80.3213 1.4793 a 0.5215 0.2690 0.0019 0.0176 80.3213 1.8935 pE0 0.0002 0.0005 0.0000 0.0000 80.3213 1.4380 p[1] 0.0316 0.0626 0.0004 0.0044 80.3213 2.4790 p[2] 0.0031 0.0089 0.0001 0.0006 80.3213 1.2621 \u03b2[1] 0.7778 0.8358 0.0059 0.0555 80.3213 1.4458 \u03b2[2] 0.6354 0.7074 0.0050 0.0398 80.3370 1.2070 \u03b2[3] 2.0175 1.3794 0.0098 0.0966 80.3213 3.3188 \u03b3[1] 1.5137 1.1407 0.0081 0.0801 80.3213 3.0029 \u03b3[2] 0.0515 0.0970 0.0007 0.0068 80.3213 1.3837 \u03c1[1] 0.5549 0.2274 0.0016 0.0136 80.3213 1.3732 \u03c1[2] 47.0605 11.1717 0.0790 0.7350 80.3213 1.2892 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0001 0.2457 sigD 2.4786 2.9594 5.5550 7.6868 8.8722 sigC 217.4268 265.3300 550.1108 1232.7812 4068.7156 sigRc 673.8938 765.5432 836.1810 916.0544 1064.8168 a 0.0746 0.3490 0.5506 0.7427 0.9411 pE0 0.0000 0.0000 0.0000 0.0000 0.0016 p[1] 0.0000 0.0001 0.0003 0.0240 0.2330 p[2] 0.0008 0.0008 0.0008 0.0009 0.0354 \u03b2[1] 0.0000 0.0026 0.5025 1.2954 2.8868 \u03b2[2] 0.0000 0.0583 0.3820 1.0063 2.4575 \u03b2[3] 0.0405 0.7791 1.7061 3.3934 4.2446 \u03b3[1] 0.0316 0.5668 1.1303 3.0000 3.0000 \u03b3[2] 0.0214 0.0264 0.0275 0.0286 0.4204 \u03c1[1] 0.1889 0.3367 0.5769 0.7514 0.9503 \u03c1[2] 28.7635 38.8180 46.5332 53.4753 75.3812","title":"Estimates"},{"location":"korea/#fit","text":"sdf = simtrajectories(cc, korea, 1:150) f = plotvars(sdf, korea, dayt0=dayt0) plot!(f.fit, xlim=nothing) We see that the model does not fit the rapid drop in new cases in South Korea. This may be caused by the model\u2019s implausible assumption that transmission and testing rates are constant over time.","title":"Fit"},{"location":"korea/#implications","text":"for fig in f.trajectories display(plot(fig, xlim=nothing)) end","title":"Implications"},{"location":"license/","text":"The model and results are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf. BibTeX citation. The license for the package source code is here.","title":"License"},{"location":"rt_longdiff/","text":"Longer Differences and Smoothing \u00b6 One way to reduce $\\sigma_k/\\sigma_R$ is to reduce noise in new cases by taking a longer difference or smoothing case counts in some other way. How does this affect the estimation and interpretation of $R_t$? As in the first section, we start with the approximate recursive relation C(t) - C(t-1) \\equiv \\Delta C(t) \\approx \\frac{\\tau(t)}{\\tau(t-1)} e^{\\gamma (R_t - 1)} \\Delta C(t-1) If we instead look at a longer difference, \\begin{align*} C(t) - C(t-L) = & \\sum_{i=0}^{L-1} \\Delta C_{t-i} \\\\ \\approx & \\sum_{i=0}^{L-1} \\frac{\\tau(t-i)}{\\tau(t-i-1)} e^{\\gamma (R_{t-i} - 1)} \\Delta C_{t-i-1} \\\\ = & \\overline {T_{t,L} e^{\\gamma(R_{t,L} - 1)}} \\sum_{i=0}^{L-1}\\Delta C_{t-i-1} \\\\ = & \\overline {T_{t,L} e^{\\gamma(R_{t,L} - 1)}} \\left( C(t-1) - C(t-1-L) \\right) \\end{align*} where $\\overline {T_{t,L} e^{\\gamma(R_{t,L} - 1)}}$ is some intermediate value in between the minimum and maximum of the ${ \\frac{\\tau(t-i)}{\\tau(t-i-1)} e^{\\gamma (R_{t-i} - 1)} }_{i=0}^{L-1}$. If testing is constant over time, we can then obtain an interpretable $\\overline{R_{t,L}}$ by using $k_{t,L} =\\log(C(t)-C(t-L))$ and following the procedure above. If testing varies with time, it becomes hard to separate testing rate changes from $R_t$ after taking long differnces. Note The same analysis can be applied to other smoothing operations, i.e. using \\sum_{i=0}^L w_i \\Delta C_{t-i} in place of $C(t) - C(t-L)$. However, there\u2019s something strange about smoothing $C_t$, and then extracting a smoothed component of it using the Kalman filter. The inference afterwards is suspect; we would essentially be estimating a kernel regression of $C_t$ on time, and using the estimated regression as though it\u2019s known with certainty. When would long differences reduce variance? Well if $\\Delta C(t) = \\Delta C^\\ast(t) + \\epsilon_t$ with $\\epsilon_t$ indepenedent over time with mean $0$ and constant variance, then you would need $C^\\ast(t) - C^\\ast(t-L)$ to increase faster than linearly with $L$. This is true if $C^\\ast$ is growing exponentially. Alternatively, if $\\epsilon_t$ is not independent over time, but negatively correlated (as seems likely), then variance can decrease with $L$. For example, if $\\Delta C(t) = C^\\ast(t) - C^\\ast(t-\\delta)$ with $\\delta$ a random, independent increment with mean $1$, then variance will tend to decrease with $L$ regardless of $C^\\ast(t)$. Results \u00b6 Here, we will allow the initial and time varying mean of $R_{s,t}$ to depend on covariates. \\begin{align*} \\tilde{R}_{s,0} & \\sim N(X_{0,s} \\alpha_0, \\sigma^2_{R,0}) \\\\ \\tilde{R}_{s,t} & = \\rho \\tilde{R}_{s,t} + u_{s,t} \\;,\\; u_{s,t} \\sim N(0, \\sigma^2_R) \\\\ R_{s,t} & = X_{s,t} \\alpha + \\tilde{R}_{s,t} \\\\ \\Delta \\log(k)_{s,t} & = \\gamma (R_{s,t} - 1) + \\epsilon_{s,t} - \\epsilon_{s,t-1} \\;, \\; \\epsilon_{s,t} \\sim N(0, \\sigma^2_k) \\end{align*} reestimate=false rlo=-1 #1 - eps(Float64) rhi=1.2 #1+ eps(Float64) K = size(X[1],2) priors = (\u03b3 = truncated(Normal(1/7,1/7), 1/28, 1/1), \u03c3R0 = truncated(Normal(1, 3), 0, Inf), \u03b10 = MvNormal(zeros(length(X0[1])), sqrt(10)), #truncated(Normal(1, 3), 0, Inf), \u03c3R = truncated(Normal(0.25,1),0,Inf), \u03c3k = truncated(Normal(0.1, 5), 0, Inf), \u03c1 = Uniform(rlo, rhi), \u03b1 = MvNormal(zeros(K), sqrt(10)) ) mdl = RT.RtRW(dlogk, X, X0, priors); trans = as( (\u03b3 = as\u211d\u208a, \u03c3R0 = as\u211d\u208a, \u03b10 = as(Array, length(X0[1])), \u03c3R = as\u211d\u208a, \u03c3k = as\u211d\u208a, \u03c1=as(Real, rlo, rhi), \u03b1 = as(Array, K)) ) P = TransformedLogDensity(trans, mdl) \u2207P = ADgradient(:ForwardDiff, P) p0 = (\u03b3 = 1/7, \u03c3R0=1.0, \u03b10 = zeros(length(X0[1])), \u03c3R=0.25, \u03c3k=2.0, \u03c1=1.0, \u03b1=zeros(K)) x0 = inverse(trans,p0) @time LogDensityProblems.logdensity_and_gradient(\u2207P, x0); 2.090468 seconds (4.01 M allocations: 193.476 MiB, 9.03% gc time) rng = MersenneTwister() steps = 100 warmup=default_warmup_stages(local_optimization=nothing, #FindLocalOptimum(1e-6, 200), stepsize_search=nothing, init_steps=steps, middle_steps=steps, terminating_steps=2*steps, doubling_stages=4, M=Symmetric) x0 = x0 if !isfile(\"rt7.jld2\") || reestimate res = DynamicHMC.mcmc_keep_warmup(rng, \u2207P, 2000;initialization = (q = x0, \u03f5=0.1), reporter = LogProgressReport(nothing, 25, 15), warmup_stages =warmup); post = transform.(trans,res.inference.chain) @save \"rt7.jld2\" post end @load \"rt7.jld2\" post p = post[1] vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)' vals = reshape(vals, size(vals)..., 1) names = vcat([length(p[s])==1 ? String(s) : String.(s).*\"[\".*string.(1:length(p[s])).*\"]\" for s in keys(p)]...) cc = MCMCChains.Chains(vals, names) display(plot(cc)) display(describe(cc)) 2-element Array{ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03b3 0.0379 0.0025 0.0001 0.0003 108.5762 1.0094 \u03c3R0 2.8472 0.5901 0.0132 0.0656 48.7521 1.0343 \u03b10[1] 0.7643 2.5342 0.0567 0.1661 187.4434 1.0018 \u03b10[2] 0.7817 0.3966 0.0089 0.0296 171.7902 1.0026 \u03b10[3] 0.0789 0.7772 0.0174 0.0986 37.6326 1.0154 \u03b10[4] -0.3337 0.3151 0.0070 0.0315 82.1813 1.0035 \u03b10[5] 0.2207 0.1280 0.0029 0.0147 47.5974 1.0322 \u03c3R 0.5057 0.0870 0.0019 0.0041 255.7555 1.0007 \u03c3k 0.1271 0.0024 0.0001 0.0001 446.0680 0.9998 \u03c1 0.9309 0.0116 0.0003 0.0010 107.4759 1.0163 \u03b1[1] 1.8516 1.0681 0.0239 0.1446 25.1433 1.0501 \u03b1[2] -1.1438 0.4449 0.0099 0.0416 105.1060 1.0127 \u03b1[3] -0.0083 0.8520 0.0191 0.1048 44.8124 1.0340 \u03b1[4] 0.8243 0.5430 0.0121 0.0493 124.4263 1.0032 \u03b1[5] 0.0994 0.8315 0.0186 0.1093 31.5759 0.9995 \u03b1[6] -0.1280 0.8602 0.0192 0.1103 40.2397 0.9995 \u03b1[7] 0.1713 0.4355 0.0097 0.0390 160.8671 1.0006 \u03b1[8] -0.6264 0.3775 0.0084 0.0300 124.0985 1.0104 \u03b1[9] -0.6664 0.4365 0.0098 0.0395 96.5867 1.0059 \u03b1[10] -0.3495 0.6313 0.0141 0.0483 173.2760 0.9999 \u03b1[11] -0.7192 1.9604 0.0438 0.2069 78.9574 1.0042 \u03b1[12] -0.7804 1.5990 0.0358 0.1891 39.9999 1.0244 \u03b1[13] -0.2912 0.3787 0.0085 0.0497 38.9951 1.0060 \u03b1[14] 0.0375 1.7094 0.0382 0.2016 77.1582 1.0000 \u03b1[15] 1.6999 1.8242 0.0408 0.2300 52.0837 1.0387 \u03b1[16] 4.5390 2.9292 0.0655 0.3112 78.9740 1.0021 \u03b1[17] 0.5028 1.6706 0.0374 0.2142 47.3735 1.0128 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03b3 0.0358 0.0363 0.0370 0.0385 0.0447 \u03c3R0 1.7240 2.4285 2.8403 3.2445 4.0325 \u03b10[1] -4.6969 -0.8651 0.8286 2.4189 5.8212 \u03b10[2] 0.0048 0.5012 0.8055 1.0475 1.5065 \u03b10[3] -1.4842 -0.4245 0.0886 0.6008 1.5086 \u03b10[4] -0.9854 -0.5299 -0.3391 -0.1320 0.3514 \u03b10[5] -0.0282 0.1411 0.2189 0.2999 0.4784 \u03c3R 0.3405 0.4481 0.5033 0.5608 0.6871 \u03c3k 0.1224 0.1255 0.1272 0.1286 0.1318 \u03c1 0.9081 0.9232 0.9311 0.9394 0.9532 \u03b1[1] -0.4662 1.1087 1.9482 2.6229 3.7126 \u03b1[2] -1.8904 -1.4731 -1.1892 -0.8362 -0.2449 \u03b1[3] -1.6674 -0.5660 -0.0063 0.5603 1.6550 \u03b1[4] -0.2679 0.4651 0.8096 1.2006 1.8807 \u03b1[5] -1.5504 -0.4773 0.1007 0.7080 1.6550 \u03b1[6] -1.7526 -0.7365 -0.1388 0.4751 1.5934 \u03b1[7] -0.6846 -0.1063 0.1747 0.4540 1.0258 \u03b1[8] -1.3522 -0.8877 -0.6086 -0.3772 0.1172 \u03b1[9] -1.4709 -0.9545 -0.6563 -0.3671 0.2092 \u03b1[10] -1.5648 -0.7817 -0.3454 0.0693 0.8933 \u03b1[11] -4.1944 -2.0611 -0.8113 0.6417 3.3614 \u03b1[12] -4.0476 -1.7987 -0.7897 0.3959 2.3293 \u03b1[13] -0.9347 -0.5773 -0.3129 -0.0225 0.4680 \u03b1[14] -3.1091 -1.2218 0.0735 1.1604 3.4156 \u03b1[15] -1.7118 0.4843 1.7194 2.8932 5.6024 \u03b1[16] -1.3076 2.5101 4.5436 6.6371 9.8855 \u03b1[17] -2.5148 -0.6405 0.3466 1.6276 3.9682 display([1:length(x0vars) x0vars]) 5\u00d72 Array{Any,2}: 1 :constant 2 :logpopdens 3 Symbol(\"Percent.Unemployed..2018.\") 4 Symbol(\"Percent.living.under.the.federal.poverty.line..2018.\") 5 Symbol(\"Percent.at.risk.for.serious.illness.due.to.COVID\") display([1:length(xvars) xvars]) 17\u00d72 Array{Any,2}: 1 :constant 2 Symbol(\"Stay.at.home..shelter.in.place\") 3 Symbol(\"State.of.emergency\") 4 Symbol(\"Date.closed.K.12.schools\") 5 Symbol(\"Closed.gyms\") 6 Symbol(\"Closed.movie.theaters\") 7 Symbol(\"Closed.day.cares\") 8 Symbol(\"Date.banned.visitors.to.nursing.homes\") 9 Symbol(\"Closed.non.essential.businesses\") 10 Symbol(\"Closed.restaurants.except.take.out\") 11 :retail_and_recreation_percent_change_from_baseline 12 :grocery_and_pharmacy_percent_change_from_baseline 13 :parks_percent_change_from_baseline 14 :transit_stations_percent_change_from_baseline 15 :workplaces_percent_change_from_baseline 16 :residential_percent_change_from_baseline 17 :percentchangebusinesses states = unique(sdf.state) states_to_plot = [\"New York\", \"New Jersey\",\"Massachusetts\",\"California\", \"Georgia\",\"Illinois\",\"Michigan\", \"Ohio\",\"Wisconsin\",\"Washington\"] S = length(states_to_plot) figs = fill(plot(), S) for (i,st) in enumerate(states_to_plot) s = findfirst(states.==st) figr = RT.plotpostr(dates[s],dlogk[s],post, X[s], X0[s]) l = @layout [a{.1h}; grid(1,1)] figs[i] = plot(plot(annotation=(0.5,0.5, st), framestyle = :none), plot(figr, ylim=(-1,10)), layout=l) display(figs[i]) end Another Derivation \u00b6 An alternative (and easier) way to derive the same estimator will be described here. This approach will easily generalize to more complicated models, but let\u2019s begin with the simplest SIR model with testing. \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\beta I \\\\ \\dot{I} & = \\frac{S}{N} \\beta I - \\gamma I \\\\ \\dot{C} & = \\tau I \\\\ \\dot{\\mathcal{R}} & = \\gamma I \\end{align*} Then note that \\ddot{C} = \\dot{\\tau} I + \\tau \\dot{I} and \\begin{align*} \\frac{\\ddot{C}}{\\dot{C}} = & \\frac{\\dot{\\tau}}{\\tau} + \\frac{\\dot{I}}{I} \\\\ \\frac{d}{dt} \\log(\\dot{C}) = & \\frac{\\dot{\\tau}}{\\tau} + \\frac{S}{N}\\beta - \\gamma \\\\ = & \\frac{\\dot{\\tau}}{\\tau} + \\gamma(R_t - 1) \\end{align*} which is the equation we have been using for estimation. Incorporating death \u00b6 If we add deaths to the model, \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\beta I \\\\ \\dot{I} & = \\frac{S}{N} \\beta I - \\gamma I - p I\\\\ \\dot{C} & = \\tau I \\\\ \\dot{\\mathcal{R}} & = \\gamma I \\\\ \\dot{D} & = p I \\end{align*} then, \\begin{align*} \\frac{\\ddot{C}}{\\dot{C}} = & \\frac{\\dot{\\tau}}{\\tau} + \\frac{S}{N}\\beta - \\gamma - p \\\\ \\frac{\\ddot{D}}{\\dot{D}} = & \\frac{S}{N}\\beta - \\gamma - p \\\\ \\end{align*} Other observable states could similarly be added to the model. Time delays \u00b6 A drawback of the above approach is that it implies changes in $R_t$ show up in the derivatives of case and death numbers instantly. This is definitely not true. Instead consider a model where infections last $\\ell$ days. After $\\ell$ days, each infected person dies with probability $\\pi$ and recovers otherwise. Then we have \\begin{align*} \\dot{S}(t) & = -\\frac{S(t)}{N} \\beta I \\\\ \\dot{I}(t) & = \\frac{S(t)}{N} \\beta(t) I(t) - \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) I(t-\\ell) \\\\ \\dot{C}(t) & = \\tau(t) I(t) \\\\ \\dot{\\mathcal{R}}(t) & = (1-\\pi) \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) I(t-\\ell) \\\\ \\dot{D}(t) & = \\pi \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) I(t-\\ell) \\end{align*} Rearranging gives \\frac{\\ddot{C}(t)}{\\dot{C}(t)} = \\frac{\\dot{\\tau}(t)}{\\tau(t)} + \\frac{S(t)}{N}\\beta(t) - \\frac{1}{\\pi} \\frac{\\dot{D}(t)}{\\dot{C}(t)} and \\dot{D}(t) = \\pi \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) \\frac{\\dot{C}(t-\\ell)}{\\tau(t-\\ell)} Note These last two equations also hold in the model without time delay by setting $\\ell=0$ and $\\pi = \\frac{p}{p+\\gamma}$ Note Random durations can be accomodated by replacing the shift by $\\ell$ with a convolution.","title":"Long-differening"},{"location":"rt_longdiff/#longer-differences-and-smoothing","text":"One way to reduce $\\sigma_k/\\sigma_R$ is to reduce noise in new cases by taking a longer difference or smoothing case counts in some other way. How does this affect the estimation and interpretation of $R_t$? As in the first section, we start with the approximate recursive relation C(t) - C(t-1) \\equiv \\Delta C(t) \\approx \\frac{\\tau(t)}{\\tau(t-1)} e^{\\gamma (R_t - 1)} \\Delta C(t-1) If we instead look at a longer difference, \\begin{align*} C(t) - C(t-L) = & \\sum_{i=0}^{L-1} \\Delta C_{t-i} \\\\ \\approx & \\sum_{i=0}^{L-1} \\frac{\\tau(t-i)}{\\tau(t-i-1)} e^{\\gamma (R_{t-i} - 1)} \\Delta C_{t-i-1} \\\\ = & \\overline {T_{t,L} e^{\\gamma(R_{t,L} - 1)}} \\sum_{i=0}^{L-1}\\Delta C_{t-i-1} \\\\ = & \\overline {T_{t,L} e^{\\gamma(R_{t,L} - 1)}} \\left( C(t-1) - C(t-1-L) \\right) \\end{align*} where $\\overline {T_{t,L} e^{\\gamma(R_{t,L} - 1)}}$ is some intermediate value in between the minimum and maximum of the ${ \\frac{\\tau(t-i)}{\\tau(t-i-1)} e^{\\gamma (R_{t-i} - 1)} }_{i=0}^{L-1}$. If testing is constant over time, we can then obtain an interpretable $\\overline{R_{t,L}}$ by using $k_{t,L} =\\log(C(t)-C(t-L))$ and following the procedure above. If testing varies with time, it becomes hard to separate testing rate changes from $R_t$ after taking long differnces. Note The same analysis can be applied to other smoothing operations, i.e. using \\sum_{i=0}^L w_i \\Delta C_{t-i} in place of $C(t) - C(t-L)$. However, there\u2019s something strange about smoothing $C_t$, and then extracting a smoothed component of it using the Kalman filter. The inference afterwards is suspect; we would essentially be estimating a kernel regression of $C_t$ on time, and using the estimated regression as though it\u2019s known with certainty. When would long differences reduce variance? Well if $\\Delta C(t) = \\Delta C^\\ast(t) + \\epsilon_t$ with $\\epsilon_t$ indepenedent over time with mean $0$ and constant variance, then you would need $C^\\ast(t) - C^\\ast(t-L)$ to increase faster than linearly with $L$. This is true if $C^\\ast$ is growing exponentially. Alternatively, if $\\epsilon_t$ is not independent over time, but negatively correlated (as seems likely), then variance can decrease with $L$. For example, if $\\Delta C(t) = C^\\ast(t) - C^\\ast(t-\\delta)$ with $\\delta$ a random, independent increment with mean $1$, then variance will tend to decrease with $L$ regardless of $C^\\ast(t)$.","title":"Longer Differences and Smoothing"},{"location":"rt_longdiff/#results","text":"Here, we will allow the initial and time varying mean of $R_{s,t}$ to depend on covariates. \\begin{align*} \\tilde{R}_{s,0} & \\sim N(X_{0,s} \\alpha_0, \\sigma^2_{R,0}) \\\\ \\tilde{R}_{s,t} & = \\rho \\tilde{R}_{s,t} + u_{s,t} \\;,\\; u_{s,t} \\sim N(0, \\sigma^2_R) \\\\ R_{s,t} & = X_{s,t} \\alpha + \\tilde{R}_{s,t} \\\\ \\Delta \\log(k)_{s,t} & = \\gamma (R_{s,t} - 1) + \\epsilon_{s,t} - \\epsilon_{s,t-1} \\;, \\; \\epsilon_{s,t} \\sim N(0, \\sigma^2_k) \\end{align*} reestimate=false rlo=-1 #1 - eps(Float64) rhi=1.2 #1+ eps(Float64) K = size(X[1],2) priors = (\u03b3 = truncated(Normal(1/7,1/7), 1/28, 1/1), \u03c3R0 = truncated(Normal(1, 3), 0, Inf), \u03b10 = MvNormal(zeros(length(X0[1])), sqrt(10)), #truncated(Normal(1, 3), 0, Inf), \u03c3R = truncated(Normal(0.25,1),0,Inf), \u03c3k = truncated(Normal(0.1, 5), 0, Inf), \u03c1 = Uniform(rlo, rhi), \u03b1 = MvNormal(zeros(K), sqrt(10)) ) mdl = RT.RtRW(dlogk, X, X0, priors); trans = as( (\u03b3 = as\u211d\u208a, \u03c3R0 = as\u211d\u208a, \u03b10 = as(Array, length(X0[1])), \u03c3R = as\u211d\u208a, \u03c3k = as\u211d\u208a, \u03c1=as(Real, rlo, rhi), \u03b1 = as(Array, K)) ) P = TransformedLogDensity(trans, mdl) \u2207P = ADgradient(:ForwardDiff, P) p0 = (\u03b3 = 1/7, \u03c3R0=1.0, \u03b10 = zeros(length(X0[1])), \u03c3R=0.25, \u03c3k=2.0, \u03c1=1.0, \u03b1=zeros(K)) x0 = inverse(trans,p0) @time LogDensityProblems.logdensity_and_gradient(\u2207P, x0); 2.090468 seconds (4.01 M allocations: 193.476 MiB, 9.03% gc time) rng = MersenneTwister() steps = 100 warmup=default_warmup_stages(local_optimization=nothing, #FindLocalOptimum(1e-6, 200), stepsize_search=nothing, init_steps=steps, middle_steps=steps, terminating_steps=2*steps, doubling_stages=4, M=Symmetric) x0 = x0 if !isfile(\"rt7.jld2\") || reestimate res = DynamicHMC.mcmc_keep_warmup(rng, \u2207P, 2000;initialization = (q = x0, \u03f5=0.1), reporter = LogProgressReport(nothing, 25, 15), warmup_stages =warmup); post = transform.(trans,res.inference.chain) @save \"rt7.jld2\" post end @load \"rt7.jld2\" post p = post[1] vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)' vals = reshape(vals, size(vals)..., 1) names = vcat([length(p[s])==1 ? String(s) : String.(s).*\"[\".*string.(1:length(p[s])).*\"]\" for s in keys(p)]...) cc = MCMCChains.Chains(vals, names) display(plot(cc)) display(describe(cc)) 2-element Array{ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03b3 0.0379 0.0025 0.0001 0.0003 108.5762 1.0094 \u03c3R0 2.8472 0.5901 0.0132 0.0656 48.7521 1.0343 \u03b10[1] 0.7643 2.5342 0.0567 0.1661 187.4434 1.0018 \u03b10[2] 0.7817 0.3966 0.0089 0.0296 171.7902 1.0026 \u03b10[3] 0.0789 0.7772 0.0174 0.0986 37.6326 1.0154 \u03b10[4] -0.3337 0.3151 0.0070 0.0315 82.1813 1.0035 \u03b10[5] 0.2207 0.1280 0.0029 0.0147 47.5974 1.0322 \u03c3R 0.5057 0.0870 0.0019 0.0041 255.7555 1.0007 \u03c3k 0.1271 0.0024 0.0001 0.0001 446.0680 0.9998 \u03c1 0.9309 0.0116 0.0003 0.0010 107.4759 1.0163 \u03b1[1] 1.8516 1.0681 0.0239 0.1446 25.1433 1.0501 \u03b1[2] -1.1438 0.4449 0.0099 0.0416 105.1060 1.0127 \u03b1[3] -0.0083 0.8520 0.0191 0.1048 44.8124 1.0340 \u03b1[4] 0.8243 0.5430 0.0121 0.0493 124.4263 1.0032 \u03b1[5] 0.0994 0.8315 0.0186 0.1093 31.5759 0.9995 \u03b1[6] -0.1280 0.8602 0.0192 0.1103 40.2397 0.9995 \u03b1[7] 0.1713 0.4355 0.0097 0.0390 160.8671 1.0006 \u03b1[8] -0.6264 0.3775 0.0084 0.0300 124.0985 1.0104 \u03b1[9] -0.6664 0.4365 0.0098 0.0395 96.5867 1.0059 \u03b1[10] -0.3495 0.6313 0.0141 0.0483 173.2760 0.9999 \u03b1[11] -0.7192 1.9604 0.0438 0.2069 78.9574 1.0042 \u03b1[12] -0.7804 1.5990 0.0358 0.1891 39.9999 1.0244 \u03b1[13] -0.2912 0.3787 0.0085 0.0497 38.9951 1.0060 \u03b1[14] 0.0375 1.7094 0.0382 0.2016 77.1582 1.0000 \u03b1[15] 1.6999 1.8242 0.0408 0.2300 52.0837 1.0387 \u03b1[16] 4.5390 2.9292 0.0655 0.3112 78.9740 1.0021 \u03b1[17] 0.5028 1.6706 0.0374 0.2142 47.3735 1.0128 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03b3 0.0358 0.0363 0.0370 0.0385 0.0447 \u03c3R0 1.7240 2.4285 2.8403 3.2445 4.0325 \u03b10[1] -4.6969 -0.8651 0.8286 2.4189 5.8212 \u03b10[2] 0.0048 0.5012 0.8055 1.0475 1.5065 \u03b10[3] -1.4842 -0.4245 0.0886 0.6008 1.5086 \u03b10[4] -0.9854 -0.5299 -0.3391 -0.1320 0.3514 \u03b10[5] -0.0282 0.1411 0.2189 0.2999 0.4784 \u03c3R 0.3405 0.4481 0.5033 0.5608 0.6871 \u03c3k 0.1224 0.1255 0.1272 0.1286 0.1318 \u03c1 0.9081 0.9232 0.9311 0.9394 0.9532 \u03b1[1] -0.4662 1.1087 1.9482 2.6229 3.7126 \u03b1[2] -1.8904 -1.4731 -1.1892 -0.8362 -0.2449 \u03b1[3] -1.6674 -0.5660 -0.0063 0.5603 1.6550 \u03b1[4] -0.2679 0.4651 0.8096 1.2006 1.8807 \u03b1[5] -1.5504 -0.4773 0.1007 0.7080 1.6550 \u03b1[6] -1.7526 -0.7365 -0.1388 0.4751 1.5934 \u03b1[7] -0.6846 -0.1063 0.1747 0.4540 1.0258 \u03b1[8] -1.3522 -0.8877 -0.6086 -0.3772 0.1172 \u03b1[9] -1.4709 -0.9545 -0.6563 -0.3671 0.2092 \u03b1[10] -1.5648 -0.7817 -0.3454 0.0693 0.8933 \u03b1[11] -4.1944 -2.0611 -0.8113 0.6417 3.3614 \u03b1[12] -4.0476 -1.7987 -0.7897 0.3959 2.3293 \u03b1[13] -0.9347 -0.5773 -0.3129 -0.0225 0.4680 \u03b1[14] -3.1091 -1.2218 0.0735 1.1604 3.4156 \u03b1[15] -1.7118 0.4843 1.7194 2.8932 5.6024 \u03b1[16] -1.3076 2.5101 4.5436 6.6371 9.8855 \u03b1[17] -2.5148 -0.6405 0.3466 1.6276 3.9682 display([1:length(x0vars) x0vars]) 5\u00d72 Array{Any,2}: 1 :constant 2 :logpopdens 3 Symbol(\"Percent.Unemployed..2018.\") 4 Symbol(\"Percent.living.under.the.federal.poverty.line..2018.\") 5 Symbol(\"Percent.at.risk.for.serious.illness.due.to.COVID\") display([1:length(xvars) xvars]) 17\u00d72 Array{Any,2}: 1 :constant 2 Symbol(\"Stay.at.home..shelter.in.place\") 3 Symbol(\"State.of.emergency\") 4 Symbol(\"Date.closed.K.12.schools\") 5 Symbol(\"Closed.gyms\") 6 Symbol(\"Closed.movie.theaters\") 7 Symbol(\"Closed.day.cares\") 8 Symbol(\"Date.banned.visitors.to.nursing.homes\") 9 Symbol(\"Closed.non.essential.businesses\") 10 Symbol(\"Closed.restaurants.except.take.out\") 11 :retail_and_recreation_percent_change_from_baseline 12 :grocery_and_pharmacy_percent_change_from_baseline 13 :parks_percent_change_from_baseline 14 :transit_stations_percent_change_from_baseline 15 :workplaces_percent_change_from_baseline 16 :residential_percent_change_from_baseline 17 :percentchangebusinesses states = unique(sdf.state) states_to_plot = [\"New York\", \"New Jersey\",\"Massachusetts\",\"California\", \"Georgia\",\"Illinois\",\"Michigan\", \"Ohio\",\"Wisconsin\",\"Washington\"] S = length(states_to_plot) figs = fill(plot(), S) for (i,st) in enumerate(states_to_plot) s = findfirst(states.==st) figr = RT.plotpostr(dates[s],dlogk[s],post, X[s], X0[s]) l = @layout [a{.1h}; grid(1,1)] figs[i] = plot(plot(annotation=(0.5,0.5, st), framestyle = :none), plot(figr, ylim=(-1,10)), layout=l) display(figs[i]) end","title":"Results"},{"location":"rt_longdiff/#another-derivation","text":"An alternative (and easier) way to derive the same estimator will be described here. This approach will easily generalize to more complicated models, but let\u2019s begin with the simplest SIR model with testing. \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\beta I \\\\ \\dot{I} & = \\frac{S}{N} \\beta I - \\gamma I \\\\ \\dot{C} & = \\tau I \\\\ \\dot{\\mathcal{R}} & = \\gamma I \\end{align*} Then note that \\ddot{C} = \\dot{\\tau} I + \\tau \\dot{I} and \\begin{align*} \\frac{\\ddot{C}}{\\dot{C}} = & \\frac{\\dot{\\tau}}{\\tau} + \\frac{\\dot{I}}{I} \\\\ \\frac{d}{dt} \\log(\\dot{C}) = & \\frac{\\dot{\\tau}}{\\tau} + \\frac{S}{N}\\beta - \\gamma \\\\ = & \\frac{\\dot{\\tau}}{\\tau} + \\gamma(R_t - 1) \\end{align*} which is the equation we have been using for estimation.","title":"Another Derivation"},{"location":"rt_longdiff/#incorporating-death","text":"If we add deaths to the model, \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\beta I \\\\ \\dot{I} & = \\frac{S}{N} \\beta I - \\gamma I - p I\\\\ \\dot{C} & = \\tau I \\\\ \\dot{\\mathcal{R}} & = \\gamma I \\\\ \\dot{D} & = p I \\end{align*} then, \\begin{align*} \\frac{\\ddot{C}}{\\dot{C}} = & \\frac{\\dot{\\tau}}{\\tau} + \\frac{S}{N}\\beta - \\gamma - p \\\\ \\frac{\\ddot{D}}{\\dot{D}} = & \\frac{S}{N}\\beta - \\gamma - p \\\\ \\end{align*} Other observable states could similarly be added to the model.","title":"Incorporating death"},{"location":"rt_longdiff/#time-delays","text":"A drawback of the above approach is that it implies changes in $R_t$ show up in the derivatives of case and death numbers instantly. This is definitely not true. Instead consider a model where infections last $\\ell$ days. After $\\ell$ days, each infected person dies with probability $\\pi$ and recovers otherwise. Then we have \\begin{align*} \\dot{S}(t) & = -\\frac{S(t)}{N} \\beta I \\\\ \\dot{I}(t) & = \\frac{S(t)}{N} \\beta(t) I(t) - \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) I(t-\\ell) \\\\ \\dot{C}(t) & = \\tau(t) I(t) \\\\ \\dot{\\mathcal{R}}(t) & = (1-\\pi) \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) I(t-\\ell) \\\\ \\dot{D}(t) & = \\pi \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) I(t-\\ell) \\end{align*} Rearranging gives \\frac{\\ddot{C}(t)}{\\dot{C}(t)} = \\frac{\\dot{\\tau}(t)}{\\tau(t)} + \\frac{S(t)}{N}\\beta(t) - \\frac{1}{\\pi} \\frac{\\dot{D}(t)}{\\dot{C}(t)} and \\dot{D}(t) = \\pi \\frac{S(t-\\ell)}{N} \\beta(t-\\ell) \\frac{\\dot{C}(t-\\ell)}{\\tau(t-\\ell)} Note These last two equations also hold in the model without time delay by setting $\\ell=0$ and $\\pi = \\frac{p}{p+\\gamma}$ Note Random durations can be accomodated by replacing the shift by $\\ell$ with a convolution.","title":"Time delays"},{"location":"state/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License In this section we estimate a SEIR model using data from US States. Compared to the model in the introductory section , the state model introduces some additional components to incorporate additional data. Data \u00b6 The data combines information on Daily case counts and deaths from JHU CSSE Daily Hospitalizations, recoveries, and testing from the Covid Tracking Project Covid related policy changes from Raifman et al Movements from Google Mobility Reports Hourly workers from Hoembase Model \u00b6 For each each the epidemic is modeled as: \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\left(\\beta_1(t) I_1 + \\beta_2(t) C_1 + \\beta_3(t) C_3\\right) \\\\ \\dot{E} & = \\frac{S}{N} \\left(\\beta_1(t) I_1 + \\beta_2(t) C_1 + \\beta_3(t) C_3\\right) - a E \\\\ \\dot{I_1} & = a E - \\gamma_1 I_1 - p_1 I_1 - \\tau C_1 \\\\ \\dot{C_1} & = \\tau I - \\gamma_1 C_1 - p_1 C_1 \\\\ \\dot{C_2} & = p_1(I_1 + C_1) - \\gamma_2 C_2 - p_2 C_2 \\\\ \\dot{R_u} & = \\sum_i \\gamma_i I_i \\\\ \\dot{R_c} & = \\sum_i \\gamma_i C_i \\\\ \\dot{CC} & = (\\tau + p_1)I_1 \\\\ \\dot{X} & = p_2 C_2 \\end{align*} where variables are defined as in the introduction , except $CC$, which is new. $CC$ is cumulative confirmed cases. Our data does not contain recoveries for all states, so it is not possible to calculate active cases. Our data does contain cumulative confirmed cases for every state. Heterogeneity \u00b6 Some parameters are heterogenous across states and/or time. Specifically, we assume that for state $s$, \\beta_{j,s}(t) = \\beta_{j,0} \\exp( x_{s,t} \\alpha + \\epsilon_{\\beta,s}) where $x_{s,t}$ are observables that shift infection rates. In the estimates below, $x_{s,t}$ will be indicators for whether a state of emergency, stay-at-home order, or other related policy is in place, and measures of movement and business operations. 1 $\\epsilon_{\\beta,s}$ is an unobserved error term with mean $0$. Each of the components of $x$ are $0$ in a baseline, pre-epidemic world. Hence $\\beta_{j,0}$ is the (median across states) infection rate absent any policy or behavioral response. The model imposes that $\\beta_{j}(t)/\\beta_{k}(t)$ are constant across states and time. I have no opinion on whether this is a good assumption. Additionally, testing rates, $\\tau_s$, and the portion of people exposed at time $0$, $p_{0,s}$ vary with state. Analogous, to the way $\\beta$ is parameterized, we assume \\tau_{s} = \\tau e^{\\epsilon_{\\tau,s}} and p_{0,s} = p_0 e^{\\epsilon_{p,s}}. Finally, we assume that $a$, $p_1$, $p_2$, $\\gamma_1$, and $\\gamma_2$ are common across states and time. Arguably these could vary with state demographics (e.g. older populations have lower recovery and higher death rates), and over time with strain on the medical system. We abstract from these concerns for now. Least Squares estimates \u00b6 It is much faster to compute least squares point estimates than a full Bayesian posterior. Although the statistical properties of these estimates are unclear, they give some idea of how well the model can fit the data, and serve as good initial values for computing Bayesian posteriors. Let $\\theta = (a, p, \\gamma, \\tau, \\beta, \\alpha, \\epsilon)$ denote the parameters. We simply minimize \\begin{align*} Q(\\theta) = \\sum_{s, t} & (dead_{s,t} - X_s(t))^2 + (cases_{s,t} - CC_{s}(t))^2 + \\\\ & (hospitalized_{s,t} - C_{2,s}(t))^2 + (recovered_{s,t} - R_{c,s}(t)^2) \\end{align*} where $words$ are variables in the data, and capital letters are computed from the model (and implicitly depend on $\\theta$). Hospitalizations and recoveries are not observed for all states and days, in which case those terms are simply omitted from the objective function. Given the equal weights to all observables, the objective function will be dominated by the cumulative cases terms. Particularly in the states and days where it is large. Estimation \u00b6 using CovidSEIR, Plots, VegaLite, PrettyTables, DataFrames, JLD2 Plots.pyplot() df = CovidSEIR.statedata() ode = CovidSEIR.MultiRegion.odeSEIR() dat = CovidSEIR.RegionsData(df[df[!,:fips].<60,:], idvar=:fips ); out = CovidSEIR.LeastSquares.leastsquares(dat, ode) params = out.params @save \"jmd/states_$(Dates.today()).jld2\" params f Results \u00b6 Parameters \u00b6 @load \"jmd/states_2020-04-19.jld2\" params parms = DataFrame() for k in keys(params) if length(params[k])==1 global parms = vcat(parms, DataFrame(Parameter=string(k), Estimate=params[k])) elseif k == :\u03b1 \u03b1names = [\"Emergency\",\"K12 closed\",\"Stay at home\",\"Restaurants closed\",\"Non-essential businesses closed\", \"%/100 reduction workplaces\",\"%/100 reduction grocery\",\"%/100 reduction retail\", \"%/100 reduction hourly businesses open\"] global parms = vcat(parms, DataFrame(Parameter=string(k).*\u03b1names, Estimate=params[k])) else L = length(params[k]) global parms = vcat(parms, DataFrame(Parameter=string(k).*string.(1:L), Estimate=params[k])) end end pretty_table(parms[1:19,:], formatters=ft_printf(\"%5.3g\")) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Parameter \u2502 Estimate \u2502 \u2502 String \u2502 Float64 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u03b21 \u2502 0.000354 \u2502 \u2502 \u03b22 \u2502 0.077 \u2502 \u2502 \u03b23 \u2502 1.04 \u2502 \u2502 \u03b31 \u2502 0.0451 \u2502 \u2502 \u03b32 \u2502 0.019 \u2502 \u2502 p1 \u2502 1e-08 \u2502 \u2502 p2 \u2502 0.0379 \u2502 \u2502 \u03c4 \u2502 2.21 \u2502 \u2502 a \u2502 0.0356 \u2502 \u2502 pE0 \u2502 6.14e-06 \u2502 \u2502 \u03b1Emergency \u2502 -0.098 \u2502 \u2502 \u03b1K12 closed \u2502 -3.13 \u2502 \u2502 \u03b1Stay at home \u2502 -0.836 \u2502 \u2502 \u03b1Restaurants closed \u2502 -2.33 \u2502 \u2502 \u03b1Non-essential businesses closed \u2502 -3.63 \u2502 \u2502 \u03b1%/100 reduction workplaces \u2502 -0.00287 \u2502 \u2502 \u03b1%/100 reduction grocery \u2502 -0.899 \u2502 \u2502 \u03b1%/100 reduction retail \u2502 -0.000107 \u2502 \u2502 \u03b1%/100 reduction hourly businesses open \u2502 -4.99 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 The parameter estimates have some apparent problems. The estimate of $a$ is much smaller than expected. $1/a$ should be the period of time from infection to becoming contagious. This is generally believed to be around 5 days. $\\beta_{1,0}$ being the smallest means that undetected infectious people are less likely to infect others compared to confirmed mildr or severe cases. The value of $\\tau$ is very high. To avoid numeric difficulties in optimization, I restricted $-5 \\leq \\alpha \\leq 0$. These constraints sometimes bind. For the policy variables, it is strange that the coefficient on stay at home is not the largest. The magnitude of these are quite larege. For example, closing non-essential businesses reduces infection rate to $e^{-3.63} \\approx 0.026$ of what it is was. The combination of all policies cuts infection rates to about 1/20,000th of baseline. display(histogram(params.e\u03b2, label=\"\u03f5_\u03b2\")) display(histogram(params.e\u03c4, label=\"\u03f5_\u03c4\")) display(histogram(params.ep, label=\"\u03f5_p\")) Fit \u00b6 using TransformVariables trans = as( (\u03b2=as(Array, as\u211d, 3), \u03b3=as(Array, as\u211d, 2), p=as(Array, as\u211d, 2), \u03c4=as\u211d, a=as\u211d, pE0=as\u211d, \u03b1=as(Array, as\u211d, size(dat.X,1)), e\u03b2=as(Array, as\u211d, length(dat.population)-1), e\u03c4=as(Array, as\u211d, length(dat.population)-1), ep=as(Array, as\u211d, length(dat.population)-1)) ) f = CovidSEIR.LeastSquares.plotfits(dat, ode, inverse(trans, params), trans) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) for fig in f.figs display(fig) end Including movement and business operations in $x$ makes interpretation difficult. If we want to estimate policy effects, we should be using these auxillary measures in some other way. We should not be holding them constant or conditioning on them. \u21a9","title":"US States"},{"location":"state/#data","text":"The data combines information on Daily case counts and deaths from JHU CSSE Daily Hospitalizations, recoveries, and testing from the Covid Tracking Project Covid related policy changes from Raifman et al Movements from Google Mobility Reports Hourly workers from Hoembase","title":"Data"},{"location":"state/#model","text":"For each each the epidemic is modeled as: \\begin{align*} \\dot{S} & = -\\frac{S}{N} \\left(\\beta_1(t) I_1 + \\beta_2(t) C_1 + \\beta_3(t) C_3\\right) \\\\ \\dot{E} & = \\frac{S}{N} \\left(\\beta_1(t) I_1 + \\beta_2(t) C_1 + \\beta_3(t) C_3\\right) - a E \\\\ \\dot{I_1} & = a E - \\gamma_1 I_1 - p_1 I_1 - \\tau C_1 \\\\ \\dot{C_1} & = \\tau I - \\gamma_1 C_1 - p_1 C_1 \\\\ \\dot{C_2} & = p_1(I_1 + C_1) - \\gamma_2 C_2 - p_2 C_2 \\\\ \\dot{R_u} & = \\sum_i \\gamma_i I_i \\\\ \\dot{R_c} & = \\sum_i \\gamma_i C_i \\\\ \\dot{CC} & = (\\tau + p_1)I_1 \\\\ \\dot{X} & = p_2 C_2 \\end{align*} where variables are defined as in the introduction , except $CC$, which is new. $CC$ is cumulative confirmed cases. Our data does not contain recoveries for all states, so it is not possible to calculate active cases. Our data does contain cumulative confirmed cases for every state.","title":"Model"},{"location":"state/#heterogeneity","text":"Some parameters are heterogenous across states and/or time. Specifically, we assume that for state $s$, \\beta_{j,s}(t) = \\beta_{j,0} \\exp( x_{s,t} \\alpha + \\epsilon_{\\beta,s}) where $x_{s,t}$ are observables that shift infection rates. In the estimates below, $x_{s,t}$ will be indicators for whether a state of emergency, stay-at-home order, or other related policy is in place, and measures of movement and business operations. 1 $\\epsilon_{\\beta,s}$ is an unobserved error term with mean $0$. Each of the components of $x$ are $0$ in a baseline, pre-epidemic world. Hence $\\beta_{j,0}$ is the (median across states) infection rate absent any policy or behavioral response. The model imposes that $\\beta_{j}(t)/\\beta_{k}(t)$ are constant across states and time. I have no opinion on whether this is a good assumption. Additionally, testing rates, $\\tau_s$, and the portion of people exposed at time $0$, $p_{0,s}$ vary with state. Analogous, to the way $\\beta$ is parameterized, we assume \\tau_{s} = \\tau e^{\\epsilon_{\\tau,s}} and p_{0,s} = p_0 e^{\\epsilon_{p,s}}. Finally, we assume that $a$, $p_1$, $p_2$, $\\gamma_1$, and $\\gamma_2$ are common across states and time. Arguably these could vary with state demographics (e.g. older populations have lower recovery and higher death rates), and over time with strain on the medical system. We abstract from these concerns for now.","title":"Heterogeneity"},{"location":"state/#least-squares-estimates","text":"It is much faster to compute least squares point estimates than a full Bayesian posterior. Although the statistical properties of these estimates are unclear, they give some idea of how well the model can fit the data, and serve as good initial values for computing Bayesian posteriors. Let $\\theta = (a, p, \\gamma, \\tau, \\beta, \\alpha, \\epsilon)$ denote the parameters. We simply minimize \\begin{align*} Q(\\theta) = \\sum_{s, t} & (dead_{s,t} - X_s(t))^2 + (cases_{s,t} - CC_{s}(t))^2 + \\\\ & (hospitalized_{s,t} - C_{2,s}(t))^2 + (recovered_{s,t} - R_{c,s}(t)^2) \\end{align*} where $words$ are variables in the data, and capital letters are computed from the model (and implicitly depend on $\\theta$). Hospitalizations and recoveries are not observed for all states and days, in which case those terms are simply omitted from the objective function. Given the equal weights to all observables, the objective function will be dominated by the cumulative cases terms. Particularly in the states and days where it is large.","title":"Least Squares estimates"},{"location":"state/#estimation","text":"using CovidSEIR, Plots, VegaLite, PrettyTables, DataFrames, JLD2 Plots.pyplot() df = CovidSEIR.statedata() ode = CovidSEIR.MultiRegion.odeSEIR() dat = CovidSEIR.RegionsData(df[df[!,:fips].<60,:], idvar=:fips ); out = CovidSEIR.LeastSquares.leastsquares(dat, ode) params = out.params @save \"jmd/states_$(Dates.today()).jld2\" params f","title":"Estimation"},{"location":"state/#results","text":"","title":"Results"},{"location":"state/#parameters","text":"@load \"jmd/states_2020-04-19.jld2\" params parms = DataFrame() for k in keys(params) if length(params[k])==1 global parms = vcat(parms, DataFrame(Parameter=string(k), Estimate=params[k])) elseif k == :\u03b1 \u03b1names = [\"Emergency\",\"K12 closed\",\"Stay at home\",\"Restaurants closed\",\"Non-essential businesses closed\", \"%/100 reduction workplaces\",\"%/100 reduction grocery\",\"%/100 reduction retail\", \"%/100 reduction hourly businesses open\"] global parms = vcat(parms, DataFrame(Parameter=string(k).*\u03b1names, Estimate=params[k])) else L = length(params[k]) global parms = vcat(parms, DataFrame(Parameter=string(k).*string.(1:L), Estimate=params[k])) end end pretty_table(parms[1:19,:], formatters=ft_printf(\"%5.3g\")) \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Parameter \u2502 Estimate \u2502 \u2502 String \u2502 Float64 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u03b21 \u2502 0.000354 \u2502 \u2502 \u03b22 \u2502 0.077 \u2502 \u2502 \u03b23 \u2502 1.04 \u2502 \u2502 \u03b31 \u2502 0.0451 \u2502 \u2502 \u03b32 \u2502 0.019 \u2502 \u2502 p1 \u2502 1e-08 \u2502 \u2502 p2 \u2502 0.0379 \u2502 \u2502 \u03c4 \u2502 2.21 \u2502 \u2502 a \u2502 0.0356 \u2502 \u2502 pE0 \u2502 6.14e-06 \u2502 \u2502 \u03b1Emergency \u2502 -0.098 \u2502 \u2502 \u03b1K12 closed \u2502 -3.13 \u2502 \u2502 \u03b1Stay at home \u2502 -0.836 \u2502 \u2502 \u03b1Restaurants closed \u2502 -2.33 \u2502 \u2502 \u03b1Non-essential businesses closed \u2502 -3.63 \u2502 \u2502 \u03b1%/100 reduction workplaces \u2502 -0.00287 \u2502 \u2502 \u03b1%/100 reduction grocery \u2502 -0.899 \u2502 \u2502 \u03b1%/100 reduction retail \u2502 -0.000107 \u2502 \u2502 \u03b1%/100 reduction hourly businesses open \u2502 -4.99 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 The parameter estimates have some apparent problems. The estimate of $a$ is much smaller than expected. $1/a$ should be the period of time from infection to becoming contagious. This is generally believed to be around 5 days. $\\beta_{1,0}$ being the smallest means that undetected infectious people are less likely to infect others compared to confirmed mildr or severe cases. The value of $\\tau$ is very high. To avoid numeric difficulties in optimization, I restricted $-5 \\leq \\alpha \\leq 0$. These constraints sometimes bind. For the policy variables, it is strange that the coefficient on stay at home is not the largest. The magnitude of these are quite larege. For example, closing non-essential businesses reduces infection rate to $e^{-3.63} \\approx 0.026$ of what it is was. The combination of all policies cuts infection rates to about 1/20,000th of baseline. display(histogram(params.e\u03b2, label=\"\u03f5_\u03b2\")) display(histogram(params.e\u03c4, label=\"\u03f5_\u03c4\")) display(histogram(params.ep, label=\"\u03f5_p\"))","title":"Parameters"},{"location":"state/#fit","text":"using TransformVariables trans = as( (\u03b2=as(Array, as\u211d, 3), \u03b3=as(Array, as\u211d, 2), p=as(Array, as\u211d, 2), \u03c4=as\u211d, a=as\u211d, pE0=as\u211d, \u03b1=as(Array, as\u211d, size(dat.X,1)), e\u03b2=as(Array, as\u211d, length(dat.population)-1), e\u03c4=as(Array, as\u211d, length(dat.population)-1), ep=as(Array, as\u211d, length(dat.population)-1)) ) f = CovidSEIR.LeastSquares.plotfits(dat, ode, inverse(trans, params), trans) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) (size(sol), size(tsave)) = ((9, 94), (94,)) for fig in f.figs display(fig) end Including movement and business operations in $x$ makes interpretation difficult. If we want to estimate policy effects, we should be using these auxillary measures in some other way. We should not be holding them constant or conditioning on them. \u21a9","title":"Fit"},{"location":"us/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License using CovidSEIR, Plots, DataFrames, JLD2, StatsPlots, Dates, MCMCChains Plots.pyplot() jmddir = normpath(joinpath(dirname(Base.find_package(\"CovidSEIR\")),\"..\",\"docs\",\"jmd\")) covdf = covidjhudata(); United States \u00b6 us = CountryData(covdf, \"US\") CovidSEIR.CountryData{Float64,Int64}(3.2716743e8, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10 \u2026 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], [0.0, 0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0, 0.0 \u2026 12722.0, 14695.0, 16478.0, 18586.0, 20462.0, 22019.0, 23528.0, 25831.0, 28325.0, 32916.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0 \u2026 21763.0, 23559.0, 25410.0, 28790.0, 31270.0, 32988 .0, 43482.0, 47763.0, 52096.0, 54703.0], [1.0, 1.0, 2.0, 2.0, 5.0, 5.0, 5.0 , 5.0, 5.0, 7.0 \u2026 361738.0, 390798.0, 419549.0, 449159.0, 474664.0, 50030 6.0, 513609.0, 534076.0, 555929.0, 580182.0]) using Turing mdl = CovidSEIR.TimeVarying.countrymodel(us) cc = Turing.psample(mdl, NUTS(0.65), 5000, 4) import JLD2 JLD2.@save \"$jmddir/us_tv_$(Dates.today()).jld2\" cc JLD2.@load \"$jmddir/us_dhmc_2020-04-13.jld2\" cc; cc = MCMCChains.Chains(collect(cc.value.data), replace.(cc.name_map.parameters, r\"([^\\[])([1-9])\" => s\"\\1[\\2]\")) Object of type Chains, with data of type 5000\u00d715\u00d74 Array{Float64,3} Iterations = 1:5000 Thinning interval = 1 Chains = 1, 2, 3, 4 Samples per chain = 5000 parameters = \u03c4, sigD, sigC, sigRc, a, pE0, p[1], p[2], \u03b2[1], \u03b2[2], \u03b2 [3], \u03b3[1], \u03b3[2], \u03c1[1], \u03c1[2] 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0001 0.0000 0.0000 0.0000 81.9924 1.2819 sigD 137.7127 8.0087 0.0566 0.3734 140.4813 1.0810 sigC 1334.2734 92.6313 0.6550 4.5797 97.6593 1.2625 sigRc 731.7612 43.2561 0.3059 2.2038 96.7621 1.2997 a 0.7609 0.1862 0.0013 0.0131 80.3213 3.9037 pE0 0.0000 0.0000 0.0000 0.0000 100.5049 1.1776 p[1] 0.0193 0.0021 0.0000 0.0001 80.3213 1.7318 p[2] 0.0045 0.0000 0.0000 0.0000 141.0271 1.0764 \u03b2[1] 0.8113 1.2418 0.0088 0.0876 80.3213 9.1362 \u03b2[2] 1.1373 0.6511 0.0046 0.0422 80.3213 1.5722 \u03b2[3] 3.9270 0.7566 0.0053 0.0529 80.3213 3.8765 \u03b3[1] 2.9379 0.1078 0.0008 0.0076 80.3213 7.4234 \u03b3[2] 0.0067 0.0003 0.0000 0.0000 80.3213 1.4663 \u03c1[1] 0.2486 0.0306 0.0002 0.0021 80.3213 2.2939 \u03c1[2] 61.2539 0.4053 0.0029 0.0241 80.3213 1.3951 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0001 0.0002 sigD 118.6077 135.2341 139.7070 141.1686 152.2954 sigC 1155.0157 1294.3130 1305.6873 1394.1663 1504.5102 sigRc 653.7688 701.9086 724.4422 762.5418 816.5018 a 0.3665 0.5791 0.7621 0.9520 0.9683 pE0 0.0000 0.0000 0.0000 0.0000 0.0000 p[1] 0.0151 0.0178 0.0190 0.0216 0.0223 p[2] 0.0045 0.0045 0.0045 0.0045 0.0045 \u03b2[1] 0.0000 0.0000 0.1534 0.7802 3.2127 \u03b2[2] 0.0723 0.7875 1.1188 1.5326 2.6644 \u03b2[3] 2.4007 4.1450 4.1807 4.4108 4.7409 \u03b3[1] 2.7158 2.8516 3.0000 3.0000 3.0000 \u03b3[2] 0.0061 0.0064 0.0067 0.0070 0.0070 \u03c1[1] 0.2132 0.2236 0.2414 0.2667 0.3312 \u03c1[2] 60.5702 60.9084 61.1496 61.5801 62.0245 Estimates \u00b6 plot(cc) describe(cc) 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0001 0.0000 0.0000 0.0000 81.9924 1.2819 sigD 137.7127 8.0087 0.0566 0.3734 140.4813 1.0810 sigC 1334.2734 92.6313 0.6550 4.5797 97.6593 1.2625 sigRc 731.7612 43.2561 0.3059 2.2038 96.7621 1.2997 a 0.7609 0.1862 0.0013 0.0131 80.3213 3.9037 pE0 0.0000 0.0000 0.0000 0.0000 100.5049 1.1776 p[1] 0.0193 0.0021 0.0000 0.0001 80.3213 1.7318 p[2] 0.0045 0.0000 0.0000 0.0000 141.0271 1.0764 \u03b2[1] 0.8113 1.2418 0.0088 0.0876 80.3213 9.1362 \u03b2[2] 1.1373 0.6511 0.0046 0.0422 80.3213 1.5722 \u03b2[3] 3.9270 0.7566 0.0053 0.0529 80.3213 3.8765 \u03b3[1] 2.9379 0.1078 0.0008 0.0076 80.3213 7.4234 \u03b3[2] 0.0067 0.0003 0.0000 0.0000 80.3213 1.4663 \u03c1[1] 0.2486 0.0306 0.0002 0.0021 80.3213 2.2939 \u03c1[2] 61.2539 0.4053 0.0029 0.0241 80.3213 1.3951 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0001 0.0002 sigD 118.6077 135.2341 139.7070 141.1686 152.2954 sigC 1155.0157 1294.3130 1305.6873 1394.1663 1504.5102 sigRc 653.7688 701.9086 724.4422 762.5418 816.5018 a 0.3665 0.5791 0.7621 0.9520 0.9683 pE0 0.0000 0.0000 0.0000 0.0000 0.0000 p[1] 0.0151 0.0178 0.0190 0.0216 0.0223 p[2] 0.0045 0.0045 0.0045 0.0045 0.0045 \u03b2[1] 0.0000 0.0000 0.1534 0.7802 3.2127 \u03b2[2] 0.0723 0.7875 1.1188 1.5326 2.6644 \u03b2[3] 2.4007 4.1450 4.1807 4.4108 4.7409 \u03b3[1] 2.7158 2.8516 3.0000 3.0000 3.0000 \u03b3[2] 0.0061 0.0064 0.0067 0.0070 0.0070 \u03c1[1] 0.2132 0.2236 0.2414 0.2667 0.3312 \u03c1[2] 60.5702 60.9084 61.1496 61.5801 62.0245 Fit \u00b6 sdf = simtrajectories(cc, us, 1:200) f = plotvars(sdf, us) plot(f.fit, ylim=(0, maximum(us.active)*1.3)) Implications \u00b6 for fig in f.trajectories display(plot(fig)) end","title":"United States"},{"location":"us/#united-states","text":"us = CountryData(covdf, \"US\") CovidSEIR.CountryData{Float64,Int64}(3.2716743e8, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10 \u2026 77, 78, 79, 80, 81, 82, 83, 84, 85, 86], [0.0, 0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0, 0.0 \u2026 12722.0, 14695.0, 16478.0, 18586.0, 20462.0, 22019.0, 23528.0, 25831.0, 28325.0, 32916.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0 \u2026 21763.0, 23559.0, 25410.0, 28790.0, 31270.0, 32988 .0, 43482.0, 47763.0, 52096.0, 54703.0], [1.0, 1.0, 2.0, 2.0, 5.0, 5.0, 5.0 , 5.0, 5.0, 7.0 \u2026 361738.0, 390798.0, 419549.0, 449159.0, 474664.0, 50030 6.0, 513609.0, 534076.0, 555929.0, 580182.0]) using Turing mdl = CovidSEIR.TimeVarying.countrymodel(us) cc = Turing.psample(mdl, NUTS(0.65), 5000, 4) import JLD2 JLD2.@save \"$jmddir/us_tv_$(Dates.today()).jld2\" cc JLD2.@load \"$jmddir/us_dhmc_2020-04-13.jld2\" cc; cc = MCMCChains.Chains(collect(cc.value.data), replace.(cc.name_map.parameters, r\"([^\\[])([1-9])\" => s\"\\1[\\2]\")) Object of type Chains, with data of type 5000\u00d715\u00d74 Array{Float64,3} Iterations = 1:5000 Thinning interval = 1 Chains = 1, 2, 3, 4 Samples per chain = 5000 parameters = \u03c4, sigD, sigC, sigRc, a, pE0, p[1], p[2], \u03b2[1], \u03b2[2], \u03b2 [3], \u03b3[1], \u03b3[2], \u03c1[1], \u03c1[2] 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0001 0.0000 0.0000 0.0000 81.9924 1.2819 sigD 137.7127 8.0087 0.0566 0.3734 140.4813 1.0810 sigC 1334.2734 92.6313 0.6550 4.5797 97.6593 1.2625 sigRc 731.7612 43.2561 0.3059 2.2038 96.7621 1.2997 a 0.7609 0.1862 0.0013 0.0131 80.3213 3.9037 pE0 0.0000 0.0000 0.0000 0.0000 100.5049 1.1776 p[1] 0.0193 0.0021 0.0000 0.0001 80.3213 1.7318 p[2] 0.0045 0.0000 0.0000 0.0000 141.0271 1.0764 \u03b2[1] 0.8113 1.2418 0.0088 0.0876 80.3213 9.1362 \u03b2[2] 1.1373 0.6511 0.0046 0.0422 80.3213 1.5722 \u03b2[3] 3.9270 0.7566 0.0053 0.0529 80.3213 3.8765 \u03b3[1] 2.9379 0.1078 0.0008 0.0076 80.3213 7.4234 \u03b3[2] 0.0067 0.0003 0.0000 0.0000 80.3213 1.4663 \u03c1[1] 0.2486 0.0306 0.0002 0.0021 80.3213 2.2939 \u03c1[2] 61.2539 0.4053 0.0029 0.0241 80.3213 1.3951 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0001 0.0002 sigD 118.6077 135.2341 139.7070 141.1686 152.2954 sigC 1155.0157 1294.3130 1305.6873 1394.1663 1504.5102 sigRc 653.7688 701.9086 724.4422 762.5418 816.5018 a 0.3665 0.5791 0.7621 0.9520 0.9683 pE0 0.0000 0.0000 0.0000 0.0000 0.0000 p[1] 0.0151 0.0178 0.0190 0.0216 0.0223 p[2] 0.0045 0.0045 0.0045 0.0045 0.0045 \u03b2[1] 0.0000 0.0000 0.1534 0.7802 3.2127 \u03b2[2] 0.0723 0.7875 1.1188 1.5326 2.6644 \u03b2[3] 2.4007 4.1450 4.1807 4.4108 4.7409 \u03b3[1] 2.7158 2.8516 3.0000 3.0000 3.0000 \u03b3[2] 0.0061 0.0064 0.0067 0.0070 0.0070 \u03c1[1] 0.2132 0.2236 0.2414 0.2667 0.3312 \u03c1[2] 60.5702 60.9084 61.1496 61.5801 62.0245","title":"United States"},{"location":"us/#estimates","text":"plot(cc) describe(cc) 2-element Array{MCMCChains.ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0001 0.0000 0.0000 0.0000 81.9924 1.2819 sigD 137.7127 8.0087 0.0566 0.3734 140.4813 1.0810 sigC 1334.2734 92.6313 0.6550 4.5797 97.6593 1.2625 sigRc 731.7612 43.2561 0.3059 2.2038 96.7621 1.2997 a 0.7609 0.1862 0.0013 0.0131 80.3213 3.9037 pE0 0.0000 0.0000 0.0000 0.0000 100.5049 1.1776 p[1] 0.0193 0.0021 0.0000 0.0001 80.3213 1.7318 p[2] 0.0045 0.0000 0.0000 0.0000 141.0271 1.0764 \u03b2[1] 0.8113 1.2418 0.0088 0.0876 80.3213 9.1362 \u03b2[2] 1.1373 0.6511 0.0046 0.0422 80.3213 1.5722 \u03b2[3] 3.9270 0.7566 0.0053 0.0529 80.3213 3.8765 \u03b3[1] 2.9379 0.1078 0.0008 0.0076 80.3213 7.4234 \u03b3[2] 0.0067 0.0003 0.0000 0.0000 80.3213 1.4663 \u03c1[1] 0.2486 0.0306 0.0002 0.0021 80.3213 2.2939 \u03c1[2] 61.2539 0.4053 0.0029 0.0241 80.3213 1.3951 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03c4 0.0000 0.0000 0.0000 0.0001 0.0002 sigD 118.6077 135.2341 139.7070 141.1686 152.2954 sigC 1155.0157 1294.3130 1305.6873 1394.1663 1504.5102 sigRc 653.7688 701.9086 724.4422 762.5418 816.5018 a 0.3665 0.5791 0.7621 0.9520 0.9683 pE0 0.0000 0.0000 0.0000 0.0000 0.0000 p[1] 0.0151 0.0178 0.0190 0.0216 0.0223 p[2] 0.0045 0.0045 0.0045 0.0045 0.0045 \u03b2[1] 0.0000 0.0000 0.1534 0.7802 3.2127 \u03b2[2] 0.0723 0.7875 1.1188 1.5326 2.6644 \u03b2[3] 2.4007 4.1450 4.1807 4.4108 4.7409 \u03b3[1] 2.7158 2.8516 3.0000 3.0000 3.0000 \u03b3[2] 0.0061 0.0064 0.0067 0.0070 0.0070 \u03c1[1] 0.2132 0.2236 0.2414 0.2667 0.3312 \u03c1[2] 60.5702 60.9084 61.1496 61.5801 62.0245","title":"Estimates"},{"location":"us/#fit","text":"sdf = simtrajectories(cc, us, 1:200) f = plotvars(sdf, us) plot(f.fit, ylim=(0, maximum(us.active)*1.3))","title":"Fit"},{"location":"us/#implications","text":"for fig in f.trajectories display(plot(fig)) end","title":"Implications"}]}